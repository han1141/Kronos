# Kronos æ•…éšœæ’é™¤å’Œå¸¸è§é—®é¢˜è§£ç­”

## ğŸ”§ å®‰è£…å’Œç¯å¢ƒé—®é¢˜

### Q1: PyTorchå®‰è£…å¤±è´¥
**é—®é¢˜æè¿°**: `pip install torch` å¤±è´¥æˆ–ç‰ˆæœ¬ä¸å…¼å®¹

**è§£å†³æ–¹æ¡ˆ**:
```bash
# æ–¹æ¡ˆ1: ä½¿ç”¨å®˜æ–¹å®‰è£…å‘½ä»¤
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118

# æ–¹æ¡ˆ2: æ¸…ç†ç¼“å­˜é‡æ–°å®‰è£…
pip cache purge
pip uninstall torch torchvision torchaudio
pip install torch torchvision torchaudio

# æ–¹æ¡ˆ3: ä½¿ç”¨condaå®‰è£…
conda install pytorch torchvision torchaudio pytorch-cuda=11.8 -c pytorch -c nvidia
```

**éªŒè¯å®‰è£…**:
```python
import torch
print(f"PyTorchç‰ˆæœ¬: {torch.__version__}")
print(f"CUDAå¯ç”¨: {torch.cuda.is_available()}")
print(f"CUDAç‰ˆæœ¬: {torch.version.cuda}")
```

### Q2: Hugging Faceè¿æ¥è¶…æ—¶
**é—®é¢˜æè¿°**: ä¸‹è½½æ¨¡å‹æ—¶ç½‘ç»œè¿æ¥å¤±è´¥

**è§£å†³æ–¹æ¡ˆ**:
```bash
# æ–¹æ¡ˆ1: ä½¿ç”¨é•œåƒæº
export HF_ENDPOINT=https://hf-mirror.com
pip install -U huggingface_hub

# æ–¹æ¡ˆ2: è®¾ç½®ä»£ç†
export HTTP_PROXY=http://your-proxy:port
export HTTPS_PROXY=http://your-proxy:port

# æ–¹æ¡ˆ3: ç¦»çº¿ä¸‹è½½
# æ‰‹åŠ¨ä¸‹è½½æ¨¡å‹æ–‡ä»¶åˆ°æœ¬åœ°ï¼Œç„¶åä½¿ç”¨æœ¬åœ°è·¯å¾„
```

**éªŒè¯è¿æ¥**:
```python
from huggingface_hub import HfApi
api = HfApi()
try:
    models = api.list_models(author="NeoQuasar", limit=5)
    print("âœ… Hugging Faceè¿æ¥æ­£å¸¸")
except Exception as e:
    print(f"âŒ è¿æ¥å¤±è´¥: {e}")
```

### Q3: ä¾èµ–ç‰ˆæœ¬å†²çª
**é—®é¢˜æè¿°**: ä¸åŒåŒ…ä¹‹é—´ç‰ˆæœ¬ä¸å…¼å®¹

**è§£å†³æ–¹æ¡ˆ**:
```bash
# æ–¹æ¡ˆ1: åˆ›å»ºæ–°çš„è™šæ‹Ÿç¯å¢ƒ
conda create -n kronos-clean python=3.10
conda activate kronos-clean
pip install -r requirements.txt

# æ–¹æ¡ˆ2: ä½¿ç”¨pip-toolsç®¡ç†ä¾èµ–
pip install pip-tools
pip-compile requirements.in
pip-sync requirements.txt

# æ–¹æ¡ˆ3: æ‰‹åŠ¨è§£å†³å†²çª
pip install --upgrade --force-reinstall package_name
```

## ğŸš€ æ¨¡å‹åŠ è½½é—®é¢˜

### Q4: æ¨¡å‹ä¸‹è½½å¤±è´¥
**é—®é¢˜æè¿°**: `from_pretrained()` æ–¹æ³•å¤±è´¥

**è§£å†³æ–¹æ¡ˆ**:
```python
# æ–¹æ¡ˆ1: æŒ‡å®šç¼“å­˜ç›®å½•
from transformers import AutoModel
model = AutoModel.from_pretrained(
    "NeoQuasar/Kronos-small",
    cache_dir="./models_cache",
    force_download=True
)

# æ–¹æ¡ˆ2: ä½¿ç”¨æœ¬åœ°è·¯å¾„
# å…ˆæ‰‹åŠ¨ä¸‹è½½æ¨¡å‹åˆ°æœ¬åœ°
model = Kronos.from_pretrained("./local_models/Kronos-small")

# æ–¹æ¡ˆ3: åˆ†æ­¥ä¸‹è½½
from huggingface_hub import snapshot_download
snapshot_download(
    repo_id="NeoQuasar/Kronos-small",
    local_dir="./models/Kronos-small"
)
```

### Q5: GPUå†…å­˜ä¸è¶³
**é—®é¢˜æè¿°**: CUDA out of memoryé”™è¯¯

**è§£å†³æ–¹æ¡ˆ**:
```python
# æ–¹æ¡ˆ1: å‡å°‘æ‰¹å¤„ç†å¤§å°
predictor = KronosPredictor(
    model, tokenizer, 
    device="cuda:0", 
    max_context=256  # å‡å°‘ä¸Šä¸‹æ–‡é•¿åº¦
)

# æ–¹æ¡ˆ2: ä½¿ç”¨CPU
predictor = KronosPredictor(
    model, tokenizer, 
    device="cpu"
)

# æ–¹æ¡ˆ3: æ¸…ç†GPUç¼“å­˜
import torch
torch.cuda.empty_cache()

# æ–¹æ¡ˆ4: ä½¿ç”¨æ›´å°çš„æ¨¡å‹
model = Kronos.from_pretrained("NeoQuasar/Kronos-mini")  # 4.1Må‚æ•°
```

### Q6: æ¨¡å‹åŠ è½½é€Ÿåº¦æ…¢
**é—®é¢˜æè¿°**: é¦–æ¬¡åŠ è½½æ¨¡å‹è€—æ—¶å¾ˆé•¿

**è§£å†³æ–¹æ¡ˆ**:
```python
# æ–¹æ¡ˆ1: é¢„ä¸‹è½½æ¨¡å‹
from huggingface_hub import snapshot_download
snapshot_download("NeoQuasar/Kronos-small", local_dir="./models")

# æ–¹æ¡ˆ2: ä½¿ç”¨æœ¬åœ°ç¼“å­˜
import os
os.environ['TRANSFORMERS_CACHE'] = './models_cache'

# æ–¹æ¡ˆ3: å¹¶è¡Œä¸‹è½½
from concurrent.futures import ThreadPoolExecutor
def download_model(model_name):
    return snapshot_download(model_name)

with ThreadPoolExecutor() as executor:
    future = executor.submit(download_model, "NeoQuasar/Kronos-small")
```

## ğŸ“Š æ•°æ®å¤„ç†é—®é¢˜

### Q7: æ•°æ®æ ¼å¼é”™è¯¯
**é—®é¢˜æè¿°**: æ•°æ®åˆ—åæˆ–æ ¼å¼ä¸æ­£ç¡®

**è§£å†³æ–¹æ¡ˆ**:
```python
import pandas as pd

# æ£€æŸ¥æ•°æ®æ ¼å¼
def validate_data(df):
    required_cols = ['open', 'high', 'low', 'close']
    missing_cols = [col for col in required_cols if col not in df.columns]
    
    if missing_cols:
        print(f"âŒ ç¼ºå°‘å¿…éœ€åˆ—: {missing_cols}")
        return False
    
    # æ£€æŸ¥æ•°æ®ç±»å‹
    for col in required_cols:
        if not pd.api.types.is_numeric_dtype(df[col]):
            print(f"âŒ åˆ— {col} ä¸æ˜¯æ•°å€¼ç±»å‹")
            return False
    
    # æ£€æŸ¥ç¼ºå¤±å€¼
    if df[required_cols].isnull().any().any():
        print("âŒ å­˜åœ¨ç¼ºå¤±å€¼")
        return False
    
    print("âœ… æ•°æ®æ ¼å¼æ­£ç¡®")
    return True

# æ•°æ®æ¸…ç†
def clean_data(df):
    # è½¬æ¢æ•°æ®ç±»å‹
    numeric_cols = ['open', 'high', 'low', 'close', 'volume']
    for col in numeric_cols:
        if col in df.columns:
            df[col] = pd.to_numeric(df[col], errors='coerce')
    
    # å¤„ç†æ—¶é—´æˆ³
    if 'timestamps' not in df.columns:
        if 'timestamp' in df.columns:
            df['timestamps'] = pd.to_datetime(df['timestamp'])
        elif 'date' in df.columns:
            df['timestamps'] = pd.to_datetime(df['date'])
        else:
            df['timestamps'] = pd.date_range(start='2024-01-01', periods=len(df), freq='1H')
    
    # åˆ é™¤ç¼ºå¤±å€¼
    df = df.dropna()
    
    return df
```

### Q8: æ—¶é—´æˆ³å¤„ç†é—®é¢˜
**é—®é¢˜æè¿°**: æ—¶é—´æˆ³æ ¼å¼ä¸æ­£ç¡®æˆ–ç¼ºå¤±

**è§£å†³æ–¹æ¡ˆ**:
```python
# å¤„ç†å„ç§æ—¶é—´æˆ³æ ¼å¼
def fix_timestamps(df):
    timestamp_cols = ['timestamps', 'timestamp', 'date', 'datetime']
    
    for col in timestamp_cols:
        if col in df.columns:
            try:
                df['timestamps'] = pd.to_datetime(df[col])
                break
            except:
                continue
    
    # å¦‚æœæ²¡æœ‰æ—¶é—´æˆ³ï¼Œåˆ›å»ºä¸€ä¸ª
    if 'timestamps' not in df.columns:
        df['timestamps'] = pd.date_range(
            start='2024-01-01', 
            periods=len(df), 
            freq='5T'  # 5åˆ†é’Ÿé—´éš”
        )
    
    # ç¡®ä¿æ—¶é—´æˆ³æ’åº
    df = df.sort_values('timestamps').reset_index(drop=True)
    
    return df
```

### Q9: æ•°æ®é•¿åº¦ä¸è¶³
**é—®é¢˜æè¿°**: å†å²æ•°æ®ä¸å¤Ÿ400ä¸ªç‚¹

**è§£å†³æ–¹æ¡ˆ**:
```python
def check_data_length(df, lookback=400, pred_len=120):
    required_length = lookback + pred_len
    
    if len(df) < required_length:
        print(f"âŒ æ•°æ®é•¿åº¦ä¸è¶³: éœ€è¦{required_length}ä¸ªç‚¹ï¼Œå®é™…{len(df)}ä¸ªç‚¹")
        
        # æ–¹æ¡ˆ1: å‡å°‘lookback
        max_lookback = len(df) - pred_len
        if max_lookback > 0:
            print(f"å»ºè®®å°†lookbackè°ƒæ•´ä¸º: {max_lookback}")
            return max_lookback, pred_len
        
        # æ–¹æ¡ˆ2: å‡å°‘pred_len
        max_pred_len = len(df) - 100  # ä¿ç•™æœ€å°‘100ä¸ªå†å²ç‚¹
        if max_pred_len > 0:
            print(f"å»ºè®®å°†pred_lenè°ƒæ•´ä¸º: {max_pred_len}")
            return 100, max_pred_len
        
        return None, None
    
    print("âœ… æ•°æ®é•¿åº¦å……è¶³")
    return lookback, pred_len
```

## ğŸŒ Web UIé—®é¢˜

### Q10: Web UIå¯åŠ¨å¤±è´¥
**é—®é¢˜æè¿°**: Flaskåº”ç”¨æ— æ³•å¯åŠ¨

**è§£å†³æ–¹æ¡ˆ**:
```bash
# æ£€æŸ¥ç«¯å£å ç”¨
lsof -i :7070
netstat -tulpn | grep 7070

# æ›´æ¢ç«¯å£
cd webui
python -c "
from app import app
app.run(debug=True, host='0.0.0.0', port=7071)
"

# æ£€æŸ¥ä¾èµ–
pip install -r webui/requirements.txt

# æƒé™é—®é¢˜
sudo chown -R $USER:$USER webui/
```

### Q11: Web UIæ¨¡å‹åŠ è½½å¤±è´¥
**é—®é¢˜æè¿°**: åœ¨Webç•Œé¢ä¸­æ— æ³•åŠ è½½æ¨¡å‹

**è§£å†³æ–¹æ¡ˆ**:
```python
# æ£€æŸ¥æ¨¡å‹å¯ç”¨æ€§
def check_model_availability():
    try:
        from model import Kronos, KronosTokenizer, KronosPredictor
        print("âœ… æ¨¡å‹æ¨¡å—å¯¼å…¥æˆåŠŸ")
        
        # æµ‹è¯•æ¨¡å‹åŠ è½½
        tokenizer = KronosTokenizer.from_pretrained("NeoQuasar/Kronos-Tokenizer-base")
        model = Kronos.from_pretrained("NeoQuasar/Kronos-small")
        print("âœ… æ¨¡å‹åŠ è½½æˆåŠŸ")
        
        return True
    except Exception as e:
        print(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
        return False

# åœ¨webuiç›®å½•ä¸‹è¿è¡Œ
check_model_availability()
```

### Q12: é¢„æµ‹ç»“æœå¼‚å¸¸
**é—®é¢˜æè¿°**: Web UIé¢„æµ‹ç»“æœä¸åˆç†

**è§£å†³æ–¹æ¡ˆ**:
```python
# æ£€æŸ¥é¢„æµ‹å‚æ•°
def validate_prediction_params(T, top_p, sample_count):
    issues = []
    
    if T < 0.1 or T > 2.0:
        issues.append(f"Temperature {T} è¶…å‡ºèŒƒå›´ [0.1, 2.0]")
    
    if top_p < 0.1 or top_p > 1.0:
        issues.append(f"top_p {top_p} è¶…å‡ºèŒƒå›´ [0.1, 1.0]")
    
    if sample_count < 1 or sample_count > 5:
        issues.append(f"sample_count {sample_count} è¶…å‡ºèŒƒå›´ [1, 5]")
    
    if issues:
        print("âŒ å‚æ•°é—®é¢˜:")
        for issue in issues:
            print(f"  - {issue}")
        return False
    
    print("âœ… å‚æ•°è®¾ç½®æ­£ç¡®")
    return True

# æ¨èå‚æ•°
recommended_params = {
    "conservative": {"T": 0.8, "top_p": 0.7, "sample_count": 3},
    "balanced": {"T": 1.0, "top_p": 0.9, "sample_count": 2},
    "exploratory": {"T": 1.5, "top_p": 1.0, "sample_count": 5}
}
```

## ğŸ”§ å¾®è°ƒè®­ç»ƒé—®é¢˜

### Q13: è®­ç»ƒè¿‡ç¨‹ä¸­æ–­
**é—®é¢˜æè¿°**: è®­ç»ƒè¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯ä¸­æ–­

**è§£å†³æ–¹æ¡ˆ**:
```python
# æ·»åŠ æ£€æŸ¥ç‚¹æ¢å¤
def resume_training(checkpoint_path, model, optimizer):
    if os.path.exists(checkpoint_path):
        checkpoint = torch.load(checkpoint_path)
        model.load_state_dict(checkpoint['model_state_dict'])
        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
        start_epoch = checkpoint['epoch'] + 1
        print(f"ä»ç¬¬{start_epoch}è½®æ¢å¤è®­ç»ƒ")
        return start_epoch
    return 0

# å®šæœŸä¿å­˜æ£€æŸ¥ç‚¹
def save_checkpoint(epoch, model, optimizer, loss, path):
    torch.save({
        'epoch': epoch,
        'model_state_dict': model.state_dict(),
        'optimizer_state_dict': optimizer.state_dict(),
        'loss': loss,
    }, path)
```

### Q14: è®­ç»ƒé€Ÿåº¦æ…¢
**é—®é¢˜æè¿°**: è®­ç»ƒè¿‡ç¨‹è€—æ—¶è¿‡é•¿

**è§£å†³æ–¹æ¡ˆ**:
```python
# ä¼˜åŒ–è®­ç»ƒé€Ÿåº¦
def optimize_training():
    # 1. ä½¿ç”¨å¤šGPU
    if torch.cuda.device_count() > 1:
        model = torch.nn.DataParallel(model)
    
    # 2. æ··åˆç²¾åº¦è®­ç»ƒ
    from torch.cuda.amp import autocast, GradScaler
    scaler = GradScaler()
    
    # 3. ä¼˜åŒ–æ•°æ®åŠ è½½
    dataloader = DataLoader(
        dataset, 
        batch_size=batch_size,
        num_workers=4,  # å¤šè¿›ç¨‹åŠ è½½
        pin_memory=True,  # å›ºå®šå†…å­˜
        persistent_workers=True
    )
    
    # 4. ç¼–è¯‘æ¨¡å‹ï¼ˆPyTorch 2.0+ï¼‰
    model = torch.compile(model)
    
    return model, scaler, dataloader
```

### Q15: å†…å­˜æº¢å‡º
**é—®é¢˜æè¿°**: è®­ç»ƒæ—¶å‡ºç°OOMé”™è¯¯

**è§£å†³æ–¹æ¡ˆ**:
```python
# å†…å­˜ä¼˜åŒ–ç­–ç•¥
def optimize_memory():
    # 1. å‡å°‘æ‰¹å¤§å°
    batch_size = 16  # ä»50å‡å°‘åˆ°16
    
    # 2. æ¢¯åº¦ç´¯ç§¯
    accumulation_steps = 4  # æœ‰æ•ˆæ‰¹å¤§å° = 16 * 4 = 64
    
    # 3. æ¢¯åº¦æ£€æŸ¥ç‚¹
    model.gradient_checkpointing_enable()
    
    # 4. æ¸…ç†ç¼“å­˜
    torch.cuda.empty_cache()
    
    # 5. ä½¿ç”¨CPUå¸è½½
    from torch.distributed.fsdp import FullyShardedDataParallel as FSDP
    model = FSDP(model, cpu_offload=True)
    
    return batch_size, accumulation_steps
```

## ğŸ“ˆ æ€§èƒ½ä¼˜åŒ–é—®é¢˜

### Q16: é¢„æµ‹é€Ÿåº¦æ…¢
**é—®é¢˜æè¿°**: å•æ¬¡é¢„æµ‹è€—æ—¶è¿‡é•¿

**è§£å†³æ–¹æ¡ˆ**:
```python
# é¢„æµ‹é€Ÿåº¦ä¼˜åŒ–
def optimize_inference():
    # 1. æ¨¡å‹é‡åŒ–
    model = torch.quantization.quantize_dynamic(
        model, {torch.nn.Linear}, dtype=torch.qint8
    )
    
    # 2. TorchScriptç¼–è¯‘
    model = torch.jit.script(model)
    
    # 3. æ‰¹é‡é¢„æµ‹
    def batch_predict(data_list):
        with torch.no_grad():
            results = []
            for batch in data_list:
                pred = model(batch)
                results.append(pred)
        return results
    
    # 4. ç¼“å­˜å¸¸ç”¨æ•°æ®
    @lru_cache(maxsize=128)
    def cached_predict(data_hash):
        return model.predict(data)
    
    return model
```

### Q17: å†…å­˜ä½¿ç”¨è¿‡é«˜
**é—®é¢˜æè¿°**: ç¨‹åºå ç”¨å†…å­˜è¿‡å¤š

**è§£å†³æ–¹æ¡ˆ**:
```python
import gc
import psutil

def monitor_memory():
    # ç›‘æ§å†…å­˜ä½¿ç”¨
    process = psutil.Process()
    memory_info = process.memory_info()
    print(f"å†…å­˜ä½¿ç”¨: {memory_info.rss / 1024 / 1024:.2f} MB")
    
    # æ¸…ç†å†…å­˜
    gc.collect()
    torch.cuda.empty_cache()
    
    # ä¼˜åŒ–æ•°æ®åŠ è½½
    def efficient_data_loading(file_path):
        # åˆ†å—è¯»å–å¤§æ–‡ä»¶
        chunk_size = 10000
        for chunk in pd.read_csv(file_path, chunksize=chunk_size):
            yield chunk
    
    return memory_info
```

## ğŸ” è°ƒè¯•æŠ€å·§

### Q18: å¦‚ä½•è°ƒè¯•é¢„æµ‹ç»“æœ
**é—®é¢˜æè¿°**: é¢„æµ‹ç»“æœä¸ç¬¦åˆé¢„æœŸ

**è°ƒè¯•æ–¹æ³•**:
```python
def debug_prediction(predictor, data):
    # 1. æ£€æŸ¥è¾“å…¥æ•°æ®
    print("è¾“å…¥æ•°æ®ç»Ÿè®¡:")
    print(data.describe())
    
    # 2. æ£€æŸ¥æ¨¡å‹çŠ¶æ€
    print(f"æ¨¡å‹è®¾å¤‡: {next(predictor.model.parameters()).device}")
    print(f"æ¨¡å‹æ¨¡å¼: {'è®­ç»ƒ' if predictor.model.training else 'è¯„ä¼°'}")
    
    # 3. é€æ­¥é¢„æµ‹
    with torch.no_grad():
        # æ•°æ®é¢„å¤„ç†
        processed_data = predictor.preprocess(data)
        print(f"é¢„å¤„ç†åæ•°æ®å½¢çŠ¶: {processed_data.shape}")
        
        # æ¨¡å‹æ¨ç†
        output = predictor.model(processed_data)
        print(f"æ¨¡å‹è¾“å‡ºå½¢çŠ¶: {output.shape}")
        
        # åå¤„ç†
        final_result = predictor.postprocess(output)
        print(f"æœ€ç»ˆç»“æœå½¢çŠ¶: {final_result.shape}")
    
    return final_result

# 4. å¯è§†åŒ–ä¸­é—´ç»“æœ
def visualize_debug(data, prediction):
    import matplotlib.pyplot as plt
    
    fig, axes = plt.subplots(2, 2, figsize=(12, 8))
    
    # åŸå§‹æ•°æ®
    axes[0,0].plot(data['close'])
    axes[0,0].set_title('åŸå§‹æ•°æ®')
    
    # é¢„æµ‹ç»“æœ
    axes[0,1].plot(prediction['close'])
    axes[0,1].set_title('é¢„æµ‹ç»“æœ')
    
    # æ•°æ®åˆ†å¸ƒ
    axes[1,0].hist(data['close'], bins=50)
    axes[1,0].set_title('åŸå§‹æ•°æ®åˆ†å¸ƒ')
    
    axes[1,1].hist(prediction['close'], bins=50)
    axes[1,1].set_title('é¢„æµ‹æ•°æ®åˆ†å¸ƒ')
    
    plt.tight_layout()
    plt.show()
```

## ğŸ“ è·å–å¸®åŠ©

### å®˜æ–¹èµ„æº
- **GitHub Issues**: [https://github.com/shiyu-coder/Kronos/issues](https://github.com/shiyu-coder/Kronos/issues)
- **è®ºæ–‡**: [arXiv:2508.02739](https://arxiv.org/abs/2508.02739)
- **åœ¨çº¿æ¼”ç¤º**: [Live Demo](https://shiyu-coder.github.io/Kronos-demo/)

### ç¤¾åŒºæ”¯æŒ
- **è®¨è®ºåŒº**: GitHub Discussions
- **æŠ€æœ¯äº¤æµ**: ç›¸å…³æŠ€æœ¯è®ºå›
- **æ–‡æ¡£åé¦ˆ**: é€šè¿‡Issuesæäº¤æ–‡æ¡£æ”¹è¿›å»ºè®®

### æ—¥å¿—æ”¶é›†
```python
import logging

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.DEBUG,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('kronos_debug.log'),
        logging.StreamHandler()
    ]
)

logger = logging.getLogger('kronos')

# åœ¨å…³é”®ä½ç½®æ·»åŠ æ—¥å¿—
logger.info("å¼€å§‹åŠ è½½æ¨¡å‹")
logger.debug(f"æ•°æ®å½¢çŠ¶: {data.shape}")
logger.error(f"é¢„æµ‹å¤±è´¥: {error}")
```

---

**æç¤º**: å¦‚æœä»¥ä¸Šè§£å†³æ–¹æ¡ˆéƒ½æ— æ³•è§£å†³æ‚¨çš„é—®é¢˜ï¼Œè¯·æ”¶é›†è¯¦ç»†çš„é”™è¯¯ä¿¡æ¯å’Œç¯å¢ƒä¿¡æ¯ï¼Œé€šè¿‡GitHub Issueså¯»æ±‚å¸®åŠ©ã€‚