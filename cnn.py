# -*- coding-utf-8 -*-
"""
ã€v19.3 (Cost-Cover) - æˆæœ¬è¦†ç›–ç‰ˆã€‘
æ ¸å¿ƒæ”¹åŠ¨ï¼š
1. ç›®æ ‡å˜æ›´ï¼šé¢„æµ‹äº¤æ˜“æ˜¯å¦èƒ½è¦†ç›–æ‰‹ç»­è´¹å’Œæ»‘ç‚¹ (R > 0.15)ã€‚
2. æ¨¡å‹å‡çº§ï¼šå¼ƒç”¨ LSTMï¼Œæ”¹ç”¨ XGBoost (æ›´é€‚åˆè¡¨æ ¼å‹å°æ ·æœ¬äºŒåˆ†ç±»)ã€‚
3. ç‰¹å¾å¢å¼ºï¼šåŠ å…¥ RSI å’Œ ATR æ³¢åŠ¨ç‡ç‰¹å¾ï¼Œå±•å¹³æ—¶é—´çª—å£ã€‚
"""
import os
import random
import time
import logging
import warnings
import requests
import numpy as np
import pandas as pd
import pandas_ta as ta
from xgboost import XGBClassifier
from sklearn.metrics import accuracy_score, precision_score

# ================= é”æ­»éšæœºæ€§ =================
SEED_VALUE = 42
os.environ["PYTHONHASHSEED"] = str(SEED_VALUE)
random.seed(SEED_VALUE)
np.random.seed(SEED_VALUE)

warnings.filterwarnings("ignore")

# ================= é…ç½®å‚æ•° =================
SYMBOL = "ETHUSDT"
INTERVAL = "15m"

# ä¸ºäº†é¿å…åªåœ¨å•ä¸€å¹´ä»½ä¸Šè¿‡æ‹Ÿåˆï¼Œè¿™é‡Œæ”¹æˆå¤šå¹´ä»½æ•°æ®ï¼Œ
# åé¢æŒ‰â€œå†å²å¹´ä»½è®­ç»ƒã€ç›®æ ‡å¹´ä»½æµ‹è¯•â€çš„æ–¹å¼æ»šåŠ¨è¯„ä¼°ã€‚
START_DATE = "2024-01-01"
END_DATE = "2025-11-20"

# è¶‹åŠ¿ç³»ç»Ÿ
MA_MICRO_PERIOD = 200
EMA_MACRO_PERIOD = 960
BB_STD = 4.0
BB_PERIOD = 20

# --- ã€æ™ºèƒ½åˆ†æµå‚æ•°ã€‘ ---
LARGE_CANDLE_THRESHOLD = 1.5
RETRACEMENT_LEVEL = 0.5

# è¿½è¸ªæ­¢æŸ
INITIAL_SL_ATR = 1.2
TRAILING_ACTIVATION = 1.0
TRAILING_CALLBACK = 1.0

# è®­ç»ƒå‚æ•°
LOOK_FORWARD_TRAIN = 48
# XGBoost ä¸éœ€è¦å¤ªé•¿çš„æ—¶é—´åºåˆ—ï¼Œæˆ‘ä»¬å–æœ€è¿‘ 3 æ ¹ K çº¿çš„ç‰¹å¾å±•å¼€
WINDOW_SIZE = 3

logging.basicConfig(level=logging.INFO, format="%(asctime)s - %(message)s")
logger = logging.getLogger(__name__)


# ================= 1. æ•°æ®è·å– =================
def fetch_binance_klines(symbol, interval, start_str, end_str):
    filename = f"{symbol}_{interval}_{start_str}_{end_str}.csv"
    if os.path.exists(filename):
        try:
            df = pd.read_csv(filename, index_col="timestamp", parse_dates=True)
            return df.sort_index()
        except:
            pass

    logger.info(f"ğŸŒ ä¸‹è½½æ•°æ®: {symbol}...")
    url = "https://api.binance.com/api/v3/klines"
    limit = 1000
    all_data = []
    start_ts = int(pd.to_datetime(start_str).timestamp() * 1000)
    end_ts = int(pd.to_datetime(end_str).timestamp() * 1000)

    while start_ts < end_ts:
        params = {
            "symbol": symbol,
            "interval": interval,
            "startTime": start_ts,
            "endTime": end_ts,
            "limit": limit,
        }
        try:
            r = requests.get(url, params=params, timeout=5)
            data = r.json()
            if not data or not isinstance(data, list):
                break
            all_data.extend(data)
            start_ts = data[-1][0] + 1
            time.sleep(0.05)
        except Exception as e:
            logger.error(f"ä¸‹è½½é”™è¯¯: {e}")
            time.sleep(1)

    if not all_data:
        return pd.DataFrame()

    df = pd.DataFrame(
        all_data,
        columns=[
            "timestamp",
            "Open",
            "High",
            "Low",
            "Close",
            "Volume",
            "x",
            "x",
            "x",
            "x",
            "x",
            "x",
        ],
    )
    df = df.iloc[:, :6]
    df["timestamp"] = pd.to_datetime(df["timestamp"], unit="ms")
    for c in df.columns[1:]:
        df[c] = pd.to_numeric(df[c])
    df = df.set_index("timestamp").sort_index()
    df.to_csv(filename)
    return df


# ================= 2. ç‰¹å¾å·¥ç¨‹ (å¢å¼ºç‰ˆ) =================
def process_data(df):
    logger.info("è®¡ç®—ç‰¹å¾ (XGBoost å¢å¼ºç‰ˆ)...")

    # åŸºç¡€è¶‹åŠ¿
    df["MA_Micro"] = df["Close"].rolling(MA_MICRO_PERIOD).mean()
    df["EMA_Macro"] = ta.ema(df["Close"], length=EMA_MACRO_PERIOD)

    # å¸ƒæ—å¸¦
    bb = ta.bbands(df["Close"], length=BB_PERIOD, std=BB_STD)
    df["BB_Mid"] = bb[f"BBM_{BB_PERIOD}_{BB_STD}"]
    df["bb_width"] = bb[f"BBB_{BB_PERIOD}_{BB_STD}"]

    # æ ¸å¿ƒç‰¹å¾
    df["ADX"] = ta.adx(df["High"], df["Low"], df["Close"], length=14)["ADX_14"]
    df["ATR"] = ta.atr(df["High"], df["Low"], df["Close"], length=14)

    # --- æ–°å¢ç‰¹å¾ ---
    # 1. RSI (åŠ¨é‡)
    df["RSI"] = ta.rsi(df["Close"], length=14)

    # 2. ä»·æ ¼ç›¸å¯¹äºå‡çº¿çš„ä½ç½® (å½’ä¸€åŒ–)
    df["Price_vs_Micro"] = (df["Close"] - df["MA_Micro"]) / (df["MA_Micro"] + 1e-9)

    # 3. ç›¸å¯¹æˆäº¤é‡
    df["relative_volume"] = df["Volume"] / (df["Volume"].rolling(20).mean() + 1e-9)

    # 4. Kçº¿å®ä½“å¤§å°
    df["body_size_norm"] = (df["Close"] - df["Open"]).abs() / (df["ATR"] + 1e-9)

    # 5. æ³¢åŠ¨ç‡è¶‹åŠ¿ (ATR çŸ­æœŸ/é•¿æœŸ)
    df["ATR_Trend"] = df["ATR"] / (df["ATR"].rolling(50).mean() + 1e-9)

    df.dropna(inplace=True)

    # ä¿¡å·é€»è¾‘ (ä¿æŒä¸å˜)
    prev_close = df["Close"].shift(1)
    prev_mid = df["BB_Mid"].shift(1)
    curr_close = df["Close"]
    curr_mid = df["BB_Mid"]

    trend_bullish = (curr_close > df["MA_Micro"]) & (curr_close > df["EMA_Macro"])
    trend_bearish = (curr_close < df["MA_Micro"]) & (curr_close < df["EMA_Macro"])

    df["Rule_Direction"] = 0
    df.loc[
        (prev_close < prev_mid) & (curr_close > curr_mid) & trend_bullish,
        "Rule_Direction",
    ] = 1
    df.loc[
        (prev_close > prev_mid) & (curr_close < curr_mid) & trend_bearish,
        "Rule_Direction",
    ] = -1

    return df


# ================= 3. æ™ºèƒ½äº¤æ˜“æ¨¡æ‹Ÿå™¨ =================
def simulate_smart_trade(direction, open_p, close_p, atr_val, body_norm, highs, lows):
    is_climax = body_norm >= LARGE_CANDLE_THRESHOLD
    mode = "LIMIT" if is_climax else "MARKET"

    entry_price = close_p
    start_idx = 0

    if is_climax:
        target_price = (open_p + close_p) / 2
        filled = False
        check_range = min(3, len(highs))
        for i in range(check_range):
            if direction == 1:
                if lows[i] <= target_price:
                    entry_price = target_price
                    filled = True
                    start_idx = i
                    break
            else:
                if highs[i] >= target_price:
                    entry_price = target_price
                    filled = True
                    start_idx = i
                    break
        if not filled:
            return 0.0, mode

    # æ­¢æŸè®¾ç½®
    if direction == 1:
        current_sl = entry_price - (atr_val * INITIAL_SL_ATR)
        highest_price = entry_price
    else:
        current_sl = entry_price + (atr_val * INITIAL_SL_ATR)
        lowest_price = entry_price

    for i in range(start_idx, len(highs)):
        h = highs[i]
        l = lows[i]

        if direction == 1:
            if l <= current_sl:
                return (current_sl - entry_price) / atr_val, mode
            if h > highest_price:
                highest_price = h
                if (highest_price - entry_price) / atr_val > TRAILING_ACTIVATION:
                    new_sl = highest_price - (atr_val * TRAILING_CALLBACK)
                    if new_sl > current_sl:
                        current_sl = new_sl
        else:
            if h >= current_sl:
                return (entry_price - current_sl) / atr_val, mode
            if l < lowest_price:
                lowest_price = l
                if (entry_price - lowest_price) / atr_val > TRAILING_ACTIVATION:
                    new_sl = lowest_price + (atr_val * TRAILING_CALLBACK)
                    if new_sl < current_sl:
                        current_sl = new_sl

    exit_p = (highs[-1] + lows[-1]) / 2
    if direction == 1:
        r = (exit_p - entry_price) / atr_val
    else:
        r = (entry_price - exit_p) / atr_val

    return r, mode


# ================= 4. æ„å»º XGBoost æ•°æ®é›† =================
def create_dataset_xgb(df):
    # ç‰¹å¾åˆ—è¡¨
    feature_cols = [
        "ADX",
        "bb_width",
        "body_size_norm",
        "Price_vs_Micro",
        "relative_volume",
        "RSI",
        "ATR_Trend",
    ]

    # æå–åŸå§‹æ•°æ®
    raw_data = df[feature_cols].values

    X, y = [], []
    indices, directions = [], []
    real_r_list, mode_list = [], []

    # è½¬æ¢ä¸º Numpy ä»¥åŠ é€Ÿ
    opens = df["Open"].values
    closes = df["Close"].values
    highs = df["High"].values
    lows = df["Low"].values
    atrs = df["ATR"].values
    body_norms = df["body_size_norm"].values
    rule_dirs = df["Rule_Direction"].values

    # éå†
    for i in range(WINDOW_SIZE, len(df) - LOOK_FORWARD_TRAIN):
        d = rule_dirs[i]
        if d == 0:
            continue

        # æ¨¡æ‹Ÿäº¤æ˜“
        w_highs = highs[i + 1 : i + 1 + LOOK_FORWARD_TRAIN]
        w_lows = lows[i + 1 : i + 1 + LOOK_FORWARD_TRAIN]

        r_result, mode = simulate_smart_trade(
            d, opens[i], closes[i], atrs[i], body_norms[i], w_highs, w_lows
        )

        # ã€æ ¸å¿ƒä¿®æ”¹ã€‘ æ ‡ç­¾å®šä¹‰ï¼šæ˜¯å¦è¦†ç›–æ‰‹ç»­è´¹å’Œæ»‘ç‚¹
        # å‡è®¾æ‰‹ç»­è´¹+æ»‘ç‚¹çº¦ 0.15% æ³¢åŠ¨ï¼Œå¯¹åº”çº¦ 0.15 R
        label = 1 if r_result > 0.15 else 0

        # æ„å»ºç‰¹å¾å‘é‡ï¼šå±•å¹³ WINDOW_SIZE çª—å£
        # ä¾‹å¦‚ï¼š[t-2ç‰¹å¾, t-1ç‰¹å¾, tç‰¹å¾] æ‹¼æ¥æˆä¸€ä¸ªé•¿å‘é‡
        window_feat = raw_data[i - WINDOW_SIZE + 1 : i + 1].flatten()

        X.append(window_feat)
        y.append(label)
        indices.append(i)
        directions.append(d)
        real_r_list.append(r_result)
        mode_list.append(mode)

    return (
        np.array(X),
        np.array(y),
        np.array(indices),
        np.array(directions),
        np.array(real_r_list),
        np.array(mode_list),
    )


# ================= 5. ä¸»ç¨‹åº =================
def run():
    df_raw = fetch_binance_klines(SYMBOL, INTERVAL, START_DATE, END_DATE)
    if df_raw.empty:
        print("æ•°æ®ä¸‹è½½å¤±è´¥")
        return

    df = process_data(df_raw)

    # 1. æ„å»º XGBoost ä¸“ç”¨æ•°æ®é›†
    X, y, indices, _, real_r, modes = create_dataset_xgb(df)

    logger.info(f"æ€»æ ·æœ¬æ•°: {len(X)}")
    logger.info(f"æ­£æ ·æœ¬æ¯”ä¾‹ (è¦†ç›–æˆæœ¬): {np.mean(y):.2%}")

    if len(X) < 50:
        print("æ ·æœ¬ä¸è¶³")
        return

    # å°†æ ·æœ¬ç´¢å¼•æ˜ å°„å›æ—¶é—´ï¼Œç”¨äºæŒ‰å¹´ä»½æ»šåŠ¨è¯„ä¼°
    years = df.index[indices].year

    # ç»Ÿè®¡æˆ˜æŠ¥è¾…åŠ©å‡½æ•°
    from collections import Counter

    def print_stats(r_vals, m_vals, title):
        if len(r_vals) == 0:
            print(f"\n>>>>>> {title} (æ— æ•°æ®) <<<<<<")
            return

        r_arr = np.array(r_vals)
        win_rate = np.mean(r_arr > 0)  # ç»å¯¹ç›ˆåˆ©ç‡
        cover_rate = np.mean(r_arr > 0.15)  # è¦†ç›–æˆæœ¬æ¯”ä¾‹ (R > 0.15)

        print(f"\n>>>>>> {title} <<<<<<")
        print(
            f"  äº¤æ˜“æ•°: {len(r_arr)} | èƒœç‡(>0): {win_rate:.1%} | æˆæœ¬è¦†ç›–ç‡(>0.15): {cover_rate:.1%}"
        )
        print(f"  æ€»æ”¶ç›Š: {np.sum(r_arr):.2f} R | å¹³å‡æœŸæœ›: {np.mean(r_arr):.3f} R")

        m_counts = Counter(m_vals)
        print(f"  æ¨¡å¼åˆ†å¸ƒ: Market={m_counts['MARKET']}, Limit={m_counts['LIMIT']}")

    print("\n" + "=" * 60)
    print(f"ã€æˆ˜æŠ¥ V19.3ã€‘ XGBoost æˆæœ¬è¦†ç›–ç‰ˆï¼ˆå¤šå¹´ä»½æ»šåŠ¨è¯„ä¼°ï¼‰")
    print(f"ç›®æ ‡: é¢„æµ‹ R > 0.15 (è¦†ç›–æ‰‹ç»­è´¹+æ»‘ç‚¹)")
    print("=" * 60)

    threshold = 0.5
    all_ai_r = []
    all_ai_modes = []

    for year in sorted(set(years)):
        # ä½¿ç”¨â€œå†å²å¹´ä»½ < yearâ€ä½œä¸ºè®­ç»ƒï¼Œå½“å¹´ == year ä½œä¸ºæµ‹è¯•
        train_mask = years < year
        test_mask = years == year

        if np.sum(train_mask) < 50 or np.sum(test_mask) == 0:
            continue

        X_train, X_test = X[train_mask], X[test_mask]
        y_train, y_test = y[train_mask], y[test_mask]
        r_test = real_r[test_mask]
        mode_test = modes[test_mask]

        # è®­ç»ƒ XGBoostï¼ˆæ¯ä¸ªå¹´ä»½ç‹¬ç«‹ä¸€å¥—æ¨¡å‹ï¼‰
        print(f"\nè®­ç»ƒå¹´ä»½ < {year} çš„æ¨¡å‹ï¼Œç”¨äºè¯„ä¼° {year} å¹´...")
        ratio = (len(y_train) - np.sum(y_train)) / (np.sum(y_train) + 1e-9)

        model = XGBClassifier(
            n_estimators=100,
            max_depth=4,
            learning_rate=0.05,
            subsample=0.8,
            colsample_bytree=0.8,
            scale_pos_weight=ratio,
            eval_metric="logloss",
            use_label_encoder=False,
            random_state=SEED_VALUE,
            n_jobs=-1,
        )

        model.fit(X_train, y_train)

        y_pred = model.predict(X_test)
        y_prob = model.predict_proba(X_test)[:, 1]
        acc = accuracy_score(y_test, y_pred)
        print(f"{year} å¹´æµ‹è¯•é›†å‡†ç¡®ç‡: {acc:.2%}")

        # å½“å¹´åŸºå‡†è¡¨ç°ï¼ˆè§„åˆ™ç›˜å…¨é‡ä¿¡å·ï¼‰
        print_stats(r_test, mode_test, f"{year} å¹´åŸºå‡† (å…¨é‡ä¿¡å·)")

        # AI ä¼˜é€‰ï¼šProb > é˜ˆå€¼
        ai_mask = y_prob > threshold
        ai_r = r_test[ai_mask]
        ai_modes = mode_test[ai_mask]

        print_stats(ai_r, ai_modes, f"{year} å¹´ AI ä¼˜é€‰ (Prob > {threshold})")
        print("-" * 60)

        all_ai_r.extend(list(ai_r))
        all_ai_modes.extend(list(ai_modes))

    # æ±‡æ€»æ‰€æœ‰å¹´ä»½çš„ AI ä¼˜é€‰è¡¨ç°
    if all_ai_r:
        print("\n" + "=" * 60)
        print("ã€å¤šå¹´ä»½æ±‡æ€»ã€‘AI ä¼˜é€‰ä¿¡å·è¡¨ç° (æ‰€æœ‰æµ‹è¯•å¹´ä»½åˆå¹¶)")
        print("=" * 60)
        print_stats(np.array(all_ai_r), np.array(all_ai_modes), "AI ä¼˜é€‰ (å…¨éƒ¨å¹´ä»½)")

    # ç‰¹å¾é‡è¦æ€§ï¼šæœ€åä¸€æ¬¡è®­ç»ƒçš„æ¨¡å‹ï¼ˆæœ€æ–°å¹´ä»½çš„æ¨¡å‹ï¼‰
    # ç”±äºçª—å£å±•å¹³ï¼Œæ„é€ å¯¹åº”çš„ç‰¹å¾å
    print("\n[ç‰¹å¾é‡è¦æ€§ Top 5] ï¼ˆä»¥æœ€åä¸€æ¬¡è®­ç»ƒçš„æ¨¡å‹ä¸ºä¾‹ï¼‰")
    cols = [
        "ADX",
        "bb_width",
        "body_size_norm",
        "Price_vs_Micro",
        "relative_volume",
        "RSI",
        "ATR_Trend",
    ]
    all_feat_names = []
    for w in range(WINDOW_SIZE):
        for c in cols:
            all_feat_names.append(f"{c}_t-{WINDOW_SIZE-1-w}")

    imps = model.feature_importances_
    sorted_idx = np.argsort(imps)[::-1]
    for i in sorted_idx[:5]:
        print(f"  {all_feat_names[i]}: {imps[i]:.4f}")


if __name__ == "__main__":
    run()
