import numpy as np
import pandas as pd
import requests
import time
import logging

# Temporary NumPy 2.0 compatibility for pandas_ta (expects numpy.NaN/Inf)
# pandas_ta<0.4 uses `from numpy import NaN`, which NumPy 2.0 removed.
# Ensure these aliases exist before importing pandas_ta.
if not hasattr(np, "NaN"):
    np.NaN = np.nan  # alias for backward compatibility
if not hasattr(np, "Inf"):
    np.Inf = np.inf  # alias for backward compatibility

import pandas_ta as ta
# Prefer pandas_ta's pure-Python backend; disable optional TA-Lib to avoid
# NumPy 2.x binary-ABI issues with compiled talib wheels.
try:
    if hasattr(ta, "Imports"):
        ta.Imports["talib"] = False
except Exception:
    pass
import os
import joblib
import lightgbm as lgb
from sklearn.preprocessing import MinMaxScaler
from sklearn.metrics import (
    accuracy_score,
    precision_score,
    recall_score,
    f1_score,
    precision_recall_curve,
    confusion_matrix,
)
from tqdm import tqdm

# --- 0. è®¾ç½® ---
logging.basicConfig(
    level=logging.INFO, format="%(asctime)s - %(levelname)s - %(message)s"
)
logger = logging.getLogger(__name__)

# --- ğŸš€ å…¨å±€é…ç½® ---
SYMBOL = "ETHUSDT"
INTERVAL = "1h"
DATA_START_DATE = "2017-01-01"
TRAIN_START = "2018-01-01"
VALIDATION_START = "2024-01-01"
TEST_START = "2025-01-01"
TEST_END = "2025-11-17"
LOOK_BACK = 60
TREND_CONFIG = {"look_forward_steps": 5, "ema_length": 8}

# --- å®éªŒ/å®‰å…¨é€‰é¡¹ ---
# ä½¿ç”¨ä»…åŸºäºå†å²çª—å£çš„â€œå¸‚åœºç»“æ„â€ç‰¹å¾ï¼ˆåœ¨çº¿ã€æ— å‰è§†ï¼‰ï¼›
# è‹¥è®¾ä¸º False åˆ™å®Œå…¨ç¦ç”¨è¯¥ç‰¹å¾ã€‚
USE_TRAILING_MARKET_STRUCTURE = True
# å›æµ‹æ—¶æ˜¯å¦é¢å¤–è¿è¡Œä¸€æ¬¡ï¼šå…³é—­ MACD è¿‡æ»¤åšå¯¹ç…§
RUN_ABLATION_NO_MACD_FILTER = True
# æ˜¯å¦å¯ç”¨åŸºäº LightGBM çš„ç®€å•ç‰¹å¾é€‰æ‹©ï¼Œé™ä½é«˜ç»´é£é™©
ENABLE_FEATURE_SELECTION = True
# é€‰ä¿ç•™çš„ç‰¹å¾é‡è¦æ€§åˆ†ä½ï¼ˆç™¾åˆ†æ¯”ï¼‰ï¼Œæ•°å€¼è¶Šå°ä¿ç•™è¶Šå¤šç‰¹å¾
FEATURE_IMPORTANCE_KEEP_PERCENTILE = 50  # ä¿ç•™å‰ 50% é‡è¦æ€§
# æ˜¯å¦åœ¨å›æµ‹ä¸­ä¸¥æ ¼ä½¿ç”¨â€œåœ¨çº¿ï¼ˆexpandingï¼‰è®¡ç®—â€ï¼Œå®Œå…¨é¿å…ä¸€æ¬¡æ€§é¢„è®¡ç®—
STRICT_ONLINE_EVAL = True

# MACD è¿‡æ»¤é€»è¾‘å‚æ•°ï¼ˆæ›´å®½æ¾ä¸”å¯é…ç½®ï¼‰
MACD_FILTER_MODE = "hist_pos_or_rising"  # å¯é€‰ï¼š"strict", "hist_pos", "hist_pos_or_rising", "recent_cross"
MACD_RECENT_CROSS_LOOKBACK = 3  # recent_cross æ¨¡å¼ä¸‹å…è®¸æœ€è¿‘å¤šå°‘æ ¹å†…ä¸Šç©¿
USE_ADX_GATE = False  # å¯é€‰æ˜¯å¦è”åŠ¨ ADX
ADX_THRESHOLD = 18.0

# --- æ–‡ä»¶è·¯å¾„ ---
MODELS_DIR, DATA_DIR = "models_gbm2", "data"
# Using v6 to denote the final, warning-free version
MODEL_SAVE_PATH = os.path.join(
    MODELS_DIR, f"{SYMBOL.lower()}_model_high_precision_v6_{INTERVAL}.joblib"
)
SCALER_SAVE_PATH = os.path.join(
    MODELS_DIR, f"{SYMBOL.lower()}_scaler_high_precision_v6_{INTERVAL}.joblib"
)
FEATURE_COLUMNS_PATH = os.path.join(
    MODELS_DIR, f"{SYMBOL.lower()}_feature_columns_high_precision_v6_{INTERVAL}.joblib"
)
FLATTENED_COLUMNS_PATH = os.path.join(
    MODELS_DIR,
    f"{SYMBOL.lower()}_flattened_columns_high_precision_v6_{INTERVAL}.joblib",
)
# è‹¥å¯ç”¨ç‰¹å¾é€‰æ‹©ï¼Œä¿å­˜é€‰æ‹©åçš„åˆ—å
SELECTED_FLATTENED_COLUMNS_PATH = os.path.join(
    MODELS_DIR,
    f"{SYMBOL.lower()}_selected_flattened_columns_high_precision_v6_{INTERVAL}.joblib",
)
DATA_CACHE_PATH = os.path.join(DATA_DIR, f"{SYMBOL.lower()}_{INTERVAL}_data.csv")

os.makedirs(MODELS_DIR, exist_ok=True)
os.makedirs(DATA_DIR, exist_ok=True)


# --- æ•°æ®è·å–å‡½æ•° ---
def fetch_binance_klines(s, i, st, en=None, l=1000):
    url, cols = "https://api.binance.com/api/v3/klines", [
        "timestamp",
        "Open",
        "High",
        "Low",
        "Close",
        "Volume",
    ]
    sts, ets = int(pd.to_datetime(st).timestamp() * 1000), (
        int(pd.to_datetime(en).timestamp() * 1000) if en else int(time.time() * 1000)
    )
    all_d, retries, max_retries = [], 0, 5
    while sts < ets:
        try:
            r = requests.get(
                url,
                params={
                    "symbol": s.upper(),
                    "interval": i,
                    "startTime": sts,
                    "endTime": ets,
                    "limit": l,
                },
                timeout=15,
            )
            r.raise_for_status()
            d = r.json()
            if not d:
                break
            all_d.extend(d)
            sts = d[-1][0] + 1
            retries = 0
        except requests.exceptions.RequestException as e:
            retries += 1
            if retries > max_retries:
                logger.error(f"è·å–æ•°æ®å¤±è´¥è¶…è¿‡æœ€å¤§é‡è¯•æ¬¡æ•°: {e}")
                return pd.DataFrame()
            logger.warning(
                f"è·å–æ•°æ®å¤±è´¥ï¼Œæ­£åœ¨é‡è¯• ({retries}/{max_retries})... Error: {e}"
            )
            time.sleep(retries * 2)
    if not all_d:
        return pd.DataFrame()
    df = pd.DataFrame(all_d, columns=[*cols, "c1", "c2", "c3", "c4", "c5", "c6"])[
        ["timestamp", "Open", "High", "Low", "Close", "Volume"]
    ].copy()
    df["timestamp"] = pd.to_datetime(df["timestamp"], unit="ms")
    for col in df.columns[1:]:
        df[col] = pd.to_numeric(df[col], errors="coerce")
    logger.info(f"âœ… è·å– {s} æ•°æ®æˆåŠŸ: {len(df)} æ¡")
    return df.set_index("timestamp").sort_index()


# --- ğŸš€ 1. å‡çº§ç‰ˆç‰¹å¾å·¥ç¨‹ (å½»åº•ä¿®å¤å‰è§†åå·® & è­¦å‘Š) ---


def get_market_structure_features_trailing(df, window=5):
    """åœ¨çº¿ã€ä»…ä¾èµ–è¿‡å»æ•°æ®çš„å¸‚åœºç»“æ„è¿‘ä¼¼ã€‚
    ä½¿ç”¨æ»šåŠ¨é«˜ç‚¹/ä½ç‚¹å˜åŒ–æ¥åº¦é‡è¶‹åŠ¿å¼ºå¼±ï¼Œä¸ä½¿ç”¨ find_peaksï¼ˆé¿å…å‰è§†ï¼‰ã€‚
    """
    df_copy = df.copy()
    rolling_high = df_copy["High"].rolling(window=window, min_periods=1).max()
    rolling_low = df_copy["Low"].rolling(window=window, min_periods=1).min()
    is_higher_high = rolling_high > rolling_high.shift(1)
    is_higher_low = rolling_low > rolling_low.shift(1)
    is_lower_high = rolling_high < rolling_high.shift(1)
    is_lower_low = rolling_low < rolling_low.shift(1)
    df_copy["is_uptrend"] = (is_higher_high & is_higher_low).astype(int)
    df_copy["is_downtrend"] = (is_lower_high & is_lower_low).astype(int)
    df_copy["market_structure"] = df_copy["is_uptrend"] - df_copy["is_downtrend"]
    return df_copy[["market_structure"]]


def feature_engineering(df, verbose=True):
    df = df.copy()
    if verbose:
        logger.info("--- å¼€å§‹è®¡ç®—ç‰¹å¾ (V6 - Final Causal & Warning-Free) ---")

    # æŒ‡æ ‡è®¡ç®— + å®¹é”™ï¼šç¡®ä¿å…³é”®åˆ—å­˜åœ¨ï¼ˆå³ä½¿å‰æœŸçª—å£ä¸è¶³ä¹Ÿåˆ›å»ºä¸º NaNï¼‰
    # RSI
    if len(df) >= 14:
        df.ta.rsi(length=14, append=True)
    if "RSI_14" not in df.columns:
        df["RSI_14"] = np.nan

    # MACD(12,26,9)
    try:
        if len(df) >= (26 + 9):
            df.ta.macd(fast=12, slow=26, signal=9, append=True)
    except Exception:
        pass
    for col in ["MACD_12_26_9", "MACDh_12_26_9", "MACDs_12_26_9"]:
        if col not in df.columns:
            df[col] = np.nan

    df.ta.bbands(length=20, std=2, append=True)

    df.ta.adx(length=14, append=True)
    if "ADX_14" not in df.columns:
        df["ADX_14"] = np.nan

    df.ta.atr(length=14, append=True)
    df.ta.obv(append=True)
    df["volatility_log_ret"] = (
        (np.log(df["Close"] / df["Close"].shift(1))).rolling(window=20).std()
    )
    # åœ¨çº¿ã€æ— å‰è§†çš„å¸‚åœºç»“æ„ï¼ˆå¯å…³é—­ï¼‰
    if USE_TRAILING_MARKET_STRUCTURE:
        market_structure_df = get_market_structure_features_trailing(df, window=5)
    else:
        market_structure_df = pd.DataFrame(index=df.index)
    if len(df) >= (52 + 18):
        try:
            df.ta.macd(
                fast=24,
                slow=52,
                signal=18,
                append=True,
                col_names=("MACD_long", "MACDh_long", "MACDs_long"),
            )
        except Exception:
            pass
    for col in ["MACD_long", "MACDh_long", "MACDs_long"]:
        if col not in df.columns:
            df[col] = np.nan
    df["ret_1"] = df["Close"].pct_change(1)
    df["ret_4"] = df["Close"].pct_change(4)
    df["ret_16"] = df["Close"].pct_change(16)
    df["rsi_delta_1"] = df["RSI_14"].diff(1)
    if "MACDh_12_26_9" not in df.columns:
        df["MACDh_12_26_9"] = np.nan
    df["macd_delta_1"] = df["MACDh_12_26_9"].diff(1)
    all_features_df = df.drop(columns=["Open", "High", "Low", "Close", "Volume"])
    all_features_df = pd.concat([all_features_df, market_structure_df], axis=1)
    all_features_df.replace([np.inf, -np.inf], np.nan, inplace=True)
    if verbose:
        logger.info(f"ç‰¹å¾è®¡ç®—å®Œæˆï¼Œå…± {all_features_df.shape[1]} ä¸ªç‰¹å¾ã€‚")
    return all_features_df


# --- æ ‡ç­¾ä¸åºåˆ—å‡½æ•° ---
def create_trend_labels(df, look_forward_steps=5, ema_length=8):
    df_copy = df.copy()
    df_copy.ta.ema(length=ema_length, close=df_copy["Close"], append=True)
    future_ema = df_copy[f"EMA_{ema_length}"].shift(-look_forward_steps)
    df_copy["label"] = (future_ema > df_copy[f"EMA_{ema_length}"]).astype(int)
    return df_copy


def create_flattened_sequences(data, labels, look_back=60):
    X, y = [], []
    if len(data) <= look_back:
        return np.array([], dtype=np.float32), np.array([], dtype=np.int32)
    for i in range(len(data) - look_back):
        X.append(data[i : (i + look_back), :].flatten())
        y.append(labels[i + look_back])
    return np.array(X, dtype=np.float32), np.array(y, dtype=np.int32)


# --- è®­ç»ƒä¸éªŒè¯å‡½æ•° ---
def train_and_validate(train_df, validation_df, look_back, trend_config):
    logger.info("--- å¼€å§‹è®­ç»ƒå’ŒéªŒè¯æµç¨‹ (V6 - Final Causal & Warning-Free) ---")
    X_train_full_df = feature_engineering(train_df).dropna()
    X_validation_full_df_raw = feature_engineering(validation_df)
    train_labeled = create_trend_labels(
        train_df.loc[X_train_full_df.index], **trend_config
    )
    y_train_full = train_labeled["label"]
    common_cols = X_train_full_df.columns
    X_validation_full_df = X_validation_full_df_raw[common_cols].dropna()
    validation_labeled = create_trend_labels(
        validation_df.loc[X_validation_full_df.index], **trend_config
    )
    y_validation_full = validation_labeled["label"]

    scaler = MinMaxScaler(feature_range=(0, 1))
    X_train_scaled = scaler.fit_transform(X_train_full_df)
    X_validation_scaled = scaler.transform(X_validation_full_df)

    original_columns = X_train_full_df.columns.tolist()
    flattened_columns = [
        f"{col}_lag_{lag}"
        for lag in range(look_back - 1, -1, -1)
        for col in original_columns
    ]
    # æŒä¹…åŒ–åŸºåº•åˆ—ä¸å±•å¼€åçš„å…¨éƒ¨åˆ—
    joblib.dump(original_columns, FEATURE_COLUMNS_PATH)
    joblib.dump(flattened_columns, FLATTENED_COLUMNS_PATH)

    X_train_np, y_train = create_flattened_sequences(
        X_train_scaled, y_train_full.values, look_back
    )
    X_validation_np, y_validation = create_flattened_sequences(
        X_validation_scaled, y_validation_full.values, look_back
    )
    if len(X_train_np) == 0:
        logger.error("åˆ›å»ºåºåˆ—åæ²¡æœ‰è¶³å¤Ÿçš„è®­ç»ƒæ•°æ®ã€‚")
        return None, None, None

    X_train_df_seq = pd.DataFrame(X_train_np, columns=flattened_columns)
    X_validation_df_seq = pd.DataFrame(X_validation_np, columns=flattened_columns)

    train_label_counts = np.bincount(y_train)
    if len(train_label_counts) < 2 or train_label_counts[1] == 0:
        logger.error("è®­ç»ƒæ•°æ®ä¸­æ²¡æœ‰æ­£æ ·æœ¬(label=1)ã€‚")
        return None, None, None

    scale_pos_weight = (train_label_counts[0] / train_label_counts[1]) * 0.7
    logger.info(f"è°ƒæ•´åçš„ scale_pos_weight: {scale_pos_weight:.2f}")

    lgb_params = {
        "objective": "binary",
        "metric": "logloss",
        "n_estimators": 2000,
        "learning_rate": 0.02,
        "num_leaves": 20,
        "max_depth": 5,
        "seed": 42,
        "n_jobs": -1,
        "verbose": -1,
        "scale_pos_weight": scale_pos_weight,
        "colsample_bytree": 0.7,
        "subsample": 0.7,
        "reg_alpha": 0.1,
    }
    lgb_model = lgb.LGBMClassifier(**lgb_params)
    logger.info("\nå¼€å§‹è®­ç»ƒ LightGBM æ¨¡å‹ (å…¨ç‰¹å¾)...")
    lgb_model.fit(
        X_train_df_seq,
        y_train,
        eval_set=[(X_validation_df_seq, y_validation)],
        eval_metric="logloss",
        callbacks=[lgb.early_stopping(stopping_rounds=100, verbose=False)],
    )

    def _best_threshold(y_true, y_prob):
        precisions, recalls, thresholds = precision_recall_curve(y_true, y_prob)
        f1_scores = np.divide(
            2 * recalls * precisions,
            recalls + precisions,
            out=np.zeros_like(recalls),
            where=(recalls + precisions) != 0,
        )
        best_idx = np.argmax(f1_scores)
        return thresholds[best_idx] if len(thresholds) > best_idx else 0.5

    y_val_pred_probs = lgb_model.predict_proba(X_validation_df_seq)[:, 1]
    best_f1_threshold = _best_threshold(y_validation, y_val_pred_probs)
    logger.info(f"åœ¨éªŒè¯é›†ä¸Š(å…¨ç‰¹å¾)æœ€ä½³F1é˜ˆå€¼: {best_f1_threshold:.4f}")

    selected_columns = None
    if ENABLE_FEATURE_SELECTION:
        importances = lgb_model.feature_importances_
        # ä¿ç•™å‰ç™¾åˆ†ä½çš„ç‰¹å¾
        thr = np.percentile(importances, FEATURE_IMPORTANCE_KEEP_PERCENTILE)
        keep_mask = importances >= thr
        # å…œåº•ï¼šè‡³å°‘ä¿ç•™ 200 æˆ– 20%ï¼ˆå–è¾ƒå°ï¼‰ï¼Œä½†ä¸å°‘äº 64
        if keep_mask.sum() < 64:
            order = np.argsort(importances)[::-1]
            min_keep = max(64, int(0.2 * len(importances)))
            min_keep = min(min_keep, 200)
            keep_mask[:] = False
            keep_mask[order[:min_keep]] = True
        selected_columns = [c for c, k in zip(flattened_columns, keep_mask) if k]
        logger.info(
            f"ç‰¹å¾é€‰æ‹©: ä» {len(flattened_columns)} -> {len(selected_columns)} åˆ—"
        )

        # ä½¿ç”¨é€‰æ‹©åçš„ç‰¹å¾é‡æ–°è®­ç»ƒå¹¶åŸºäºéªŒè¯é›†ç›‘æ§
        X_train_sel = X_train_df_seq[selected_columns]
        X_valid_sel = X_validation_df_seq[selected_columns]
        lgb_model = lgb.LGBMClassifier(**lgb_params)
        logger.info("\nå¼€å§‹è®­ç»ƒ LightGBM æ¨¡å‹ (é€‰æ‹©åç‰¹å¾)...")
        lgb_model.fit(
            X_train_sel,
            y_train,
            eval_set=[(X_valid_sel, y_validation)],
            eval_metric="logloss",
            callbacks=[lgb.early_stopping(stopping_rounds=100, verbose=False)],
        )
        y_val_pred_probs = lgb_model.predict_proba(X_valid_sel)[:, 1]
        best_f1_threshold = _best_threshold(y_validation, y_val_pred_probs)
        logger.info(
            f"åœ¨éªŒè¯é›†ä¸Š(é€‰æ‹©åç‰¹å¾)æœ€ä½³F1é˜ˆå€¼: {best_f1_threshold:.4f}"
        )
        # æŒä¹…åŒ–é€‰æ‹©ç»“æœ
        joblib.dump(selected_columns, SELECTED_FLATTENED_COLUMNS_PATH)
    else:
        # æ¸…ç†å†å²é€‰æ‹©æ–‡ä»¶ï¼ˆè‹¥æœ‰ï¼‰ä»¥é¿å…è¯¯ç”¨
        try:
            if os.path.exists(SELECTED_FLATTENED_COLUMNS_PATH):
                os.remove(SELECTED_FLATTENED_COLUMNS_PATH)
        except Exception:
            pass

    joblib.dump(lgb_model, MODEL_SAVE_PATH)
    joblib.dump(scaler, SCALER_SAVE_PATH)
    logger.info(f"æ¨¡å‹å’ŒScalerå·²ä¿å­˜åˆ°: {MODELS_DIR}")
    return lgb_model, scaler, best_f1_threshold


# --- å›æµ‹è¯„ä¼°å‡½æ•° ---
def run_backtest_and_evaluate(
    test_df,
    model,
    scaler,
    look_back,
    threshold,
    trend_config,
    apply_macd_filter=True,
    strict_online=True,
):
    logger.info(
        "\n" + "=" * 60 + "\n--- å¼€å§‹åœ¨æµ‹è¯•é›†ä¸Šè¿›è¡Œä¸¥æ ¼çš„å›æµ‹è¯„ä¼° ---\n" + "=" * 60
    )
    original_columns = joblib.load(FEATURE_COLUMNS_PATH)
    flattened_columns_full = joblib.load(FLATTENED_COLUMNS_PATH)
    # è‹¥å­˜åœ¨é€‰æ‹©åçš„åˆ—ï¼Œåˆ™æŒ‰å…¶è¿›è¡Œæ¨ç†ï¼›å¦åˆ™ä½¿ç”¨å…¨é‡åˆ—
    if os.path.exists(SELECTED_FLATTENED_COLUMNS_PATH):
        selected_columns = joblib.load(SELECTED_FLATTENED_COLUMNS_PATH)
    else:
        selected_columns = flattened_columns_full

    final_signals = []
    signal_timestamps = []

    def macd_filter_ok(prefix_features: pd.DataFrame) -> bool:
        try:
            macd = prefix_features["MACD_long"].iloc[-1]
            macds = prefix_features["MACDs_long"].iloc[-1]
            macdh = prefix_features["MACDh_long"].iloc[-1]
            if np.isnan(macd) or np.isnan(macds) or np.isnan(macdh):
                return False
            ok = False
            if MACD_FILTER_MODE == "strict":
                ok = (macd > macds) and (macd > 0)
            elif MACD_FILTER_MODE == "hist_pos":
                ok = macdh > 0
            elif MACD_FILTER_MODE == "hist_pos_or_rising":
                prev_macdh = prefix_features["MACDh_long"].iloc[-2] if len(prefix_features) > 1 else np.nan
                slope = macd - (prefix_features["MACD_long"].iloc[-2] if len(prefix_features) > 1 else macd)
                ok = (macdh > 0) or ((macd > macds) and (slope > 0) and (not np.isnan(prev_macdh) and macdh >= prev_macdh))
            elif MACD_FILTER_MODE == "recent_cross":
                window = prefix_features.tail(MACD_RECENT_CROSS_LOOKBACK + 1)
                crossed = False
                if len(window) >= 2:
                    prev = window.iloc[:-1]
                    crossed = ((prev["MACD_long"] <= prev["MACDs_long"]).any()) and (macd > macds)
                ok = crossed
            else:
                ok = (macd > macds)
            if USE_ADX_GATE and "ADX_14" in prefix_features.columns:
                adx = prefix_features["ADX_14"].iloc[-1]
                if np.isnan(adx) or adx < ADX_THRESHOLD:
                    return False
            return bool(ok)
        except Exception:
            return False

    if strict_online:
        logger.info("é€æ ¹Kçº¿ä¸¥æ ¼åœ¨çº¿ï¼ˆexpandingï¼‰è¯„ä¼°...")
        from collections import deque
        scaled_buffer = deque(maxlen=look_back)

        # éå†æµ‹è¯•æ•°æ®ï¼Œé€æ­¥æ‰©å±•çª—å£ï¼Œä»…ä½¿ç”¨å†å²ä¿¡æ¯
        for end in tqdm(range(1, len(test_df) + 1)):
            prefix_df = test_df.iloc[:end]
            prefix_features = feature_engineering(prefix_df, verbose=False)
            # å¯¹é½å¹¶ä»…å–æœ€åä¸€è¡Œä½œä¸ºå½“å‰æ—¶åˆ»çš„ç‰¹å¾
            prefix_aligned = prefix_features.reindex(columns=original_columns, fill_value=0)
            if prefix_aligned.empty:
                continue
            last_row = prefix_aligned.iloc[[-1]]
            # ä»¥ DataFrame å½¢å¼ä¼ å…¥ï¼Œé¿å… sklearn å…³äºç¼ºå°‘åˆ—åçš„è­¦å‘Š
            scaled_last = scaler.transform(last_row)
            scaled_buffer.append(scaled_last.reshape(-1))

            if len(scaled_buffer) < look_back:
                continue

            # æ„å»ºæ¨¡å‹è¾“å…¥çš„å±•å¼€åºåˆ—ï¼ˆlook_back x n_features -> 1 x (look_back*n_features)ï¼‰
            seq_matrix = np.vstack(list(scaled_buffer))
            input_flat = seq_matrix.flatten().reshape(1, -1)
            input_df_full = pd.DataFrame(input_flat, columns=flattened_columns_full)
            input_df = input_df_full[selected_columns]

            pred_prob = model.predict_proba(input_df)[0][1]
            model_signal = 1 if pred_prob > threshold else 0

            if apply_macd_filter:
                is_trend_confirmed = macd_filter_ok(prefix_features)
                final_signal = 1 if (model_signal == 1 and is_trend_confirmed) else 0
            else:
                final_signal = model_signal

            final_signals.append(final_signal)
            signal_timestamps.append(prefix_df.index[-1])
    else:
        logger.info("æ‰¹é‡é¢„è®¡ç®—ç‰¹å¾çš„å¿«é€Ÿè¯„ä¼°ï¼ˆä»ä½¿ç”¨å‰ç¼€è¿‡æ»¤ï¼‰...")
        test_features_df = feature_engineering(test_df, verbose=True).dropna()
        test_features_aligned = test_features_df.reindex(
            columns=original_columns, fill_value=0
        )
        test_scaled = scaler.transform(test_features_aligned)

        for i in tqdm(range(look_back, len(test_scaled))):
            current_timestamp = test_features_df.index[i]
            input_sequence = test_scaled[i - look_back : i, :].flatten().reshape(1, -1)
            input_df_full = pd.DataFrame(input_sequence, columns=flattened_columns_full)
            input_df = input_df_full[selected_columns]
            pred_prob = model.predict_proba(input_df)[0][1]
            model_signal = 1 if pred_prob > threshold else 0
            if apply_macd_filter:
                # ä½¿ç”¨ä»…åˆ° i çš„å‰ç¼€è¿›è¡Œè¿‡æ»¤åˆ¤æ–­
                prefix_features = test_features_df.iloc[: i + 1]
                is_trend_confirmed = macd_filter_ok(prefix_features)
                final_signal = 1 if (model_signal == 1 and is_trend_confirmed) else 0
            else:
                final_signal = model_signal
            final_signals.append(final_signal)
            signal_timestamps.append(current_timestamp)

    actual_labels_df = create_trend_labels(test_df, **trend_config).dropna()
    pred_series = pd.Series(final_signals, index=pd.Index(signal_timestamps))
    results_df = pd.DataFrame(actual_labels_df["label"]).join(
        pred_series.to_frame("final_signal"), how="inner"
    )

    if results_df.empty or np.sum(results_df["final_signal"]) == 0:
        logger.warning("å›æµ‹æœŸé—´æ²¡æœ‰äº§ç”Ÿä»»ä½•äº¤æ˜“ä¿¡å·ã€‚")
        return

    y_test_actual = results_df["label"].values
    y_pred_final = results_df["final_signal"].values
    title_suffix = "(åº”ç”¨MACDè¿‡æ»¤å™¨)" if apply_macd_filter else "(æ— MACDè¿‡æ»¤)"
    print(f"\n--- [å®¢è§‚] æµ‹è¯•é›†å›æµ‹è¯„ä¼°ç»“æœ {title_suffix} ---")
    print(f"æ€»å›æµ‹Kçº¿æ•°: {len(y_pred_final)}")
    print(f"å‘å‡ºçœ‹æ¶¨ä¿¡å·æ€»æ¬¡æ•°: {np.sum(y_pred_final)}")
    if np.sum(y_pred_final) > 0:
        print(f"ç²¾ç¡®ç‡ (èƒœç‡): {precision_score(y_test_actual, y_pred_final):.4f}")
        print(f"å¬å›ç‡: {recall_score(y_test_actual, y_pred_final):.4f}")
        print(
            "\næ··æ·†çŸ©é˜µ (TN, FP / FN, TP):\n",
            confusion_matrix(y_test_actual, y_pred_final),
        )


# --- ä¸»æµç¨‹ ---
if __name__ == "__main__":
    if os.path.exists(DATA_CACHE_PATH):
        logger.info(f"ä»ç¼“å­˜åŠ è½½æ•°æ®: {DATA_CACHE_PATH}")
        raw_df = pd.read_csv(DATA_CACHE_PATH, index_col=0, parse_dates=True)
    else:
        raw_df = fetch_binance_klines(
            s=SYMBOL, i=INTERVAL, st=DATA_START_DATE, en=TEST_END
        )
        if not raw_df.empty:
            raw_df.to_csv(DATA_CACHE_PATH)

    if raw_df.empty:
        logger.error("æ•°æ®ä¸ºç©ºï¼Œç¨‹åºé€€å‡ºã€‚")
        exit()

    train_df = raw_df[(raw_df.index >= TRAIN_START) & (raw_df.index < VALIDATION_START)]
    validation_df = raw_df[
        (raw_df.index >= VALIDATION_START) & (raw_df.index < TEST_START)
    ]
    test_df = raw_df[(raw_df.index >= TEST_START) & (raw_df.index <= TEST_END)]

    logger.info(
        f"æ•°æ®é›†åˆ’åˆ†å®Œæˆ: {len(train_df)} è®­ç»ƒ, {len(validation_df)} éªŒè¯, {len(test_df)} æµ‹è¯•ã€‚"
    )
    trained_model, trained_scaler, best_threshold = train_and_validate(
        train_df, validation_df, LOOK_BACK, TREND_CONFIG
    )

    if trained_model and best_threshold is not None:
        # åŸºçº¿ï¼šä¿ç•™ MACD è¿‡æ»¤
        run_backtest_and_evaluate(
            test_df,
            trained_model,
            trained_scaler,
            LOOK_BACK,
            best_threshold,
            TREND_CONFIG,
            apply_macd_filter=True,
            strict_online=STRICT_ONLINE_EVAL,
        )
        # å¯¹ç…§ï¼šç§»é™¤ MACD è¿‡æ»¤ï¼Œè¯„ä¼°è¯„ä¼°åå·®
        if RUN_ABLATION_NO_MACD_FILTER:
            run_backtest_and_evaluate(
                test_df,
                trained_model,
                trained_scaler,
                LOOK_BACK,
                best_threshold,
                TREND_CONFIG,
                apply_macd_filter=False,
                strict_online=STRICT_ONLINE_EVAL,
            )
    else:
        logger.error("æ¨¡å‹è®­ç»ƒå¤±è´¥ï¼Œè·³è¿‡å›æµ‹ã€‚")
