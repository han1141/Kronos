# -*- coding: utf-8 -*-
import os
import time
import logging
from datetime import datetime, timedelta
import numpy as np
import pandas as pd
import joblib
import requests
import matplotlib.font_manager
import matplotlib.pyplot as plt
import matplotlib.ticker as mticker

try:
    import xgboost as xgb
    from sklearn.preprocessing import RobustScaler
    from sklearn.metrics import accuracy_score
    import tensorflow as tf
    from tensorflow.keras.models import Sequential, load_model
    from tensorflow.keras.layers import (
        Conv1D,
        LSTM,
        Dense,
        Dropout,
        Input,
        BatchNormalization,
    )
    from tensorflow.keras.callbacks import EarlyStopping
    from tensorflow.keras.regularizers import l2

    ML_LIBS = True
except ImportError:
    ML_LIBS = False

try:
    from backtesting import Backtest, Strategy
    from backtesting.lib import crossover
    import ta
except ImportError:
    pass

import warnings
from pandas.errors import SettingWithCopyWarning

warnings.filterwarnings("ignore", category=FutureWarning)
warnings.filterwarnings("ignore", category=UserWarning, module="xgboost")
warnings.filterwarnings("ignore", category=SettingWithCopyWarning)
warnings.filterwarnings("ignore", category=DeprecationWarning)
logging.getLogger("matplotlib").setLevel(logging.WARNING)
logging.getLogger("tensorflow").setLevel(logging.ERROR)

logger = logging.getLogger(__name__)
logger.setLevel(logging.DEBUG)
ch = logging.StreamHandler()
ch.setLevel(logging.INFO)
formatter = logging.Formatter(
    "%(asctime)s [%(levelname)s] %(message)s", datefmt="%Y-%m-%d %H:%M:%S"
)
if not logger.handlers:
    logger.addHandler(ch)


def set_chinese_font():
    try:
        font_names = [
            "PingFang SC",
            "Microsoft YaHei",
            "SimHei",
            "Heiti TC",
            "sans-serif",
        ]
        for font in font_names:
            if font in [f.name for f in matplotlib.font_manager.fontManager.ttflist]:
                plt.rcParams["font.sans-serif"] = [font]
                plt.rcParams["axes.unicode_minus"] = False
                logger.info(f"æˆåŠŸè®¾ç½®ä¸­æ–‡å­—ä½“: {font}")
                return
        logger.warning("æœªæ‰¾åˆ°æŒ‡å®šçš„ä¸­æ–‡å­—ä½“")
    except Exception as e:
        logger.error(f"è®¾ç½®ä¸­æ–‡å­—ä½“æ—¶å‡ºé”™: {e}")


# --- CONFIGURATION ---
CONFIG = {
    "symbols_to_test": ["ETHUSDT", "BTCUSDT"],  # æ‰©å¤§æµ‹è¯•èŒƒå›´ï¼ŒéªŒè¯æ™®é€‚æ€§
    "btc_symbol": "BTCUSDT",
    "interval": "30m",
    "backtest_start_date": "2025-01-01",
    "backtest_end_date": "2025-10-16",
    "initial_cash": 500_000,
    "commission": 0.0002,
    "spread": 0.0005,
    "show_plots": True,
    "training_window_days": 365 * 2.2,
    # [V49.0 æ–°å¢] è®­ç»ƒæ•°æ®éš”ç¦»æœŸï¼Œé˜²æ­¢å‰è§†åå·®
    "ml_training_gap_days": 30,
    # [V49.0 æ–°å¢] ç­–ç•¥æ¨¡å—å¼€å…³ï¼Œç”¨äºæ¶ˆèç ”ç©¶(Ablation Study)
    "enabled_modules": {
        "trend_following": True,
        "mean_reversion": True,
        "ml_filter": True,
        "lpr_mode": True,
    },
}

# --- STRATEGY PARAMETERS ---
STRATEGY_PARAMS = {
    "kelly_trade_history": 20,
    "default_risk_pct": 0.015,
    "max_risk_pct": 0.04,
    "regime_adx_period": 14,
    "regime_atr_period": 14,
    "regime_atr_slope_period": 5,
    "regime_rsi_period": 14,
    "regime_rsi_vol_period": 14,
    "regime_norm_period": 252,
    "regime_hurst_period": 100,
    "regime_score_weight_adx": 0.6,
    "regime_score_weight_atr": 0.3,
    "regime_score_weight_rsi": 0.05,
    "regime_score_weight_hurst": 0.05,
    "regime_score_threshold": 0.5,  # ä»0.45è°ƒæ•´ä¸º0.5ï¼Œé¿å…è¿‡åº¦æ‹Ÿåˆ
    "tf_donchian_period": 30,
    "tf_ema_fast_period": 20,
    "tf_ema_slow_period": 75,
    "tf_adx_confirm_period": 14,
    "tf_adx_confirm_threshold": 18,
    "tf_chandelier_period": 22,
    "tf_chandelier_atr_multiplier": 3.0,
    "tf_atr_period": 14,
    "tf_stop_loss_atr_multiplier": 2.5,
    "mr_bb_period": 20,
    "mr_bb_std": 2.0,
    "mr_rsi_period": 14,
    "mr_rsi_oversold": 30,
    "mr_rsi_overbought": 70,
    "mr_stop_loss_atr_multiplier": 1.5,
    "mr_risk_multiplier": 0.5,
    "mtf_period": 50,
    "score_entry_threshold": 0.5,
    "score_weights_tf": {"breakout": 0.5, "momentum": 0.3, "mtf": 0.2},
    "lpr_enabled": True,
    "lpr_trigger_pct": 0.08,
    "lpr_trail_atr_multiplier": 2.0,
    "ml_filter_enabled": True,
    "lstm_sequence_length": 48,
    "lstm_epochs": 60,
    "lstm_batch_size": 128,
    "lstm_l2_reg": 0.001,  # [V49.0 æ–°å¢] LSTM L2æ­£åˆ™åŒ–
    "xgb_nrounds": 250,
    "xgb_gamma": 0.3,  # [V49.0 æ–°å¢] XGBoost Gammaæ­£åˆ™åŒ–
    "xgb_lambda": 1.5,  # [V49.0 æ–°å¢] XGBoost Lambda(L2)æ­£åˆ™åŒ–
    "ensemble_w_lstm": 0.3,
    "ensemble_w_xgb": 0.7,
    "ml_confidence_threshold": 0.6,  # ä»0.58è°ƒæ•´ä¸º0.6ï¼Œé¿å…è¿‡åº¦æ‹Ÿåˆ
    "ml_score_weight": 0.3,
}

def fetch_binance_klines(s, i, st, en=None, l=1000, cache_dir="data_cache"):
    # ... (å‡½æ•°å†…å®¹ä¿æŒä¸å˜)
    os.makedirs(cache_dir, exist_ok=True)
    cache_file = os.path.join(cache_dir, f"{s}_{i}.csv")
    start_dt, end_dt = pd.to_datetime(st), (
        pd.to_datetime(en) if en else datetime.utcnow()
    )
    if os.path.exists(cache_file):
        try:
            df = pd.read_csv(cache_file, index_col="timestamp", parse_dates=True)
            if not df.empty and df.index[0] <= start_dt and df.index[-1] >= end_dt:
                logger.info(f"âœ… ä»ç¼“å­˜åŠ è½½æ•°æ®: {cache_file}")
                return df
        except Exception:
            pass
    logger.info(f"å¼€å§‹ä»å¸å®‰APIä¸‹è½½ {s} æ•°æ®...")
    url, all_data, current_start = (
        "https://api.binance.com/api/v3/klines",
        [],
        int(start_dt.timestamp() * 1000),
    )
    end_ts = int(end_dt.timestamp() * 1000)
    while current_start < end_ts:
        p = {
            "symbol": s.upper(),
            "interval": i,
            "startTime": current_start,
            "endTime": end_ts,
            "limit": l,
        }
        try:
            r = requests.get(url, params=p, timeout=30)
            r.raise_for_status()
            d = r.json()
            if not d:
                break
            all_data.extend(d)
            current_start = d[-1][0] + 1
        except requests.exceptions.RequestException as e:
            logger.warning(f"è·å– {s} æ—¶é‡åˆ°è¿æ¥é—®é¢˜: {e}ã€‚5ç§’åé‡è¯•...")
            time.sleep(5)
    if not all_data:
        return pd.DataFrame()
    df = pd.DataFrame(
        all_data,
        columns=[
            "timestamp",
            "Open",
            "High",
            "Low",
            "Close",
            "Volume",
            "ct",
            "qav",
            "nt",
            "tbb",
            "tbq",
            "ig",
        ],
    )
    df = df[["timestamp", "Open", "High", "Low", "Close", "Volume"]].copy()
    df["timestamp"] = pd.to_datetime(df["timestamp"], unit="ms")
    for col in df.columns[1:]:
        df[col] = pd.to_numeric(df[col], errors="coerce")
    df.drop_duplicates(subset=["timestamp"], inplace=True)
    df = df.set_index("timestamp").sort_index()
    df.to_csv(cache_file)
    logger.info(f"âœ… æ•°æ®å·²ç¼“å­˜è‡³: {cache_file}")
    return df


def compute_hurst(ts, max_lag=100):
    # ... (å‡½æ•°å†…å®¹ä¿æŒä¸å˜)
    if len(ts) < 10:
        return 0.5
    lags = range(2, min(max_lag, len(ts) // 2 + 1))
    tau = []
    for lag in lags:
        diff = np.subtract(ts[lag:], ts[:-lag])
        std = np.std(diff)
        if std > 0:
            tau.append(std)
    if len(tau) < 2:
        return 0.5
    try:
        return max(
            0.0, min(1.0, np.polyfit(np.log(lags[: len(tau)]), np.log(tau), 1)[0])
        )
    except:
        return 0.5


def add_all_features(df: pd.DataFrame, btc_df: pd.DataFrame = None) -> pd.DataFrame:
    # ... (å‡½æ•°å†…å®¹åŸºæœ¬ä¸å˜, ä½†å»ºè®®åœ¨è¿™é‡Œè€ƒè™‘ç‰¹å¾é€‰æ‹©)
    # æç¤º: åœ¨æ­¤é˜¶æ®µæˆ–MLè®­ç»ƒå‰ï¼Œå¯ä»¥è€ƒè™‘è¿›è¡Œç‰¹å¾é‡è¦æ€§åˆ†æå’Œç­›é€‰ï¼Œä»¥å‡å°‘ç»´åº¦ç¾éš¾é£é™©ã€‚
    df = df.copy()
    p = STRATEGY_PARAMS
    # --- TA Features for Strategy ---
    norm = lambda s: (
        (s - s.rolling(p["regime_norm_period"]).min())
        / (
            s.rolling(p["regime_norm_period"]).max()
            - s.rolling(p["regime_norm_period"]).min()
        )
    ).fillna(0.5)
    adx = ta.trend.ADXIndicator(df.High, df.Low, df.Close, p["regime_adx_period"]).adx()
    atr = ta.volatility.AverageTrueRange(
        df.High, df.Low, df.Close, p["regime_atr_period"]
    ).average_true_range()
    rsi = ta.momentum.RSIIndicator(df.Close, p["regime_rsi_period"]).rsi()
    df["regime_adx_norm"] = norm(adx)
    df["regime_atr_slope_norm"] = norm(
        (atr - atr.shift(p["regime_atr_slope_period"]))
        / atr.shift(p["regime_atr_slope_period"])
    )
    df["regime_rsi_vol_norm"] = 1 - norm(rsi.rolling(p["regime_rsi_vol_period"]).std())
    df["regime_hurst"] = (
        df.Close.rolling(p["regime_hurst_period"])
        .apply(lambda x: compute_hurst(np.log(x + 1e-9)), raw=False)
        .fillna(0.5)
    )
    df["regime_score"] = (
        df["regime_adx_norm"] * p["regime_score_weight_adx"]
        + df["regime_atr_slope_norm"] * p["regime_score_weight_atr"]
        + df["regime_rsi_vol_norm"] * p["regime_score_weight_rsi"]
        + np.clip((df["regime_hurst"] - 0.3) / 0.7, 0, 1)
        * p["regime_score_weight_hurst"]
    )
    df["market_regime"] = np.where(
        df["regime_score"] > p["regime_score_threshold"], 1, -1
    )
    df_1d = (
        df.resample("1D")
        .agg(
            {
                "Open": "first",
                "High": "max",
                "Low": "min",
                "Close": "last",
                "Volume": "sum",
            }
        )
        .dropna()
    )
    if not df_1d.empty:
        sma = ta.trend.SMAIndicator(
            df_1d["Close"], window=p["mtf_period"]
        ).sma_indicator()
        df["mtf_signal"] = (
            pd.Series(np.where(df_1d["Close"] > sma, 1, -1), index=df_1d.index)
            .reindex(df.index, method="ffill")
            .fillna(0)
        )
    else:
        df["mtf_signal"] = 0
    # --- ML Features ---
    df["feature_log_return_1"] = np.log(df.Close / df.Close.shift(1))
    df["feature_rsi_14"] = ta.momentum.RSIIndicator(df.Close, 14).rsi()
    df["feature_atr_14_pct"] = (
        ta.volatility.AverageTrueRange(
            df.High, df.Low, df.Close, 14
        ).average_true_range()
        / df.Close
    )
    df["feature_bb_width"] = (
        ta.volatility.BollingerBands(df.Close, 20).bollinger_hband()
        - ta.volatility.BollingerBands(df.Close, 20).bollinger_lband()
    ) / df.Close
    df["feature_close_ma_200_ratio"] = (
        df.Close / ta.trend.EMAIndicator(df.Close, 200).ema_indicator()
    )
    df["feature_atr_std_ratio"] = (
        df["feature_atr_14_pct"].rolling(20).std() / df["feature_atr_14_pct"]
    )
    if btc_df is not None and not btc_df.empty:
        btc_log_ret = np.log(btc_df.Close / btc_df.Close.shift(1))
        df["feature_btc_log_return"] = btc_log_ret.reindex(df.index, method="ffill")
        df["feature_btc_corr_120"] = (
            df["feature_log_return_1"].rolling(120).corr(df["feature_btc_log_return"])
        )
    df["feature_hour"] = df.index.hour
    df["feature_dayofweek"] = df.index.dayofweek
    df.ffill(inplace=True)
    df.bfill(inplace=True)
    df.dropna(inplace=True)
    return df


def make_asymmetric_labels(
    df, look_forward=30, risk_reward_ratio=2.0, sl_atr_multiplier=1.5
):
    # ... (å‡½æ•°å†…å®¹ä¿æŒä¸å˜)
    atr = ta.volatility.AverageTrueRange(
        df.High, df.Low, df.Close, 14
    ).average_true_range()
    upper_barrier = df.Close + (atr * sl_atr_multiplier * risk_reward_ratio)
    lower_barrier = df.Close - (atr * sl_atr_multiplier)
    labels = pd.Series(np.nan, index=df.index)
    for i in range(len(df) - look_forward):
        entry_idx = df.index[i]
        path = df.iloc[i + 1 : i + 1 + look_forward]
        upper_touch_time = path[path.High >= upper_barrier.loc[entry_idx]].index.min()
        lower_touch_time = path[path.Low <= lower_barrier.loc[entry_idx]].index.min()
        if pd.notna(upper_touch_time) and pd.notna(lower_touch_time):
            labels.loc[entry_idx] = 1 if upper_touch_time <= lower_touch_time else 0
        elif pd.notna(upper_touch_time):
            labels.loc[entry_idx] = 1
        elif pd.notna(lower_touch_time):
            labels.loc[entry_idx] = 0
    return labels


def create_sequences(X, y, seq_len):
    # ... (å‡½æ•°å†…å®¹ä¿æŒä¸å˜)
    Xs, ys = [], []
    for i in range(len(X) - seq_len):
        Xs.append(X[i : i + seq_len])
        ys.append(y[i + seq_len])
    return np.array(Xs), np.array(ys)


# --- MACHINE LEARNING MODELS ---


def train_and_save_lstm(training_df, symbol, seq_len, epochs, batch):
    logger.info(f"ä¸º {symbol} è®­ç»ƒ LSTM...")
    labels = make_asymmetric_labels(training_df)
    df = training_df.join(labels.rename("target")).dropna(subset=["target"])
    df.loc[:, "target"] = df["target"].astype(int)
    features = [c for c in df.columns if c.startswith("feature_")]
    # æç¤º: åœ¨è¿™é‡Œå¯ä»¥åŠ å…¥åŸºäºè®­ç»ƒé›†çš„ç‰¹å¾é€‰æ‹©é€»è¾‘ï¼Œä¾‹å¦‚åŸºäºæ–¹å·®æˆ–ä¸ç›®æ ‡çš„äº’ä¿¡æ¯ã€‚
    X, y = df[features].values, df["target"].values
    if len(np.unique(y)) < 2:
        logger.warning("ç±»åˆ«ä¸è¶³ï¼Œè·³è¿‡ LSTMã€‚")
        return None

    # [V49.0 ç¡®è®¤] é™æ€åˆ†å‰²ï¼Œä½†ä¸¥æ ¼éµå®ˆæ—¶é—´é¡ºåºï¼Œè¿™æ˜¯æ­£ç¡®çš„åšæ³•ã€‚
    split = int(len(X) * 0.8)
    X_train, X_test, y_train, y_test = X[:split], X[split:], y[:split], y[split:]

    scaler = RobustScaler()
    X_train_s, X_test_s = scaler.fit_transform(X_train), scaler.transform(X_test)
    joblib.dump(scaler, f"lstm_scaler_{symbol}.pkl")
    X_train_seq, y_train_seq = create_sequences(X_train_s, y_train, seq_len)
    X_test_seq, y_test_seq = create_sequences(X_test_s, y_test, seq_len)
    if len(X_train_seq) == 0:
        logger.error("åºåˆ—æ ·æœ¬ä¸è¶³ï¼Œè·³è¿‡ LSTMã€‚")
        return None
    if np.unique(y_train_seq).size < 2 or np.unique(y_test_seq).size < 2:
        logger.warning("åˆ†å‰²åç±»åˆ«ä¸è¶³ï¼Œè·³è¿‡ LSTMã€‚")
        return None

    l2_reg = STRATEGY_PARAMS.get("lstm_l2_reg", 0.001)
    model = Sequential(
        [
            Input(shape=(seq_len, X_train_seq.shape[2])),
            Conv1D(64, kernel_size=5, activation="relu", kernel_regularizer=l2(l2_reg)),
            BatchNormalization(),
            Dropout(0.2),  # å¢åŠ Dropoutç‡
            LSTM(128, return_sequences=True, kernel_regularizer=l2(l2_reg)),
            Dropout(0.3),  # å¢åŠ Dropoutç‡
            LSTM(64, kernel_regularizer=l2(l2_reg)),
            Dropout(0.3),  # å¢åŠ Dropoutç‡
            Dense(64, activation="relu", kernel_regularizer=l2(l2_reg)),
            Dense(1, activation="sigmoid"),
        ]
    )
    model.compile(
        optimizer=tf.keras.optimizers.Adam(learning_rate=0.0005),
        loss="binary_crossentropy",
        metrics=["accuracy"],
    )
    es = EarlyStopping(monitor="val_loss", patience=10, restore_best_weights=True)
    model.fit(
        X_train_seq,
        y_train_seq,
        validation_data=(X_test_seq, y_test_seq),
        epochs=epochs,
        batch_size=batch,
        callbacks=[es],
        verbose=0,
    )
    loss, acc = model.evaluate(X_test_seq, y_test_seq, verbose=0)
    logger.info(f"LSTM åœ¨æ—¶é—´å¤–æ ·æœ¬ä¸Šçš„çœŸå®å‡†ç¡®ç‡: {acc:.4f}")
    model.save(f"lstm_model_{symbol}.keras")
    return acc


def train_and_save_xgb(training_df, symbol):
    logger.info(f"ä¸º {symbol} è®­ç»ƒ XGBoost...")
    labels = make_asymmetric_labels(training_df)
    df = training_df.join(labels.rename("target")).dropna(subset=["target"])
    df.loc[:, "target"] = df["target"].astype(int)
    features = [c for c in df.columns if c.startswith("feature_")]
    X, y = df[features].values, df["target"].values
    if len(np.unique(y)) < 2:
        logger.warning("ç±»åˆ«ä¸è¶³ï¼Œè·³è¿‡ XGBoostã€‚")
        return None

    # [V49.0 ç¡®è®¤] é™æ€åˆ†å‰²ï¼Œä½†ä¸¥æ ¼éµå®ˆæ—¶é—´é¡ºåºã€‚
    split = int(len(X) * 0.8)
    X_train, X_test, y_train, y_test = X[:split], X[split:], y[:split], y[split:]

    # æç¤º: å¯ä»¥åœ¨è¿™é‡ŒåŸºäºX_trainè®­ç»ƒä¸€ä¸ªåˆæ­¥æ¨¡å‹ï¼Œè¿›è¡Œç‰¹å¾é€‰æ‹©ï¼Œç„¶åå†ç”¨ç­›é€‰åçš„ç‰¹å¾è®­ç»ƒæœ€ç»ˆæ¨¡å‹ã€‚

    scaler = RobustScaler()
    X_train_s, X_test_s = scaler.fit_transform(X_train), scaler.transform(X_test)
    joblib.dump(scaler, f"xgb_scaler_{symbol}.pkl")

    model = xgb.XGBClassifier(
        objective="binary:logistic",
        eval_metric="logloss",
        use_label_encoder=False,
        n_estimators=STRATEGY_PARAMS["xgb_nrounds"],
        learning_rate=0.02,
        max_depth=3,  # é™ä½æœ€å¤§æ·±åº¦
        subsample=0.7,
        colsample_bytree=0.7,
        gamma=STRATEGY_PARAMS.get("xgb_gamma", 0.3),  # [V49.0] ä½¿ç”¨æ­£åˆ™åŒ–å‚æ•°
        reg_lambda=STRATEGY_PARAMS.get("xgb_lambda", 1.5),  # [V49.0] ä½¿ç”¨æ­£åˆ™åŒ–å‚æ•°
        early_stopping_rounds=25,  # å¢åŠ æ—©åœè½®æ¬¡
    )
    model.fit(X_train_s, y_train, eval_set=[(X_test_s, y_test)], verbose=False)
    acc = accuracy_score(y_test, model.predict(X_test_s))
    logger.info(f"XGBoost åœ¨æ—¶é—´å¤–æ ·æœ¬ä¸Šçš„çœŸå®å‡†ç¡®ç‡: {acc:.4f}")
    joblib.dump(model, f"xgb_model_{symbol}.joblib")
    return acc


# --- PLOTTING & ANALYSIS ---


def plot_walk_forward_equity(
    equity_curves, combined_trades, initial_cash, title="Walk-Forward Equity Curve"
):
    # ... (å‡½æ•°å†…å®¹ä¿æŒä¸å˜)
    plt.style.use("seaborn-v0_8-darkgrid")
    fig, ax = plt.subplots(figsize=(15, 8))
    full_equity_curve = pd.Series([initial_cash])
    for i, curve in enumerate(equity_curves):
        period_equity = curve["Equity"]
        if i > 0:
            start_val = full_equity_curve.iloc[-1]
            period_equity = period_equity / period_equity.iloc[0] * start_val
        full_equity_curve = pd.concat([full_equity_curve.iloc[:-1], period_equity])
    full_equity_curve.plot(
        ax=ax, label="æ€»æƒç›Šæ›²çº¿ (Total Equity)", color="blue", linewidth=2.5
    )
    for curve in equity_curves:
        start_date = curve.index[0]
        ax.axvline(
            x=start_date,
            color="grey",
            linestyle="--",
            linewidth=0.8,
            label="æ»šåŠ¨å‘¨æœŸåˆ†å‰²çº¿" if start_date == equity_curves[0].index[0] else "",
        )
    formatter = mticker.FuncFormatter(lambda x, p: f"${x:,.0f}")
    ax.yaxis.set_major_formatter(formatter)
    plt.title(title, fontsize=16)
    plt.xlabel("æ—¥æœŸ")
    plt.ylabel("è´¦æˆ·æƒç›Š ($)")
    plt.legend()
    plt.grid(True)
    filename = "equity_curve.png"
    plt.savefig(filename)
    logger.info(f"âœ… æƒç›Šæ›²çº¿å›¾å·²ä¿å­˜è‡³: {filename}")
    plt.close(fig)


def analyze_trade_distribution(combined_trades, title="äº¤æ˜“ç›ˆäº (PnL) åˆ†å¸ƒ"):
    # ... (å‡½æ•°å†…å®¹ä¿æŒä¸å˜)
    if combined_trades.empty:
        logger.warning("æ²¡æœ‰äº¤æ˜“è®°å½•ï¼Œæ— æ³•ç”Ÿæˆæ”¶ç›Šåˆ†å¸ƒå›¾ã€‚")
        return
    pnl = combined_trades["PnL"]
    wins = pnl[pnl > 0]
    losses = pnl[pnl < 0]
    plt.style.use("seaborn-v0_8-darkgrid")
    fig, ax = plt.subplots(figsize=(15, 8))
    ax.hist(
        losses, bins=50, color="salmon", alpha=0.7, label=f"äºæŸäº¤æ˜“ (n={len(losses)})"
    )
    ax.hist(
        wins, bins=50, color="skyblue", alpha=0.7, label=f"ç›ˆåˆ©äº¤æ˜“ (n={len(wins)})"
    )
    stats_text = (
        f"ç›ˆåˆ©äº¤æ˜“:\n  - æ•°é‡: {len(wins)}\n  - å¹³å‡ç›ˆåˆ©: ${wins.mean():,.2f}\n  - ç›ˆåˆ©æ ‡å‡†å·®: ${wins.std():,.2f}\n\n"
        f"äºæŸäº¤æ˜“:\n  - æ•°é‡: {len(losses)}\n  - å¹³å‡äºæŸ: ${losses.mean():,.2f}\n  - äºæŸæ ‡å‡†å·®: ${losses.std():,.2f}"
    )
    ax.text(
        0.02,
        0.98,
        stats_text,
        transform=ax.transAxes,
        fontsize=10,
        verticalalignment="top",
        bbox=dict(boxstyle="round,pad=0.5", fc="wheat", alpha=0.5),
    )
    plt.title(title, fontsize=16)
    plt.xlabel("å•ç¬”äº¤æ˜“ç›ˆäº ($)")
    plt.ylabel("äº¤æ˜“æ¬¡æ•°")
    plt.legend()
    plt.grid(True)
    filename = "trade_distribution.png"
    plt.savefig(filename)
    logger.info(f"âœ… äº¤æ˜“åˆ†å¸ƒå›¾å·²ä¿å­˜è‡³: {filename}")
    plt.close(fig)


def generate_optimization_suggestions(stats, model_accuracies, config):
    # ... (å‡½æ•°å†…å®¹ä¿æŒä¸å˜)
    logger.info("\n" + "#" * 80 + "\n                 ğŸ§  AI ä¼˜åŒ–å»ºè®®å™¨\n" + "#" * 80)
    suggestions = []
    if stats["Profit Factor"] < 1.1:
        suggestions.append(
            "ğŸš¨ æ ¸å¿ƒé—®é¢˜: ç›ˆäºå› å­ä½äº1.1ï¼Œç­–ç•¥ç›ˆåˆ©èƒ½åŠ›éå¸¸è„†å¼±ã€‚\n   >> ä¼˜åŒ–æ–¹å‘: ä¼˜å…ˆæé«˜ä¿¡å·è´¨é‡ï¼Œè€Œä¸æ˜¯å¢åŠ äº¤æ˜“é¢‘ç‡ã€‚è€ƒè™‘æé«˜ `score_entry_threshold` ä»¥è¿‡æ»¤æ‰æ›´å¤šå¼±ä¿¡å·ã€‚"
        )
    elif stats["Profit Factor"] < 1.3:
        suggestions.append(
            "âš ï¸ æ³¨æ„: ç›ˆäºå› å­ (1.1-1.3) å°šå¯ï¼Œä½†æœ‰æå‡ç©ºé—´ã€‚\n   >> ä¼˜åŒ–æ–¹å‘: å®¡è§†äºæŸäº¤æ˜“ï¼Œæ£€æŸ¥æ˜¯å¦èƒ½é€šè¿‡è°ƒæ•´æ­¢æŸ (`tf_stop_loss_atr_multiplier`) æˆ–è¶‹åŠ¿è¯†åˆ«å‚æ•°æ¥è§„é¿éƒ¨åˆ†äºæŸã€‚"
        )
    else:
        suggestions.append(
            "âœ… ä¼˜åŠ¿: ç›ˆäºå› å­ (>=1.3) è¡¨ç°è‰¯å¥½ï¼Œç­–ç•¥å…·å¤‡ä¸é”™çš„ç›ˆåˆ©èƒ½åŠ›ã€‚"
        )
    if stats["Win Rate [%]"] < 40:
        suggestions.append(
            f"âš ï¸ æ³¨æ„: èƒœç‡ ({stats['Win Rate [%]']:.2f}%) åä½ï¼Œå¯èƒ½å¯¼è‡´å›æ’¤è¾ƒå¤§ã€‚\n   >> ä¼˜åŒ–æ–¹å‘: 1. æé«˜ `score_entry_threshold`ã€‚ 2. æé«˜æœºå™¨å­¦ä¹ çš„ç½®ä¿¡åº¦é—¨æ§› `ml_confidence_threshold`ã€‚"
        )
    else:
        suggestions.append(
            f"âœ… ä¼˜åŠ¿: èƒœç‡ ({stats['Win Rate [%]']:.2f}%) å¤„äºå¯æ¥å—èŒƒå›´ã€‚"
        )
    avg_lstm_acc = np.mean([acc.get("lstm", 0.5) for acc in model_accuracies])
    avg_xgb_acc = np.mean([acc.get("xgb", 0.5) for acc in model_accuracies])
    if avg_lstm_acc < 0.52:
        suggestions.append(
            f"ğŸ“‰ æ¨¡å‹é—®é¢˜: LSTM å¹³å‡å‡†ç¡®ç‡ ({avg_lstm_acc:.2%}) è¿‡ä½ï¼Œå‡ ä¹æ²¡æœ‰é¢„æµ‹èƒ½åŠ›ã€‚\n   >> ä¼˜åŒ–æ–¹å‘: 1. å¤§å¹…é™ä½å…¶åœ¨é›†æˆæ¨¡å‹ä¸­çš„æƒé‡ (`ensemble_w_lstm`)ã€‚ 2. è€ƒè™‘ä»é›†æˆä¸­ç§»é™¤LSTM (é€šè¿‡ enabled_modules è®¾ç½®)ã€‚"
        )
    if avg_xgb_acc < 0.53:
        suggestions.append(
            f"ğŸ“‰ æ¨¡å‹é—®é¢˜: XGBoost å¹³å‡å‡†ç¡®ç‡ ({avg_xgb_acc:.2%}) åä½ã€‚\n   >> ä¼˜åŒ–æ–¹å‘: 1. è¿›è¡Œç‰¹å¾é€‰æ‹©ï¼Œç§»é™¤é‡è¦æ€§ä½çš„ç‰¹å¾ã€‚ 2. è°ƒæ•´`make_asymmetric_labels`ä¸­çš„å‚æ•°ã€‚"
        )
    if avg_xgb_acc > avg_lstm_acc + 0.02:
        suggestions.append(
            f"ğŸ’¡ æ¨¡å‹æ´å¯Ÿ: XGBoost (acc: {avg_xgb_acc:.2%}) è¡¨ç°æ˜¾è‘—ä¼˜äº LSTM (acc: {avg_lstm_acc:.2%})ã€‚\n   >> ä¼˜åŒ–æ–¹å‘: è°ƒæ•´é›†æˆæƒé‡ï¼Œç»™äºˆXGBoostæ›´å¤§çš„è¯è¯­æƒï¼Œä¾‹å¦‚ `ensemble_w_xgb` è°ƒæ•´è‡³ 0.7 æˆ–æ›´é«˜ã€‚"
        )
    for i, suggestion in enumerate(suggestions):
        logger.info(f"\n--- å»ºè®® {i+1} ---\n{suggestion}")
    logger.info("\n" + "#" * 80)


# --- STRATEGY IMPLEMENTATION ---


class BaseAssetStrategy:
    # ... (ç±»å†…å®¹ä¿æŒä¸å˜)
    def __init__(self, main_strategy: Strategy):
        self.main = main_strategy

    def _calculate_entry_score(self) -> float:
        m, w = self.main, self.main.score_weights_tf
        b_s = (
            1
            if m.data.High[-1] > m.tf_donchian_h[-2]
            else -1 if m.data.Low[-1] < m.tf_donchian_l[-2] else 0
        )
        mo_s = 1 if m.tf_ema_fast[-1] > m.tf_ema_slow[-1] else -1
        return (
            b_s * w.get("breakout", 0)
            + mo_s * w.get("momentum", 0)
            + m.mtf_signal[-1] * w.get("mtf", 0)
        )

    def _define_mr_entry_signal(self) -> int:
        m = self.main
        return (
            1
            if crossover(m.data.Close, m.mr_bb_lower)
            and m.mr_rsi[-1] < m.mr_rsi_oversold
            else (
                -1
                if crossover(m.mr_bb_upper, m.data.Close)
                and m.mr_rsi[-1] > m.mr_rsi_overbought
                else 0
            )
        )


class UltimateStrategy(Strategy):
    symbol = None
    vol_weight = 1.0

    # [V49.0 æ–°å¢] ç”¨äºä»å¤–éƒ¨ä¼ å…¥é…ç½®
    enabled_modules = None

    def init(self):
        # åŠ è½½åŸºç¡€å‚æ•°
        for k, v in STRATEGY_PARAMS.items():
            setattr(self, k, v)

        # [V49.0 ä¿®æ”¹] ä» CONFIG åŠ è½½æ¨¡å—å¼€å…³ï¼Œå¦‚æœå¤–éƒ¨æ²¡ä¼ ï¼Œåˆ™ä½¿ç”¨é»˜è®¤å€¼
        if self.enabled_modules is None:
            self.enabled_modules = CONFIG.get("enabled_modules", {})

        # [V49.0 ç§»é™¤] ä¸å†ä½¿ç”¨èµ„äº§ç‰¹å¼‚æ€§å‚æ•°
        self.asset_strategy = BaseAssetStrategy(self)

        c, h, l = (
            pd.Series(self.data.Close),
            pd.Series(self.data.High),
            pd.Series(self.data.Low),
        )
        self.reset_trade_state()
        self.market_regime, self.mtf_signal = self.I(
            lambda: self.data.market_regime
        ), self.I(lambda: self.data.mtf_signal)
        self.tf_atr = self.I(
            lambda: ta.volatility.AverageTrueRange(
                h, l, c, self.tf_atr_period
            ).average_true_range()
        )
        self.tf_donchian_h, self.tf_donchian_l = self.I(
            lambda: h.rolling(self.tf_donchian_period).max().shift(1)
        ), self.I(lambda: l.rolling(self.tf_donchian_period).min().shift(1))
        self.tf_ema_fast, self.tf_ema_slow = self.I(
            lambda: ta.trend.EMAIndicator(c, self.tf_ema_fast_period).ema_indicator()
        ), self.I(
            lambda: ta.trend.EMAIndicator(c, self.tf_ema_slow_period).ema_indicator()
        )
        self.tf_adx = self.I(
            lambda: ta.trend.ADXIndicator(h, l, c, self.tf_adx_confirm_period).adx()
        )
        bb = ta.volatility.BollingerBands(c, self.mr_bb_period, self.mr_bb_std)
        self.mr_bb_upper, self.mr_bb_lower, self.mr_bb_mid = (
            self.I(lambda: bb.bollinger_hband()),
            self.I(lambda: bb.bollinger_lband()),
            self.I(lambda: bb.bollinger_mavg()),
        )
        self.mr_rsi = self.I(
            lambda: ta.momentum.RSIIndicator(c, self.mr_rsi_period).rsi()
        )

        self.lstm, self.lstm_scaler, self.xgb, self.xgb_scaler = None, None, None, None
        if self.ml_filter_enabled and self.enabled_modules.get("ml_filter", False):
            try:
                self.lstm = load_model(f"lstm_model_{self.symbol}.keras")
                self.lstm_scaler = joblib.load(f"lstm_scaler_{self.symbol}.pkl")
                logger.info(f"âœ… [{self.symbol}] LSTM æ¨¡å‹åŠ è½½æˆåŠŸã€‚")
            except Exception:
                self.lstm = None
            try:
                self.xgb = joblib.load(f"xgb_model_{self.symbol}.joblib")
                self.xgb_scaler = joblib.load(f"xgb_scaler_{self.symbol}.pkl")
                logger.info(f"âœ… [{self.symbol}] XGBoost æ¨¡å‹åŠ è½½æˆåŠŸã€‚")
            except Exception:
                self.xgb = None

    def run_scoring_system_entry(self, price):
        ta_score = self.asset_strategy._calculate_entry_score()
        ml_boost = 0.0

        if self.ml_filter_enabled and self.enabled_modules.get("ml_filter", False):
            prob_lstm, prob_xgb = 0.5, 0.5
            features = [c for c in self.data.df.columns if c.startswith("feature_")]
            if (
                self.lstm
                and self.lstm_scaler
                and len(self.data.df) >= self.lstm_sequence_length
            ):
                seq = self.data.df[features].iloc[-self.lstm_sequence_length :].values
                if not np.isnan(seq).any():
                    scaled = self.lstm_scaler.transform(seq)
                    prob_lstm = float(
                        self.lstm.predict(np.expand_dims(scaled, 0), verbose=0)[0][0]
                    )
            if self.xgb and self.xgb_scaler:
                cur_feat = self.data.df[features].iloc[-1:].values
                if not np.isnan(cur_feat).any():
                    scaled = self.xgb_scaler.transform(cur_feat)
                    prob_xgb = float(self.xgb.predict_proba(scaled)[0, 1])
            ensemble_prob = (
                self.ensemble_w_lstm * prob_lstm + self.ensemble_w_xgb * prob_xgb
            )

            is_long_ta = ta_score > 0
            if is_long_ta and ensemble_prob > self.ml_confidence_threshold:
                ml_boost = (ensemble_prob - 0.5) * 2 * self.ml_score_weight
            elif not is_long_ta and (1 - ensemble_prob) > self.ml_confidence_threshold:
                ml_boost = ((1 - ensemble_prob) - 0.5) * 2 * self.ml_score_weight * -1

        final_score = ta_score + ml_boost
        if abs(final_score) <= self.score_entry_threshold:
            return
        self.open_tf_position(
            price, is_long=(final_score > 0), confidence_factor=abs(final_score)
        )

    def next(self):
        if len(self.data) < max(
            self.tf_donchian_period, self.tf_ema_slow_period, self.regime_norm_period
        ):
            return
        if self.position:
            self.manage_open_position(self.data.Close[-1])
        else:
            if self.data.market_regime[-1] == 1:
                # [V49.0 ä¿®æ”¹] æ£€æŸ¥è¶‹åŠ¿è·Ÿè¸ªæ¨¡å—æ˜¯å¦å¯ç”¨
                if self.enabled_modules.get("trend_following", False):
                    self.run_scoring_system_entry(self.data.Close[-1])
            else:
                # [V49.0 ä¿®æ”¹] æ£€æŸ¥å‡å€¼å›å½’æ¨¡å—æ˜¯å¦å¯ç”¨
                if self.enabled_modules.get("mean_reversion", False):
                    self.run_mean_reversion_entry(self.data.Close[-1])

    def run_mean_reversion_entry(self, price):
        signal = self.asset_strategy._define_mr_entry_signal()
        if signal != 0:
            self.open_mr_position(price, is_long=(signal > 0))

    def reset_trade_state(self):
        # ... (å‡½æ•°å†…å®¹ä¿æŒä¸å˜)
        self.active_sub_strategy, self.chandelier_exit_level = None, 0.0
        self.highest_high_in_trade, self.lowest_low_in_trade = 0, float("inf")
        self.mr_stop_loss, self.tf_initial_stop_loss = 0.0, 0.0
        self.lpr_is_active, self.lpr_trailing_stop = False, 0.0

    def manage_open_position(self, p):
        if self.lpr_is_active:
            self.manage_lpr_exit(p)
            return

        # [V49.0 ä¿®æ”¹] æ£€æŸ¥LPRæ¨¡å—æ˜¯å¦å¯ç”¨
        if (
            self.lpr_enabled
            and self.enabled_modules.get("lpr_mode", False)
            and self.position
        ):
            trade = self.trades[-1]
            entry_price = trade.entry_price
            profit_pct = (
                (p / entry_price - 1)
                if self.position.is_long
                else (entry_price / p - 1)
            )
            if profit_pct >= self.lpr_trigger_pct:
                self.lpr_is_active = True
                logger.info(
                    f"ğŸš€ åˆ©æ¶¦å¥”è·‘æ¨¡å¼æ¿€æ´»! ä»·æ ¼: {p:.2f}, ç›ˆåˆ©: {profit_pct:.2%}"
                )
                self.manage_lpr_exit(p)
                return

        if self.active_sub_strategy == "TF":
            self.manage_trend_following_exit(p)
        elif self.active_sub_strategy == "MR":
            self.manage_mean_reversion_exit(p)

    def open_tf_position(self, p, is_long, confidence_factor):
        # ... (å‡½æ•°å†…å®¹ä¿æŒä¸å˜)
        risk_ps = self.tf_atr[-1] * self.tf_stop_loss_atr_multiplier
        if risk_ps <= 0:
            return
        size = self._calculate_position_size(
            p, risk_ps, self._calculate_dynamic_risk() * confidence_factor
        )
        if size <= 0:
            return
        self.reset_trade_state()
        self.active_sub_strategy = "TF"
        if is_long:
            self.buy(size=size)
            self.tf_initial_stop_loss = p - risk_ps
            self.highest_high_in_trade = self.data.High[-1]
            self.chandelier_exit_level = (
                self.highest_high_in_trade
                - self.tf_atr[-1] * self.tf_chandelier_atr_multiplier
            )
        else:
            self.sell(size=size)
            self.tf_initial_stop_loss = p + risk_ps
            self.lowest_low_in_trade = self.data.Low[-1]
            self.chandelier_exit_level = (
                self.lowest_low_in_trade
                + self.tf_atr[-1] * self.tf_chandelier_atr_multiplier
            )

    def manage_trend_following_exit(self, p):
        # ... (å‡½æ•°å†…å®¹ä¿æŒä¸å˜)
        atr = self.tf_atr[-1]
        if self.position.is_long:
            if p < self.tf_initial_stop_loss:
                self.close_position("TF_Initial_SL")
                return
            self.highest_high_in_trade = max(
                self.highest_high_in_trade, self.data.High[-1]
            )
            self.chandelier_exit_level = (
                self.highest_high_in_trade - atr * self.tf_chandelier_atr_multiplier
            )
            if p < self.chandelier_exit_level:
                self.close_position("TF_Chandelier")
        elif self.position.is_short:
            if p > self.tf_initial_stop_loss:
                self.close_position("TF_Initial_SL")
                return
            self.lowest_low_in_trade = min(self.lowest_low_in_trade, self.data.Low[-1])
            self.chandelier_exit_level = (
                self.lowest_low_in_trade + atr * self.tf_chandelier_atr_multiplier
            )
            if p > self.chandelier_exit_level:
                self.close_position("TF_Chandelier")

    def open_mr_position(self, p, is_long):
        # ... (å‡½æ•°å†…å®¹ä¿æŒä¸å˜)
        risk_ps = self.tf_atr[-1] * self.mr_stop_loss_atr_multiplier
        if risk_ps <= 0:
            return
        size = self._calculate_position_size(
            p, risk_ps, self._calculate_dynamic_risk() * self.mr_risk_multiplier
        )
        if size <= 0:
            return
        self.reset_trade_state()
        self.active_sub_strategy = "MR"
        if is_long:
            self.buy(size=size)
            self.mr_stop_loss = p - risk_ps
        else:
            self.sell(size=size)
            self.mr_stop_loss = p + risk_ps

    def manage_mean_reversion_exit(self, p):
        # ... (å‡½æ•°å†…å®¹ä¿æŒä¸å˜)
        if (
            self.position.is_long
            and (p >= self.mr_bb_mid[-1] or p <= self.mr_stop_loss)
        ) or (
            self.position.is_short
            and (p <= self.mr_bb_mid[-1] or p >= self.mr_stop_loss)
        ):
            self.close_position("MR")

    def manage_lpr_exit(self, p):
        # ... (å‡½æ•°å†…å®¹ä¿æŒä¸å˜)
        atr_dist = self.tf_atr[-1] * self.lpr_trail_atr_multiplier
        if self.position.is_long:
            new_stop = self.data.High[-1] - atr_dist
            self.lpr_trailing_stop = max(self.lpr_trailing_stop, new_stop)
            if p <= self.lpr_trailing_stop:
                self.close_position("LPR_Trail_Stop")
        elif self.position.is_short:
            new_stop = self.data.Low[-1] + atr_dist
            self.lpr_trailing_stop = (
                min(self.lpr_trailing_stop, new_stop)
                if self.lpr_trailing_stop > 0
                else new_stop
            )
            if p >= self.lpr_trailing_stop:
                self.close_position("LPR_Trail_Stop")

    def close_position(self, reason: str):
        # ... (å‡½æ•°å†…å®¹ä¿æŒä¸å˜)
        self.position.close()
        self.reset_trade_state()

    def _calculate_position_size(self, p, rps, risk_pct):
        # ... (å‡½æ•°å†…å®¹ä¿æŒä¸å˜)
        if rps <= 0 or p <= 0:
            return 0
        cash_at_risk = risk_pct * self.equity
        risk_pct_per_unit = rps / p
        if risk_pct_per_unit == 0:
            return 0
        position_size_quote = cash_at_risk / risk_pct_per_unit
        size = position_size_quote / self.equity
        return min(size, 0.99)

    def _calculate_dynamic_risk(self):
        # ... (å‡½æ•°å†…å®¹ä¿æŒä¸å˜)
        trades = self.closed_trades
        if len(trades) < self.kelly_trade_history:
            return self.default_risk_pct * self.vol_weight
        recent_trades = trades[-self.kelly_trade_history :]
        returns = [t.pl_pct for t in recent_trades]
        wins, losses = [r for r in returns if r > 0], [r for r in returns if r < 0]
        if not wins or not losses:
            return self.default_risk_pct * self.vol_weight
        win_rate, avg_win, avg_loss = (
            len(wins) / len(recent_trades),
            sum(wins) / len(wins),
            abs(sum(losses) / len(losses)),
        )
        if avg_loss == 0:
            return self.max_risk_pct
        reward_ratio = avg_win / avg_loss
        if reward_ratio == 0:
            return self.default_risk_pct * self.vol_weight
        kelly = win_rate - (1 - win_rate) / reward_ratio
        return min(max(0.005, kelly * 0.5) * self.vol_weight, self.max_risk_pct)


# --- MAIN EXECUTION BLOCK ---

if __name__ == "__main__":
    set_chinese_font()
    CACHE_DIR = "data_cache"
    logger.info(f"ğŸš€ (V49.0) å¼€å§‹è¿è¡ŒæŠ—è¿‡æ‹Ÿåˆé‡æ„ç‰ˆ...")
    backtest_start_dt, backtest_end_dt = pd.to_datetime(
        CONFIG["backtest_start_date"]
    ), pd.to_datetime(CONFIG["backtest_end_date"])
    training_window = timedelta(days=CONFIG["training_window_days"])
    overall_start_date = backtest_start_dt - training_window
    logger.info(
        f"æ•°æ®è·å–æ€»æ—¶é—´æ®µ: {overall_start_date.date()} to {backtest_end_dt.date()}"
    )

    all_featured_data = {}
    btc_data = fetch_binance_klines(
        CONFIG["btc_symbol"],
        CONFIG["interval"],
        overall_start_date,
        backtest_end_dt,
        cache_dir=CACHE_DIR,
    )

    for s in CONFIG["symbols_to_test"]:
        raw_data = fetch_binance_klines(
            s,
            CONFIG["interval"],
            overall_start_date,
            backtest_end_dt,
            cache_dir=CACHE_DIR,
        )
        if raw_data.empty:
            logger.error(f"{s} æ•°æ®è·å–å¤±è´¥ï¼Œè·³è¿‡ã€‚")
            continue
        logger.info(f"ä¸º {s} æ·»åŠ æ‰€æœ‰ç‰¹å¾...")
        all_featured_data[s] = add_all_features(raw_data, btc_df=btc_data)

    if not all_featured_data:
        logger.error("æ‰€æœ‰å“ç§æ•°æ®å¤„ç†å¤±è´¥ï¼Œç¨‹åºç»ˆæ­¢ã€‚")
        exit()

    walk_forward_periods = pd.date_range(
        start=backtest_start_dt, end=backtest_end_dt, freq="3MS"
    )
    all_equity_curves, all_trades, all_model_accuracies = [], pd.DataFrame(), []
    final_equity = CONFIG["initial_cash"]

    for i, period_start in enumerate(walk_forward_periods):
        period_end = (
            walk_forward_periods[i + 1] - timedelta(seconds=1)
            if i + 1 < len(walk_forward_periods)
            else backtest_end_dt
        )
        training_end_dt = period_start - timedelta(seconds=1)

        # [V49.0 æ ¸å¿ƒä¿®æ”¹] åˆ›å»ºå¸¦æœ‰éš”ç¦»æœŸçš„è®­ç»ƒæ•°æ®ç»“æŸç‚¹
        gap_days = CONFIG.get("ml_training_gap_days", 30)
        training_end_with_gap = training_end_dt - timedelta(days=gap_days)

        print(
            "\n"
            + "#" * 80
            + f"\næ»šåŠ¨å‘¨æœŸ {i+1}: å›æµ‹ {period_start.date()} to {period_end.date()}\n"
            + "#" * 80
        )

        for symbol, data in all_featured_data.items():
            period_accuracies = {}
            if (
                STRATEGY_PARAMS["ml_filter_enabled"]
                and CONFIG["enabled_modules"].get("ml_filter", False)
                and ML_LIBS
            ):
                logger.info(
                    f"å‡†å¤‡è®­ç»ƒæ•°æ®ï¼Œæˆªæ­¢äº: {training_end_with_gap.date()} (å·²åº”ç”¨ {gap_days}-day éš”ç¦»æœŸ)"
                )
                training_slice = data.loc[
                    :training_end_with_gap
                ]  # ä½¿ç”¨å¸¦æœ‰éš”ç¦»æœŸçš„æ•°æ®

                if len(training_slice) > 1000:
                    acc_lstm = train_and_save_lstm(
                        training_slice,
                        symbol,
                        STRATEGY_PARAMS["lstm_sequence_length"],
                        STRATEGY_PARAMS["lstm_epochs"],
                        STRATEGY_PARAMS["lstm_batch_size"],
                    )
                    acc_xgb = train_and_save_xgb(training_slice, symbol)
                    if acc_lstm is not None:
                        period_accuracies["lstm"] = acc_lstm
                    if acc_xgb is not None:
                        period_accuracies["xgb"] = acc_xgb
                else:
                    logger.warning(f"[{symbol}] è®­ç»ƒæ•°æ®ä¸è¶³ (<1000)ï¼Œè·³è¿‡æœ¬å‘¨æœŸè®­ç»ƒã€‚")
            if period_accuracies:
                all_model_accuracies.append(period_accuracies)

            backtest_slice = data.loc[period_start:period_end].copy().dropna()
            if backtest_slice.empty:
                continue

            logger.info(f"å¼€å§‹å›æµ‹ {symbol}...")
            bt = Backtest(
                backtest_slice,
                UltimateStrategy,
                cash=final_equity,
                commission=CONFIG["commission"],
                finalize_trades=True,
            )
            # [V49.0 ä¿®æ”¹] å°†æ¨¡å—å¼€å…³é…ç½®ä¼ å…¥ç­–ç•¥
            stats = bt.run(symbol=symbol, enabled_modules=CONFIG["enabled_modules"])

            final_equity = stats["Equity Final [$]"]
            all_equity_curves.append(stats["_equity_curve"])
            if "_trades" in stats and not stats["_trades"].empty:
                trades_df = stats["_trades"]
                trades_df["Rolling Period"] = i + 1
                all_trades = (
                    pd.concat([all_trades, trades_df], ignore_index=True)
                    if not all_trades.empty
                    else trades_df
                )
            print(stats.get("_trades", "æœ¬å‘¨æœŸæ— äº¤æ˜“ã€‚"))

    if not all_trades.empty:
        final_stats = {
            "Profit Factor": (
                all_trades[all_trades["PnL"] > 0]["PnL"].sum()
                / abs(all_trades[all_trades["PnL"] < 0]["PnL"].sum())
            ),
            "Win Rate [%]": len(all_trades[all_trades["ReturnPct"] > 0])
            / len(all_trades)
            * 100,
            "# Trades": len(all_trades),
        }
        logger.info(
            "\n" + "#" * 80 + "\n                 æ»šåŠ¨å›æµ‹è¡¨ç°æ€»è§ˆ\n" + "#" * 80
        )
        logger.info(f"æ€»åˆå§‹èµ„é‡‘: ${CONFIG['initial_cash']:,.2f}")
        logger.info(f"æ€»æœ€ç»ˆæƒç›Š: ${final_equity:,.2f}")
        logger.info(
            f"æ€»å›æŠ¥ç‡: {(final_equity / CONFIG['initial_cash'] - 1) * 100:.2f}%"
        )
        logger.info(f"æ€»äº¤æ˜“æ¬¡æ•°: {final_stats['# Trades']}")
        logger.info(f"æ•´ä½“èƒœç‡: {final_stats['Win Rate [%]']:.2f}%")
        logger.info(f"æ•´ä½“ç›ˆäºå› å­ (Profit Factor): {final_stats['Profit Factor']:.2f}")
        plot_walk_forward_equity(all_equity_curves, all_trades, CONFIG["initial_cash"])
        analyze_trade_distribution(all_trades)
        generate_optimization_suggestions(final_stats, all_model_accuracies, CONFIG)
    else:
        logger.info("æ•´ä¸ªå›æµ‹æœŸé—´æ²¡æœ‰äº§ç”Ÿä»»ä½•äº¤æ˜“ã€‚")
