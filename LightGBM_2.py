import numpy as np
import pandas as pd
import requests
import time
import logging
import pandas_ta as ta
import os
import joblib
import lightgbm as lgb
from sklearn.preprocessing import MinMaxScaler
from sklearn.metrics import (
    accuracy_score,
    precision_score,
    recall_score,
    f1_score,
    precision_recall_curve,
    confusion_matrix,
)
from tqdm import tqdm
from scipy.signal import find_peaks

# --- 0. è®¾ç½® ---
logging.basicConfig(
    level=logging.INFO, format="%(asctime)s - %(levelname)s - %(message)s"
)
logger = logging.getLogger(__name__)

# --- ğŸš€ å…¨å±€é…ç½® ---
SYMBOL = "ETHUSDT"
INTERVAL = "15m"
DATA_START_DATE = "2017-01-01"
TRAIN_START = "2018-01-01"
VALIDATION_START = "2024-01-01"
TEST_START = "2025-01-01"
TEST_END = "2025-11-09"
LOOK_BACK = 60
TREND_CONFIG = {
    "look_forward_steps": 5,
    "target_return": 0.003,
}

# --- æ–‡ä»¶è·¯å¾„ ---
MODELS_DIR, DATA_DIR = "models_gbm", "data"
MODEL_SAVE_PATH = os.path.join(
    MODELS_DIR, f"eth_model_high_precision_v3_{INTERVAL}.joblib"
)
SCALER_SAVE_PATH = os.path.join(
    MODELS_DIR, f"eth_scaler_high_precision_v3_{INTERVAL}.joblib"
)
FEATURE_COLUMNS_PATH = os.path.join(
    MODELS_DIR, f"feature_columns_high_precision_v3_{INTERVAL}.joblib"
)
FLATTENED_COLUMNS_PATH = os.path.join(
    MODELS_DIR, f"flattened_columns_high_precision_v3_{INTERVAL}.joblib"
)
DATA_CACHE_PATH = os.path.join(DATA_DIR, f"{SYMBOL.lower()}_{INTERVAL}_data.csv")

os.makedirs(MODELS_DIR, exist_ok=True)
os.makedirs(DATA_DIR, exist_ok=True)


# --- æ•°æ®è·å–å‡½æ•° ---
def fetch_binance_klines(s, i, st, en=None, l=1000):
    url, cols = "https://api.binance.com/api/v3/klines", [
        "timestamp",
        "Open",
        "High",
        "Low",
        "Close",
        "Volume",
        "close_time",
        "quote_asset_volume",
        "number_of_trades",
        "taker_buy_base_volume",
        "taker_buy_quote_volume",
        "ignore",
    ]
    sts, ets = int(pd.to_datetime(st).timestamp() * 1000), (
        int(pd.to_datetime(en).timestamp() * 1000) if en else int(time.time() * 1000)
    )
    all_d, retries, max_retries = [], 0, 5
    while sts < ets:
        try:
            r = requests.get(
                url,
                params={
                    "symbol": s.upper(),
                    "interval": i,
                    "startTime": sts,
                    "endTime": ets,
                    "limit": l,
                },
                timeout=15,
            )
            r.raise_for_status()
            d = r.json()
            if not d:
                break
            all_d.extend(d)
            sts = d[-1][0] + 1
            retries = 0
        except requests.exceptions.RequestException as e:
            retries += 1
            if retries > max_retries:
                logger.error(f"è·å–æ•°æ®å¤±è´¥è¶…è¿‡æœ€å¤§é‡è¯•æ¬¡æ•°: {e}")
                return pd.DataFrame()
            logger.warning(
                f"è·å–æ•°æ®å¤±è´¥ï¼Œæ­£åœ¨é‡è¯• ({retries}/{max_retries})... Error: {e}"
            )
            time.sleep(retries * 2)
            continue
    if not all_d:
        return pd.DataFrame()
    df = pd.DataFrame(all_d, columns=cols)[
        ["timestamp", "Open", "High", "Low", "Close", "Volume"]
    ].copy()
    df["timestamp"] = pd.to_datetime(df["timestamp"], unit="ms")
    for col in df.columns[1:]:
        df[col] = pd.to_numeric(df[col], errors="coerce")
    logger.info(f"âœ… è·å– {s} æ•°æ®æˆåŠŸ: {len(df)} æ¡")
    return df.set_index("timestamp").sort_index()


# --- ğŸš€ 1. å‡çº§ç‰ˆç‰¹å¾å·¥ç¨‹ ---
def get_market_structure_features(df, order=5):
    df = df.copy()
    high_peaks_idx, _ = find_peaks(
        df["High"], distance=order, prominence=df["High"].std() * 0.5
    )
    low_peaks_idx, _ = find_peaks(
        -df["Low"], distance=order, prominence=df["Low"].std() * 0.5
    )

    df["swing_high_price"] = np.nan
    df.iloc[high_peaks_idx, df.columns.get_loc("swing_high_price")] = df.iloc[
        high_peaks_idx
    ]["High"]
    df["swing_low_price"] = np.nan
    df.iloc[low_peaks_idx, df.columns.get_loc("swing_low_price")] = df.iloc[
        low_peaks_idx
    ]["Low"]

    df["swing_high_price"] = df["swing_high_price"].ffill()
    df["swing_low_price"] = df["swing_low_price"].ffill()

    df["is_uptrend"] = (
        (df["swing_high_price"] > df["swing_high_price"].shift(1))
        & (df["swing_low_price"] > df["swing_low_price"].shift(1))
    ).astype(int)
    df["is_downtrend"] = (
        (df["swing_high_price"] < df["swing_high_price"].shift(1))
        & (df["swing_low_price"] < df["swing_low_price"].shift(1))
    ).astype(int)
    df["market_structure"] = df["is_uptrend"] - df["is_downtrend"]

    return df[["market_structure"]]


def feature_engineering(df):
    """æ•´åˆæ‰€æœ‰ç‰¹å¾è®¡ç®—"""
    # ä¿®å¤ SettingWithCopyWarningï¼šæ˜¾å¼åˆ›å»ºå‰¯æœ¬
    df = df.copy()

    logger.info("--- å¼€å§‹è®¡ç®—ç‰¹å¾ (V3) ---")

    # 1. åŸºç¡€æŠ€æœ¯æŒ‡æ ‡
    df.ta.rsi(length=14, append=True)
    df.ta.macd(fast=12, slow=26, signal=9, append=True)
    df.ta.bbands(length=20, std=2, append=True)
    df.ta.adx(length=14, append=True)

    # 2. é¢å¤–ç‰¹å¾
    df["volatility"] = (
        (np.log(df["Close"] / df["Close"].shift(1))).rolling(window=20).std()
    )

    # 3. ğŸš€ æ–°å¢ï¼šå¸‚åœºç»“æ„ç‰¹å¾
    market_structure_df = get_market_structure_features(df)

    # 4. ğŸš€ æ–°å¢ï¼šé•¿å‘¨æœŸMACDä½œä¸ºè¿‡æ»¤å™¨ï¼ˆç‰¹å¾ä¹ŸåŠ å…¥æ¨¡å‹ï¼‰
    df.ta.macd(
        fast=24,
        slow=52,
        signal=18,
        append=True,
        col_names=("MACD_long", "MACDh_long", "MACDs_long"),
    )

    all_features_df = df.drop(columns=["Open", "High", "Low", "Close", "Volume"])
    all_features_df = pd.concat([all_features_df, market_structure_df], axis=1)

    all_features_df.replace([np.inf, -np.inf], np.nan, inplace=True)
    return all_features_df


# --- æ ‡ç­¾ä¸åºåˆ—å‡½æ•° ---
# --- æ ‡ç­¾ä¸åºåˆ—å‡½æ•° ---
def create_trend_labels(df, look_forward_steps=5, target_return=0.01):
    """
    æ ¹æ®æœªæ¥Næ ¹Kçº¿å†…æ˜¯å¦è¾¾åˆ°ç›®æ ‡æ”¶ç›Šç‡æ¥åˆ›å»ºæ ‡ç­¾ã€‚
    - å¦‚æœæœªæ¥ look_forward_steps æ ¹Kçº¿å†…çš„æœ€é«˜ä»· >= å½“å‰æ”¶ç›˜ä»· * (1 + target_return)ï¼Œåˆ™æ ‡ç­¾ä¸º 1ã€‚
    - å¦åˆ™ä¸º 0ã€‚
    """
    df_copy = df.copy()

    # è®¡ç®—æ¯æ ¹Kçº¿çš„ç›®æ ‡ä»·æ ¼
    df_copy["target_price"] = df_copy["Close"] * (1 + target_return)

    # æ‰¾åˆ°æœªæ¥ N æ ¹Kçº¿å†…çš„æœ€é«˜ä»·
    # .rolling() æ˜¯å‘åçœ‹çš„ï¼Œæ‰€ä»¥æˆ‘ä»¬ç”¨ .shift() å°†çª—å£å‘å‰ç§»åŠ¨
    df_copy["future_max_high"] = (
        df_copy["High"]
        .rolling(window=look_forward_steps)
        .max()
        .shift(-look_forward_steps)
    )

    # å¦‚æœæœªæ¥æœ€é«˜ä»·è¾¾åˆ°äº†ç›®æ ‡ä»·ï¼Œåˆ™æ ‡ç­¾ä¸º1
    df_copy["label"] = (df_copy["future_max_high"] >= df_copy["target_price"]).astype(
        int
    )

    # è¿”å›å¸¦æœ‰æ ‡ç­¾çš„DataFrameï¼Œå¯ä»¥åªä¿ç•™labelåˆ—æˆ–å…¨éƒ¨è¿”å›
    return df_copy.drop(columns=["target_price", "future_max_high"])


def create_flattened_sequences(data, labels, look_back=60):
    X, y = [], []
    for i in range(len(data) - look_back):
        X.append(data[i : (i + look_back), :].flatten())
        y.append(labels[i + look_back])
    return np.array(X, dtype=np.float32), np.array(y, dtype=np.int32)


# --- ğŸš€ 3. å‡çº§ç‰ˆè®­ç»ƒä¸éªŒè¯å‡½æ•° ---
def train_and_validate(train_df, validation_df, look_back, trend_config):
    logger.info("--- å¼€å§‹è®­ç»ƒå’ŒéªŒè¯æµç¨‹ (V3) ---")

    X_train_full_df = feature_engineering(train_df).dropna()
    X_validation_full_df = feature_engineering(validation_df).dropna()

    train_labeled = create_trend_labels(train_df, **trend_config)
    validation_labeled = create_trend_labels(validation_df, **trend_config)

    # --- ä¿®å¤ ValueErrorï¼šåœ¨ align è°ƒç”¨ä¸­æ·»åŠ  axis=0 ---
    y_train_full = train_labeled["label"].align(X_train_full_df, join="inner", axis=0)[
        0
    ]
    X_train_full_df = X_train_full_df.align(
        train_labeled["label"], join="inner", axis=0
    )[0]

    y_validation_full = validation_labeled["label"].align(
        X_validation_full_df, join="inner", axis=0
    )[0]
    X_validation_full_df = X_validation_full_df.align(
        validation_labeled["label"], join="inner", axis=0
    )[0]

    X_validation_full_df = X_validation_full_df[X_train_full_df.columns]

    scaler = MinMaxScaler(feature_range=(0, 1))
    X_train_scaled = scaler.fit_transform(X_train_full_df)
    X_validation_scaled = scaler.transform(X_validation_full_df)

    original_columns = X_train_full_df.columns
    flattened_columns = [
        f"{col}_lag_{lag}"
        for lag in range(look_back - 1, -1, -1)
        for col in original_columns
    ]
    joblib.dump(original_columns, FEATURE_COLUMNS_PATH)
    joblib.dump(flattened_columns, FLATTENED_COLUMNS_PATH)

    X_train_np, y_train = create_flattened_sequences(
        X_train_scaled, y_train_full.values, look_back
    )
    X_validation_np, y_validation = create_flattened_sequences(
        X_validation_scaled, y_validation_full.values, look_back
    )

    X_train_df = pd.DataFrame(X_train_np, columns=flattened_columns)
    X_validation_df = pd.DataFrame(X_validation_np, columns=flattened_columns)

    logger.info(f"è®­ç»ƒæ ·æœ¬: {len(X_train_df)}, éªŒè¯æ ·æœ¬: {len(X_validation_df)}")

    train_label_counts = np.bincount(y_train)
    if train_label_counts.size < 2 or train_label_counts[1] == 0:
        logger.error("è®­ç»ƒæ•°æ®ä¸­æ²¡æœ‰æ­£æ ·æœ¬(label=1)ï¼Œæ— æ³•ç»§ç»­ã€‚")
        return None, None, None

    precision_focus_ratio = 0.7
    scale_pos_weight = (
        train_label_counts[0] / train_label_counts[1]
    ) * precision_focus_ratio
    logger.info(f"è°ƒæ•´åçš„ scale_pos_weight (è¿½æ±‚é«˜èƒœç‡): {scale_pos_weight:.2f}")

    lgb_params = {
        "objective": "binary",
        "metric": "logloss",
        "n_estimators": 2000,
        "learning_rate": 0.02,
        "num_leaves": 20,
        "max_depth": 5,
        "seed": 42,
        "n_jobs": -1,
        "verbose": -1,
        "scale_pos_weight": scale_pos_weight,
        "colsample_bytree": 0.7,
        "subsample": 0.7,
        "reg_alpha": 0.1,
    }
    lgb_model = lgb.LGBMClassifier(**lgb_params)

    logger.info("\nå¼€å§‹è®­ç»ƒ LightGBM æ¨¡å‹ (V3)...")
    lgb_model.fit(
        X_train_df,
        y_train,
        eval_set=[(X_validation_df, y_validation)],
        eval_metric="logloss",
        callbacks=[lgb.early_stopping(stopping_rounds=100, verbose=False)],
    )

    y_val_pred_probs = lgb_model.predict_proba(X_validation_df)[:, 1]
    precisions, recalls, thresholds = precision_recall_curve(
        y_validation, y_val_pred_probs
    )
    f1_scores = np.divide(
        2 * recalls * precisions,
        recalls + precisions,
        out=np.zeros_like(recalls),
        where=(recalls + precisions) != 0,
    )
    best_f1_idx = np.argmax(f1_scores)
    best_f1_threshold = thresholds[best_f1_idx] if len(thresholds) > 0 else 0.5
    logger.info(f"åœ¨éªŒè¯é›†ä¸Šæ‰¾åˆ°çš„æœ€ä½³F1é˜ˆå€¼: {best_f1_threshold:.4f}")

    joblib.dump(lgb_model, MODEL_SAVE_PATH)
    joblib.dump(scaler, SCALER_SAVE_PATH)
    logger.info(f"æ¨¡å‹å·²ä¿å­˜åˆ°: {MODEL_SAVE_PATH}")

    return lgb_model, scaler, best_f1_threshold


# --- ğŸš€ 4. å‡çº§ç‰ˆå›æµ‹è¯„ä¼°å‡½æ•° (å·²å¢åŠ æ–°åŠŸèƒ½) ---
def run_backtest_and_evaluate(
    test_df, model, scaler, look_back, threshold, trend_config
):
    logger.info(
        "\n" + "=" * 60 + "\n--- å¼€å§‹åœ¨æµ‹è¯•é›†ä¸Šè¿›è¡Œä¸¥æ ¼çš„å›æµ‹è¯„ä¼° (V3) ---\n" + "=" * 60
    )

    original_columns = joblib.load(FEATURE_COLUMNS_PATH)
    flattened_columns = joblib.load(FLATTENED_COLUMNS_PATH)

    test_features_df = feature_engineering(test_df).dropna()

    test_features_aligned = test_features_df.reindex(
        columns=original_columns, fill_value=0
    )
    test_scaled = scaler.transform(test_features_aligned)

    final_signals = []

    logger.info("é€æ ¹Kçº¿éå†æµ‹è¯•é›†è¿›è¡Œé¢„æµ‹å’Œè¿‡æ»¤...")
    for i in tqdm(range(look_back, len(test_scaled))):
        current_timestamp = test_features_df.index[i]

        input_sequence = test_scaled[i - look_back : i, :]
        input_flattened_np = input_sequence.flatten().reshape(1, -1)
        input_df = pd.DataFrame(input_flattened_np, columns=flattened_columns)
        pred_prob = model.predict_proba(input_df)[0][1]
        model_signal = 1 if pred_prob > threshold else 0

        macd_long = test_features_df.loc[current_timestamp, "MACD_long"]
        macds_long = test_features_df.loc[current_timestamp, "MACDs_long"]
        is_trend_confirmed = (macd_long > macds_long) and (macd_long > 0)

        final_signal = 1 if (model_signal == 1 and is_trend_confirmed) else 0
        final_signals.append(final_signal)

    actual_labels_df = create_trend_labels(test_df, **trend_config).dropna()

    pred_index = test_features_df.index[look_back : look_back + len(final_signals)]
    pred_series = pd.Series(final_signals, index=pred_index)
    results_df = pd.DataFrame(actual_labels_df["label"]).join(
        pred_series.to_frame("final_signal"), how="inner"
    )

    if results_df.empty or np.sum(results_df["final_signal"]) == 0:
        logger.warning("å›æµ‹æœŸé—´æ²¡æœ‰äº§ç”Ÿä»»ä½•äº¤æ˜“ä¿¡å·ï¼Œæ— æ³•è®¡ç®—èƒœç‡ã€‚")
        return

    y_test_actual = results_df["label"].values
    y_pred_final = results_df["final_signal"].values

    # --- [æ–°åŠŸèƒ½] è®¡ç®—ç›ˆåˆ©å‰æœ€å¤§å›æ’¤ ---
    winning_trades_drawdown = []
    # æ‰¾åˆ°æ‰€æœ‰æœ€ç»ˆç›ˆåˆ©çš„äº¤æ˜“ä¿¡å·
    winning_signals_df = results_df[
        (results_df["final_signal"] == 1) & (results_df["label"] == 1)
    ]

    if not winning_signals_df.empty:
        logger.info("æ­£åœ¨è®¡ç®—ç›ˆåˆ©ä¿¡å·åœ¨ç›ˆåˆ©å‰çš„æœ€å¤§å›æ’¤...")
        look_forward = trend_config["look_forward_steps"]

        # ä¸ºäº†é«˜æ•ˆè®¡ç®—ï¼Œæˆ‘ä»¬åœ¨æµ‹è¯•é›†ä¸Šé¢„å…ˆè®¡ç®—å¥½æœªæ¥Næ ¹Kçº¿çš„æœ€ä½ä»·
        test_df_copy = test_df.copy()
        test_df_copy["future_min_low"] = (
            test_df_copy["Low"].rolling(window=look_forward).min().shift(-look_forward)
        )

        # å°†åŒ…å«æœªæ¥æœ€ä½ä»·çš„åˆ—åˆå¹¶åˆ°æˆ‘ä»¬çš„ç›ˆåˆ©ä¿¡å·DataFrameä¸­
        winning_trades_details = winning_signals_df.join(
            test_df_copy[["Close", "future_min_low"]], how="inner"
        )

        # è®¡ç®—æ¯ç¬”ç›ˆåˆ©äº¤æ˜“çš„å›æ’¤ç™¾åˆ†æ¯”
        winning_trades_details["drawdown_pct"] = (
            (winning_trades_details["Close"] - winning_trades_details["future_min_low"])
            / winning_trades_details["Close"]
        ) * 100

        winning_trades_drawdown = winning_trades_details["drawdown_pct"].tolist()

    # --- æ‰“å°è¯„ä¼°ç»“æœ ---
    print("\n--- [å®¢è§‚] æµ‹è¯•é›†å›æµ‹è¯„ä¼°ç»“æœ (åº”ç”¨MACDè¿‡æ»¤å™¨å) ---")
    print(f"æ€»å›æµ‹Kçº¿æ•°: {len(y_pred_final)}")
    print(f"å‘å‡ºçœ‹æ¶¨ä¿¡å·æ€»æ¬¡æ•° (äº¤æ˜“é¢‘ç‡): {np.sum(y_pred_final)}")
    print(f"ç²¾ç¡®ç‡ (èƒœç‡): {precision_score(y_test_actual, y_pred_final):.4f}")
    print(f"å¬å›ç‡ (ç›ˆåˆ©æœºä¼šæ•æ‰ç‡): {recall_score(y_test_actual, y_pred_final):.4f}")
    print("\næ··æ·†çŸ©é˜µ (TN, FP / FN, TP):")
    print(confusion_matrix(y_test_actual, y_pred_final))

    # --- [æ–°åŠŸèƒ½] æ‰“å°ç›ˆåˆ©å‰æœ€å¤§å›æ’¤çš„ç»Ÿè®¡æ•°æ® ---
    if winning_trades_drawdown:
        print("\n--- [æ–°æŒ‡æ ‡] ç›ˆåˆ©äº¤æ˜“åœ¨ç›ˆåˆ©å‰çš„æœ€å¤§å›æ’¤åˆ†æ ---")
        print(f"ç›ˆåˆ©çš„äº¤æ˜“æ€»æ•°: {len(winning_trades_drawdown)}")
        print(f"å¹³å‡å›æ’¤: {np.mean(winning_trades_drawdown):.4f}%")
        print(f"æœ€å¤§å›æ’¤ (æœ€å·®æƒ…å†µ): {np.max(winning_trades_drawdown):.4f}%")
        print(f"æœ€å°å›æ’¤ (æœ€å¥½æƒ…å†µ): {np.min(winning_trades_drawdown):.4f}%")
    else:
        print("\n--- [æ–°æŒ‡æ ‡] ç›ˆåˆ©äº¤æ˜“åœ¨ç›ˆåˆ©å‰çš„æœ€å¤§å›æ’¤åˆ†æ ---")
        print("æœ¬æ¬¡å›æµ‹æ²¡æœ‰äº§ç”Ÿä»»ä½•ç›ˆåˆ©çš„äº¤æ˜“ï¼Œæ— æ³•è®¡ç®—ç›¸å…³æŒ‡æ ‡ã€‚")


# --- ä¸»æµç¨‹ ---
if __name__ == "__main__":
    if os.path.exists(DATA_CACHE_PATH):
        logger.info(f"ä»ç¼“å­˜åŠ è½½æ•°æ®: {DATA_CACHE_PATH}")
        raw_df = pd.read_csv(DATA_CACHE_PATH, index_col=0, parse_dates=True)
    else:
        raw_df = fetch_binance_klines(
            s=SYMBOL, i=INTERVAL, st=DATA_START_DATE, en=TEST_END
        )
        if not raw_df.empty:
            raw_df.to_csv(DATA_CACHE_PATH)

    if raw_df.empty:
        logger.error("æ•°æ®ä¸ºç©ºï¼Œç¨‹åºé€€å‡ºã€‚")
        exit()

    # --- [å·²ä¿®æ­£] ä¸¥æ ¼çš„æ•°æ®é›†åˆ’åˆ† ---
    train_df = raw_df[(raw_df.index >= TRAIN_START) & (raw_df.index < VALIDATION_START)]
    validation_df = raw_df[
        (raw_df.index >= VALIDATION_START) & (raw_df.index < TEST_START)
    ]
    test_df = raw_df[(raw_df.index >= TEST_START) & (raw_df.index <= TEST_END)]

    logger.info(
        f"æ•°æ®é›†åˆ’åˆ†å®Œæˆ: {len(train_df)} è®­ç»ƒ, {len(validation_df)} éªŒè¯, {len(test_df)} æµ‹è¯•ã€‚"
    )

    trained_model, trained_scaler, best_threshold = train_and_validate(
        train_df, validation_df, LOOK_BACK, TREND_CONFIG
    )

    if trained_model and best_threshold is not None:
        run_backtest_and_evaluate(
            test_df,
            trained_model,
            trained_scaler,
            LOOK_BACK,
            best_threshold,
            TREND_CONFIG,
        )
    else:
        logger.error("æ¨¡å‹è®­ç»ƒå¤±è´¥ï¼Œè·³è¿‡å›æµ‹ã€‚")
