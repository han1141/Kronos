import numpy as np
import pandas as pd

# --- âœ… NumPy 2.x å…¼å®¹è¡¥ä¸ï¼ˆå¿…é¡»åœ¨å¯¼å…¥ pandas_ta ä¹‹å‰æ‰§è¡Œï¼‰ ---
if not hasattr(np, "NaN"):
    np.NaN = np.nan
if not hasattr(np, "Inf"):
    np.Inf = np.inf

import pandas_ta as ta
import pandas_ta.utils as ta_utils

# ç¦ç”¨ TA-Libï¼Œå¼ºåˆ¶ä½¿ç”¨ pandas_ta çš„çº¯ Python å®ç°ï¼Œé¿å… NumPy / TA-Lib äºŒè¿›åˆ¶ä¸å…¼å®¹é—®é¢˜
ta_utils.Imports["talib"] = False
import logging
import os
import joblib
from sklearn.preprocessing import RobustScaler
from tqdm import tqdm
import warnings
import requests
import time

# --- PyTorch Imports ---
import torch
import torch.nn as nn
from torch.utils.data import Dataset, DataLoader
from torch.optim.lr_scheduler import ReduceLROnPlateau

warnings.filterwarnings("ignore", category=UserWarning)

# --- 0. è®¾ç½® ---
logging.basicConfig(
    level=logging.INFO, format="%(asctime)s - %(levelname)s - %(message)s"
)
logger = logging.getLogger(__name__)

# --- ğŸš€ å…¨å±€é…ç½® (V16 - Causal Labels) ---
SYMBOL = "ETHUSDT"
INTERVAL = "30m"
DATA_START_DATE = "2018-01-01"
DATA_END_DATE = "2025-11-12"

RUN_MODE = "train_global"  # "train_global" æˆ– "backtest_global"

# å…¨å±€æ¨¡å‹è®­ç»ƒ/å›æµ‹çš„æ—¶é—´åˆ‡åˆ†
GLOBAL_TRAIN_END_DATE = "2024-01-01"  # è®­ç»ƒæ•°æ®æˆªæ­¢æ—¶é—´ï¼›ä¹‹åçš„æ•°æ®ç”¨äºæ ·æœ¬å¤–å›æµ‹

# 1h å‘¨æœŸä¸‹çš„çª—å£è®¾ç½®
LOOK_BACK = 48   # çº¦ 2 å¤©å†å²çª—å£ (48 * 1h)
HORIZON = 12     # çº¦ 12 æ ¹ K çº¿çš„é¢„æµ‹çª—å£
ALPHA_MIN_EDGE_ATR = 1.5  # å®šä¹‰â€œå¯äº¤æ˜“â€æ ·æœ¬æ‰€éœ€çš„æœ€å°æ³¢åŠ¨å¹…åº¦ï¼ˆATR çš„å€æ•°ï¼‰

# --- äº¤æ˜“ç»“æœæ ‡ç­¾å®šä¹‰ ---
# åœ¨æœªæ¥ HORIZON æ ¹ K çº¿å†…ï¼š
#  - æœ€é«˜ä»·ç›¸å¯¹å½“å‰æ”¶ç›˜ä»·è‡³å°‘ä¸Šæ¶¨ TP_TARGET_PCTï¼ˆä¾‹å¦‚ 1%ï¼‰
#  - ä¸”æœŸé—´æœ€å¤§å›æ’¤ä¸è¶…è¿‡ MAX_DRAWDOWN_PCT
TP_TARGET_PCT = 0.01          # ç›®æ ‡æ¶¨å¹…ï¼ˆ1%ï¼‰
MAX_DRAWDOWN_PCT = 0.01       # å…è®¸çš„æœ€å¤§å›æ’¤ï¼ˆ1%ï¼‰

# --- PyTorch & Training Hyperparameters ---
if torch.cuda.is_available():
    DEVICE = "cuda"
elif hasattr(torch.backends, "mps") and torch.backends.mps.is_available():
    DEVICE = "mps"
else:
    DEVICE = "cpu"
BATCH_SIZE = 256
# å…¨å±€æ¨¡å‹è®­ç»ƒï¼šæœ€å¤š 50 ä¸ª epochï¼Œè‡³å°‘è·‘ MIN_EPOCHS_FOR_EARLY_STOPï¼Œå†ç”±æ—©åœæ§åˆ¶
FINETUNE_MAX_EPOCHS = 50
MIN_EPOCHS_FOR_EARLY_STOP = 10
EARLY_STOP_PATIENCE = 5
EARLY_STOP_MIN_DELTA = 1e-4
LEARNING_RATE = 3e-4
WEIGHT_DECAY = 1e-5
MODEL_CONFIG = {"d_model": 32, "nhead": 4, "num_layers": 2, "dropout": 0.4}

# --- ç­–ç•¥ä¼˜åŒ–å‚æ•° (Strategy Parameters) ---
# 1h å‘¨æœŸä¸‹ï¼Œé€‚åº¦æ¿€è¿›çš„é»˜è®¤é˜ˆå€¼
BACKTEST_QUALITY_THRESHOLD = 0.5   # è´¨é‡é˜ˆå€¼
BACKTEST_EDGE_THRESHOLD = 0.5      # edge æ¦‚ç‡é˜ˆå€¼
COST_PER_TRADE = 0.0015            # å•ç¬”äº¤æ˜“æˆæœ¬å‡è®¾
COOLDOWN_PERIOD = 0                # å†·é™æœŸ

# åŠ¨æ€é˜ˆå€¼æ§åˆ¶ï¼šåœ¨éœ‡è¡å¸‚ï¼ˆåŒºé—´ç›˜æ•´ï¼‰ä¸­è‡ªåŠ¨æé«˜å…¥åœºé—¨æ§›ï¼Œå‡å°‘å°äºäº¤æ˜“
USE_DYNAMIC_THRESHOLDS = True
RANGE_PRICE_POS_LOW = 0.3
RANGE_PRICE_POS_HIGH = 0.7
RANGE_ATR_ROLLING_WINDOW = 200
RANGE_QUALITY_MULTIPLIER = 1.5
RANGE_EDGE_MULTIPLIER = 1.1

# --- æ–‡ä»¶è·¯å¾„ (V16) ---
MODELS_DIR = "models_transformer_v16_causal_1h"
DATA_DIR = "data"
os.makedirs(MODELS_DIR, exist_ok=True)
os.makedirs(DATA_DIR, exist_ok=True)
DATA_CACHE_PATH = os.path.join(
    DATA_DIR, f"{SYMBOL.lower()}_{INTERVAL}_data_{DATA_START_DATE}.csv"
)


# --- æ•°æ®ä¸‹è½½åŠŸèƒ½ ---
def fetch_binance_klines(s, i, st, en=None, l=1000):
    url = "https://api.binance.com/api/v3/klines"
    cols = [
        "timestamp",
        "Open",
        "High",
        "Low",
        "Close",
        "Volume",
        "c1",
        "c2",
        "c3",
        "c4",
        "c5",
        "c6",
    ]
    sts = int(pd.to_datetime(st).timestamp() * 1000)
    ets = int(pd.to_datetime(en).timestamp() * 1000) if en else int(time.time() * 1000)
    all_d = []
    logger.info(f"ä»å¸å®‰ä¸‹è½½ {s} æ•°æ®...")
    while sts < ets:
        try:
            r = requests.get(
                url,
                params={
                    "symbol": s.upper(),
                    "interval": i,
                    "startTime": sts,
                    "limit": l,
                    "endTime": ets,
                },
                timeout=15,
            )
            r.raise_for_status()
            d = r.json()
            if not d:
                break
            all_d.extend(d)
            sts = d[-1][0] + 1
        except requests.exceptions.RequestException as e:
            logger.warning(f"æ•°æ®ä¸‹è½½å¤±è´¥ï¼Œæ­£åœ¨é‡è¯•... é”™è¯¯: {e}")
            time.sleep(5)
    df = pd.DataFrame(all_d, columns=cols)[
        ["timestamp", "Open", "High", "Low", "Close", "Volume"]
    ]
    df["timestamp"] = pd.to_datetime(df["timestamp"], unit="ms")
    for col in df.columns[1:]:
        df[col] = pd.to_numeric(df[col], errors="coerce")
    logger.info(f"âœ… ä¸‹è½½äº† {len(df)} è¡Œæ•°æ®ã€‚")
    return df.set_index("timestamp").sort_index()


# --- âœ… [REBUILT] ç‰¹å¾ä¸æ ‡ç­¾ V16 (Causal) ---
def get_features_v16(df):
    base = df.copy()
    # Ensure talib=False to use pure Python implementation
    base.ta.rsi(length=14, append=True, talib=False)
    base.ta.macd(fast=12, slow=26, signal=9, append=True, talib=False)
    base["ATR_norm"] = (
        ta.atr(base["High"], base["Low"], base["Close"], 14, talib=False)
        / base["Close"]
    )
    ma_200 = base["Close"].rolling(window=200).mean()
    base["trend_dist_200ma"] = (base["Close"] - ma_200) / ma_200

    # ä»·æ ¼åŠ¨é‡ç‰¹å¾ï¼šä¸åŒçª—å£çš„æ”¶ç›Š
    base["ret_1"] = base["Close"].pct_change(1)
    base["ret_3"] = base["Close"].pct_change(3)
    base["ret_6"] = base["Close"].pct_change(6)
    base["ret_12"] = base["Close"].pct_change(12)

    # å®ç°æ³¢åŠ¨ç‡ç‰¹å¾ï¼šçŸ­æœŸ/ä¸­æœŸæ³¢åŠ¨
    close_ret = base["Close"].pct_change()
    base["vol_12"] = close_ret.rolling(12).std()
    base["vol_48"] = close_ret.rolling(48).std()

    # ä»·æ ¼åœ¨è¿‘æœŸé«˜ä½åŒºé—´ä¸­çš„ç›¸å¯¹ä½ç½®
    rolling_high = base["High"].rolling(96).max()
    rolling_low = base["Low"].rolling(96).min()
    base["price_pos_96"] = (base["Close"] - rolling_low) / (
        (rolling_high - rolling_low) + 1e-6
    )

    # æ—¶é—´ç‰¹å¾ï¼šæ—¥å†…å’Œå‘¨å†…å‘¨æœŸ
    if isinstance(base.index, pd.DatetimeIndex):
        hours = base.index.hour
        dows = base.index.dayofweek
        base["hour_sin"] = np.sin(2 * np.pi * hours / 24.0)
        base["hour_cos"] = np.cos(2 * np.pi * hours / 24.0)
        base["dow_sin"] = np.sin(2 * np.pi * dows / 7.0)
        base["dow_cos"] = np.cos(2 * np.pi * dows / 7.0)

    feature_cols = [
        c for c in base.columns if c not in ["Open", "High", "Low", "Close", "Volume"]
    ]
    return base[feature_cols].ffill()


def create_targets_v16(df, horizon):
    """
    åŸºäºæœªæ¥ horizon æ ¹ K çº¿æ„é€ â€œå¥½äº¤æ˜“â€äºŒåˆ†ç±»æ ‡ç­¾ï¼š
    - åœ¨æœªæ¥ horizon æ ¹ K å†…ï¼Œæœ€é«˜ä»·ç›¸å¯¹å½“å‰æ”¶ç›˜ä»·ä¸Šæ¶¨è‡³å°‘ TP_TARGET_PCT
    - ä¸”åœ¨åŒä¸€çª—å£å†…ï¼Œæœ€ä½ä»·ç›¸å¯¹å½“å‰æ”¶ç›˜ä»·çš„å›æ’¤ä¸è¶…è¿‡ MAX_DRAWDOWN_PCT
    æ»¡è¶³ä¸Šè¿°æ¡ä»¶è®°ä¸º 1ï¼Œå¦åˆ™ä¸º 0ã€‚å°¾éƒ¨ä¸è¶³ horizon çš„æ ·æœ¬æ ‡ç­¾è®°ä¸º NaNã€‚
    """
    df = df.copy()
    close = df["Close"].values
    high = df["High"].values
    low = df["Low"].values
    n = len(df)

    labels = np.full(n, np.nan, dtype=float)
    for i in range(n - horizon):
        entry = close[i]
        if not np.isfinite(entry):
            continue
        window_high = high[i + 1 : i + 1 + horizon]
        window_low = low[i + 1 : i + 1 + horizon]
        if len(window_high) == 0 or len(window_low) == 0:
            continue
        max_up = window_high.max() / entry - 1.0
        max_drawdown = window_low.min() / entry - 1.0
        # æ¡ä»¶ï¼šæœªæ¥æœ€é«˜ä»·è‡³å°‘ä¸Šæ¶¨ TP_TARGET_PCTï¼Œä¸”æœ€å¤§å›æ’¤ä¸è¶…è¿‡ -MAX_DRAWDOWN_PCT
        if (max_up >= TP_TARGET_PCT) and (max_drawdown >= -MAX_DRAWDOWN_PCT):
            labels[i] = 1.0
        else:
            labels[i] = 0.0

    edge_score = pd.Series(labels, index=df.index)
    # ä¸ºäº†å…¼å®¹ç°æœ‰æ¥å£ï¼Œquality_score ä¸ edge_score ç›¸åŒï¼Œä½†åœ¨æŸå¤±å‡½æ•°ä¸­åªä½¿ç”¨ edge_score
    quality_score = edge_score.copy()
    return quality_score, edge_score


# --- âœ… [REBUILT] PyTorch ç³»ç»Ÿ V16 ---
class CausalDataset(Dataset):
    def __init__(self, features, quality_labels, edge_labels, seq_len):
        self.features, self.quality, self.edge, self.seq_len = (
            features,
            quality_labels,
            edge_labels,
            seq_len,
        )

    def __len__(self):
        return len(self.features) - self.seq_len + 1

    def __getitem__(self, idx):
        end = idx + self.seq_len
        # ä½¿ç”¨ [idx, end) è¿™ä¸€æ®µå†å²ä½œä¸ºè¾“å…¥ï¼Œ
        # æ ‡ç­¾å¯¹é½åˆ°åºåˆ—æœ«ç«¯æ—¶é—´ç‚¹ end-1ï¼Œé¿å…æœªæ¥ä¿¡æ¯æ³„éœ²
        return (
            torch.tensor(self.features[idx:end], dtype=torch.float32),
            torch.tensor(self.quality[end - 1], dtype=torch.float32),
            torch.tensor(self.edge[end - 1], dtype=torch.float32),
        )


class CausalTransformer(nn.Module):
    def __init__(self, input_dim, d_model, nhead, num_layers, dropout):
        super(CausalTransformer, self).__init__()
        self.input_projection = nn.Linear(input_dim, d_model)
        encoder_layers = nn.TransformerEncoderLayer(
            d_model, nhead, d_model * 4, dropout, batch_first=True, activation="gelu"
        )
        self.transformer_encoder = nn.TransformerEncoder(encoder_layers, num_layers)
        self.quality_head = nn.Linear(d_model, 1)
        self.edge_head = nn.Sequential(nn.Linear(d_model, 1), nn.Sigmoid())

    def forward(self, src):
        encoded_seq = self.transformer_encoder(self.input_projection(src))
        context_vector = encoded_seq[:, 0, :]
        pred_quality = self.quality_head(context_vector)
        pred_edge = self.edge_head(context_vector)
        return pred_quality, pred_edge


class CombinedLossV16(nn.Module):
    """
    ä»…ä½¿ç”¨â€œå¥½äº¤æ˜“â€æ ‡ç­¾åšäºŒåˆ†ç±»äº¤å‰ç†µï¼š
    - pred_edge: æ¨¡å‹è¾“å‡ºçš„å¥½äº¤æ˜“æ¦‚ç‡
    - target_edge: 0/1 æ ‡ç­¾ï¼ˆä¸Šæ¶¨è‡³å°‘ TP_TARGET_PCT ä¸”å›æ’¤ä¸è¶…è¿‡ MAX_DRAWDOWN_PCTï¼‰
    """

    def __init__(self, bce_weight=1.0):
        super(CombinedLossV16, self).__init__()
        self.bce = nn.BCELoss()
        self.bce_w = bce_weight

    def forward(self, pred_quality, pred_edge, target_quality, target_edge):
        # ä»…æ ¹æ® target_edge æ˜¯å¦ä¸º NaN æ¥é€‰æ‹©æœ‰æ•ˆæ ·æœ¬
        valid_mask = ~target_edge.isnan()
        if valid_mask.sum() == 0:
            return torch.tensor(float("nan"), device=pred_edge.device)
        pe = pred_edge[valid_mask].clamp(1e-6, 1 - 1e-6)
        te = target_edge[valid_mask].clamp(0.0, 1.0)
        loss_bce = self.bce(pe, te)
        return self.bce_w * loss_bce


def backtest_period_v16(
    model,
    scaler,
    test_df,
    quality_threshold,
    edge_threshold,
    cost_per_trade,
    cooldown_period,
):
    model.eval().to(DEVICE)
    features = get_features_v16(test_df)
    if features.empty or len(features) < LOOK_BACK:
        return {"pnl": 0, "win_rate": 0, "num_trades": 0}

    # è®¡ç®—ç”¨äºåŠ¨æ€é˜ˆå€¼çš„â€œéœ‡è¡å¸‚â€æ ‡è®°ï¼šä»·æ ¼åœ¨åŒºé—´ä¸­éƒ¨ä¸” ATR_norm è¾ƒä½
    if USE_DYNAMIC_THRESHOLDS:
        atr_series = features["ATR_norm"]
        atr_roll_med = (
            atr_series.rolling(RANGE_ATR_ROLLING_WINDOW, min_periods=50).median()
        )
        # ä½¿ç”¨ bfill/ffill é¿å… fillna(method=...) çš„ FutureWarning
        atr_roll_med = atr_roll_med.bfill().ffill()
        in_mid_range = features["price_pos_96"].between(
            RANGE_PRICE_POS_LOW, RANGE_PRICE_POS_HIGH
        )
        low_atr = atr_series < atr_roll_med
        is_range_regime = (in_mid_range & low_atr).astype(bool)
    else:
        is_range_regime = pd.Series(False, index=features.index)

    scaled_features = scaler.transform(features)
    actual_pnl = test_df["Close"].pct_change(HORIZON).shift(-HORIZON)
    signals = []
    cooldown_counter = 0
    with torch.no_grad():
        for i in range(len(scaled_features) - LOOK_BACK):
            if cooldown_counter > 0:
                signals.append(0)
                cooldown_counter -= 1
                continue

            idx = LOOK_BACK + i  # å¯¹åº”å½“å‰å†³ç­–çš„æ—¶é—´ç´¢å¼•
            in_range = bool(is_range_regime.iloc[idx]) if USE_DYNAMIC_THRESHOLDS else False
            # ç°åœ¨åªæ ¹æ®â€œå¥½äº¤æ˜“æ¦‚ç‡â€ pred_edge åšå†³ç­–ï¼›quality_threshold ä¿ç•™å‚æ•°ä½†ä¸å†ä½¿ç”¨
            e_th = (
                edge_threshold * RANGE_EDGE_MULTIPLIER
                if in_range
                else edge_threshold
            )
            seq = (
                torch.tensor(scaled_features[i : i + LOOK_BACK], dtype=torch.float32)
                .unsqueeze(0)
                .to(DEVICE)
            )
            pred_quality, pred_edge = model(seq)
            if pred_edge.item() > e_th:
                signals.append(1)
                cooldown_counter = cooldown_period
            else:
                signals.append(0)

    results = pd.DataFrame({"signal": signals}, index=features.index[LOOK_BACK:])
    results = results.join(actual_pnl.rename("pnl"))
    trades = results[results["signal"] == 1].dropna(subset=["pnl"])
    num_trades = len(trades)
    gross_pnl = trades["pnl"].sum()
    total_costs = num_trades * cost_per_trade
    net_pnl = gross_pnl - total_costs
    return {
        "pnl": net_pnl,
        "win_rate": (trades["pnl"] > 0).mean() if num_trades > 0 else 0,
        "num_trades": num_trades,
    }


def train_global_model_v16(full_df):
    """
    è®­ç»ƒä¸€ä¸ªâ€œå…¨å±€å•ä¸€æ¨¡å‹â€ï¼Œæ–¹ä¾¿å®ç›˜å’Œå¿«é€Ÿå›æµ‹ã€‚
    ä½¿ç”¨ GLOBAL_TRAIN_END_DATE ä¹‹å‰çš„æ•°æ®è®­ç»ƒï¼Œå¹¶ä¿å­˜åˆ° MODELS_DIR/global_model.*ã€‚
    """
    logger.info("--- è®­ç»ƒå…¨å±€å•ä¸€æ¨¡å‹ (V16 Causal, 1h) ---")
    features_df = get_features_v16(full_df)
    quality_s, edge_s = create_targets_v16(full_df, HORIZON)

    if features_df.empty:
        logger.error("ç‰¹å¾ä¸ºç©ºï¼Œæ— æ³•è®­ç»ƒå…¨å±€æ¨¡å‹ã€‚")
        return

    # ä»…ä½¿ç”¨ GLOBAL_TRAIN_END_DATE ä¹‹å‰çš„æ•°æ®è®­ç»ƒ+éªŒè¯
    idx = features_df.index
    cutoff = pd.to_datetime(GLOBAL_TRAIN_END_DATE)
    total_mask = idx < cutoff
    features_total = features_df[total_mask]
    quality_total = quality_s[total_mask]
    edge_total = edge_s[total_mask]

    if len(features_total) <= LOOK_BACK * 2:
        logger.error("å…¨å±€è®­ç»ƒæ•°æ®ä¸è¶³ï¼Œè¯·æ£€æŸ¥ GLOBAL_TRAIN_END_DATE å’Œçª—å£è®¾ç½®ã€‚")
        return

    # æŒ‰æ—¶é—´é¡ºåºåˆ‡åˆ† 80% ä½œä¸ºè®­ç»ƒï¼Œ20% ä½œä¸ºéªŒè¯ï¼ˆä»…ç”¨äºç›‘æ§ï¼Œä¸åšæ—©åœï¼‰
    n_total = len(features_total)
    split_idx = int(n_total * 0.8)
    ft_features = features_total.iloc[:split_idx].values
    ft_quality = quality_total.iloc[:split_idx].values
    ft_edge = edge_total.iloc[:split_idx].values

    val_features = features_total.iloc[split_idx:].values
    val_quality = quality_total.iloc[split_idx:].values
    val_edge = edge_total.iloc[split_idx:].values

    if len(ft_features) <= LOOK_BACK or len(val_features) <= LOOK_BACK:
        logger.error("å…¨å±€è®­ç»ƒ/éªŒè¯æ•°æ®ä¸è¶³ï¼Œè¯·æ£€æŸ¥åˆ‡åˆ†æ¯”ä¾‹ä¸çª—å£è®¾ç½®ã€‚")
        return

    scaler = RobustScaler().fit(ft_features)
    ft_features_s = scaler.transform(ft_features)
    val_features_s = scaler.transform(val_features)

    train_dataset = CausalDataset(ft_features_s, ft_quality, ft_edge, LOOK_BACK)
    val_dataset = CausalDataset(val_features_s, val_quality, val_edge, LOOK_BACK)
    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)
    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE)

    input_dim = ft_features_s.shape[1]
    model = CausalTransformer(input_dim, **MODEL_CONFIG).to(DEVICE)
    optimizer = torch.optim.Adam(
        model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY
    )
    criterion = CombinedLossV16()
    # åˆå§‹åŒ–æ—©åœç›¸å…³å˜é‡
    best_val_loss = float("inf")
    epochs_no_improve = 0

    for epoch in range(FINETUNE_MAX_EPOCHS):
        model.train()
        total_train_loss = 0.0
        num_train_batches = 0
        pbar = tqdm(
            train_loader, desc=f"Epoch {epoch+1}/{FINETUNE_MAX_EPOCHS} [Train]", leave=False
        )
        for f, t_q, t_e in pbar:
            f, t_q, t_e = f.to(DEVICE), t_q.to(DEVICE), t_e.to(DEVICE)
            optimizer.zero_grad()
            p_q, p_e = model(f)
            loss = criterion(p_q.squeeze(-1), p_e.squeeze(-1), t_q, t_e)
            if torch.isnan(loss):
                continue
            loss.backward()
            optimizer.step()
            total_train_loss += loss.item()
            num_train_batches += 1
            pbar.set_postfix(loss=loss.item())

        avg_train_loss = (
            total_train_loss / num_train_batches if num_train_batches > 0 else 0.0
        )

        # ç®€å•ç›‘æ§ä¸€ä¸‹éªŒè¯é›†ï¼Œä¸åšæ—©åœå’Œè°ƒåº¦
        model.eval()
        total_val_loss = 0.0
        num_val_batches = 0
        with torch.no_grad():
            for f, t_q, t_e in val_loader:
                f, t_q, t_e = f.to(DEVICE), t_q.to(DEVICE), t_e.to(DEVICE)
                p_q, p_e = model(f)
                loss = criterion(p_q.squeeze(-1), p_e.squeeze(-1), t_q, t_e)
                if torch.isnan(loss):
                    continue
                total_val_loss += loss.item()
                num_val_batches += 1

        avg_val_loss = (
            total_val_loss / num_val_batches if num_val_batches > 0 else float("nan")
        )
        logger.info(
            f"   Epoch {epoch+1}/{FINETUNE_MAX_EPOCHS}, "
            f"Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}"
        )

        # ç®€å•æ—©åœï¼šè‡³å°‘è®­ç»ƒ MIN_EPOCHS_FOR_EARLY_STOP ä¸ª epochï¼Œ
        # ä¹‹åè‹¥éªŒè¯æŸå¤±åœ¨ EARLY_STOP_PATIENCE ä¸ª epoch å†…æœªæå‡åˆ™åœæ­¢
        if epoch + 1 >= MIN_EPOCHS_FOR_EARLY_STOP and not torch.isnan(
            torch.tensor(avg_val_loss)
        ):
            if avg_val_loss + EARLY_STOP_MIN_DELTA < best_val_loss:
                best_val_loss = avg_val_loss
                epochs_no_improve = 0
            else:
                epochs_no_improve += 1
                if epochs_no_improve >= EARLY_STOP_PATIENCE:
                    logger.info(
                        f"   â¹ Early stopping at epoch {epoch+1} "
                        f"(no val improvement for {EARLY_STOP_PATIENCE} epochs)"
                    )
                    break

    global_model_path = os.path.join(MODELS_DIR, "global_model.pt")
    global_scaler_path = os.path.join(MODELS_DIR, "global_scaler.joblib")
    try:
        torch.save(model.state_dict(), global_model_path)
        joblib.dump(scaler, global_scaler_path)
        logger.info(
            f"ğŸ’¾ Saved global model to {global_model_path} and scaler to {global_scaler_path}"
        )
    except Exception as e:
        logger.warning(f"âš  ä¿å­˜å…¨å±€æ¨¡å‹æˆ– scaler å¤±è´¥: {e}")


def backtest_global_model_v16(full_df):
    """
    ä½¿ç”¨å·²è®­ç»ƒå¥½çš„å…¨å±€å•ä¸€æ¨¡å‹ï¼Œå¯¹ GLOBAL_TRAIN_END_DATE ä¹‹åçš„æ ·æœ¬å¤–åŒºé—´è¿›è¡Œå›æµ‹ã€‚
    """
    logger.info("--- ä½¿ç”¨å…¨å±€å•ä¸€æ¨¡å‹è¿›è¡Œå›æµ‹ (V16 Causal, 1h) ---")
    global_model_path = os.path.join(MODELS_DIR, "global_model.pt")
    global_scaler_path = os.path.join(MODELS_DIR, "global_scaler.joblib")

    if not (os.path.exists(global_model_path) and os.path.exists(global_scaler_path)):
        logger.error(
            f"æœªæ‰¾åˆ°å…¨å±€æ¨¡å‹æˆ– scaler: {global_model_path}, {global_scaler_path}ã€‚"
        )
        return

    # ä¸ºäº†ç¡®å®šç‰¹å¾ç»´åº¦ï¼Œé‡æ–°è®¡ç®—ä¸€æ¬¡ç‰¹å¾
    features_df = get_features_v16(full_df)
    if features_df.empty:
        logger.error("ç‰¹å¾ä¸ºç©ºï¼Œæ— æ³•å›æµ‹å…¨å±€æ¨¡å‹ã€‚")
        return
    input_dim = features_df.shape[1]

    model = CausalTransformer(input_dim, **MODEL_CONFIG).to(DEVICE)
    try:
        state_dict = torch.load(global_model_path, map_location=DEVICE)
        model.load_state_dict(state_dict)
    except Exception as e:
        logger.error(f"åŠ è½½å…¨å±€æ¨¡å‹å¤±è´¥: {e}")
        return

    try:
        scaler = joblib.load(global_scaler_path)
    except Exception as e:
        logger.error(f"åŠ è½½å…¨å±€ scaler å¤±è´¥: {e}")
        return

    cutoff = pd.to_datetime(GLOBAL_TRAIN_END_DATE)
    test_df = full_df[cutoff:]
    if test_df.empty:
        logger.error("æ ·æœ¬å¤–æµ‹è¯•æ•°æ®ä¸ºç©ºï¼Œæ— æ³•å›æµ‹ã€‚")
        return

    results = backtest_period_v16(
        model,
        scaler,
        test_df,
        quality_threshold=BACKTEST_QUALITY_THRESHOLD,
        edge_threshold=BACKTEST_EDGE_THRESHOLD,
        cost_per_trade=COST_PER_TRADE,
        cooldown_period=COOLDOWN_PERIOD,
    )
    logger.info("\n--- å…¨å±€æ¨¡å‹æ ·æœ¬å¤–å›æµ‹ç»“æœ (å·²æ‰£é™¤äº¤æ˜“æˆæœ¬) ---\n")
    print(results)

if __name__ == "__main__":
    if os.path.exists(DATA_CACHE_PATH):
        logger.info(f"ä»ç¼“å­˜åŠ è½½æ•°æ®: {DATA_CACHE_PATH}")
        raw_df = pd.read_csv(DATA_CACHE_PATH, index_col=0, parse_dates=True)
    else:
        logger.info("ç¼“å­˜æœªæ‰¾åˆ°ã€‚ä¸‹è½½æ–°æ•°æ®...")
        raw_df = fetch_binance_klines(
            s=SYMBOL, i=INTERVAL, st=DATA_START_DATE, en=DATA_END_DATE
        )
        if not raw_df.empty:
            logger.info(f"ä¿å­˜ä¸‹è½½çš„æ•°æ®åˆ°ç¼“å­˜: {DATA_CACHE_PATH}")
            raw_df.to_csv(DATA_CACHE_PATH)
        else:
            logger.error("æ•°æ®ä¸‹è½½å¤±è´¥ã€‚ç¨‹åºé€€å‡ºã€‚")
            exit()
    logger.info(f"ä½¿ç”¨ {DEVICE} è®¾å¤‡ã€‚")

    if RUN_MODE == "train_global":
        train_global_model_v16(raw_df)
    elif RUN_MODE == "backtest_global":
        backtest_global_model_v16(raw_df)
    else:
        raise ValueError(f"æœªçŸ¥ RUN_MODE: {RUN_MODE}")
