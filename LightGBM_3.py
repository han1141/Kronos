import numpy as np
import pandas as pd
import requests
import time
import logging
import pandas_ta as ta
import os
import joblib
import lightgbm as lgb
from sklearn.preprocessing import MinMaxScaler
from sklearn.metrics import (
    precision_score,
    recall_score,
    precision_recall_curve,
    confusion_matrix,
)
from tqdm import tqdm
from scipy.signal import find_peaks

# <<< æ–°å¢å¯¼å…¥ >>>
import numba

# --- 0. è®¾ç½® ---
logging.basicConfig(
    level=logging.INFO, format="%(asctime)s - %(levelname)s - %(message)s"
)
logger = logging.getLogger(__name__)

# --- ğŸš€ å…¨å±€é…ç½® ---
SYMBOL = "ETHUSDT"
INTERVAL = "15m"
DATA_START_DATE = "2017-01-01"
TRAIN_START = "2018-01-01"
VALIDATION_START = "2024-01-01"
TEST_START = "2025-01-01"
TEST_END = "2025-11-09"
LOOK_BACK = 60
TREND_CONFIG = {
    "look_forward_steps": 3,
    "target_return": 0.004,
    "max_drawdown_limit": 0.01,
}
logger.info(
    f"è®­ç»ƒç›®æ ‡ç›ˆåˆ©ï¼š{TREND_CONFIG['target_return']*100}%ï¼Œæœ€å¤§å›æ’¤é™åˆ¶ï¼š{TREND_CONFIG['max_drawdown_limit']*100}%"
)

# --- æ–‡ä»¶è·¯å¾„ ---
MODELS_DIR, DATA_DIR = "models_gbm2", "data"
MODEL_SAVE_PATH = os.path.join(
    MODELS_DIR, f"eth_model_high_precision_v4_{INTERVAL}.joblib"
)
SCALER_SAVE_PATH = os.path.join(
    MODELS_DIR, f"eth_scaler_high_precision_v4_{INTERVAL}.joblib"
)
FEATURE_COLUMNS_PATH = os.path.join(
    MODELS_DIR, f"feature_columns_high_precision_v4_{INTERVAL}.joblib"
)
FLATTENED_COLUMNS_PATH = os.path.join(
    MODELS_DIR, f"flattened_columns_high_precision_v4_{INTERVAL}.joblib"
)
DATA_CACHE_PATH = os.path.join(DATA_DIR, f"{SYMBOL.lower()}_{INTERVAL}_data.csv")

os.makedirs(MODELS_DIR, exist_ok=True)
os.makedirs(DATA_DIR, exist_ok=True)


# --- æ•°æ®è·å–ä¸è¾…åŠ©å‡½æ•° ---
def fetch_binance_klines(s, i, st, en=None, l=1000):
    # ... (æ­¤å‡½æ•°ä¿æŒä¸å˜) ...
    url, cols = "https://api.binance.com/api/v3/klines", [
        "timestamp",
        "Open",
        "High",
        "Low",
        "Close",
        "Volume",
        "close_time",
        "quote_asset_volume",
        "number_of_trades",
        "taker_buy_base_volume",
        "taker_buy_quote_volume",
        "ignore",
    ]
    sts, ets = int(pd.to_datetime(st).timestamp() * 1000), (
        int(pd.to_datetime(en).timestamp() * 1000) if en else int(time.time() * 1000)
    )
    all_d, retries, max_retries = [], 0, 5
    while sts < ets:
        try:
            r = requests.get(
                url,
                params={
                    "symbol": s.upper(),
                    "interval": i,
                    "startTime": sts,
                    "endTime": ets,
                    "limit": l,
                },
                timeout=15,
            )
            r.raise_for_status()
            d = r.json()
            if not d:
                break
            all_d.extend(d)
            sts = d[-1][0] + 1
            retries = 0
        except requests.exceptions.RequestException as e:
            retries += 1
            if retries > max_retries:
                logger.error(f"è·å–æ•°æ®å¤±è´¥è¶…è¿‡æœ€å¤§é‡è¯•æ¬¡æ•°: {e}")
                return pd.DataFrame()
            logger.warning(
                f"è·å–æ•°æ®å¤±è´¥ï¼Œæ­£åœ¨é‡è¯• ({retries}/{max_retries})... Error: {e}"
            )
            time.sleep(retries * 2)
    if not all_d:
        return pd.DataFrame()
    df = pd.DataFrame(all_d, columns=cols)[
        ["timestamp", "Open", "High", "Low", "Close", "Volume"]
    ].copy()
    df["timestamp"] = pd.to_datetime(df["timestamp"], unit="ms")
    for col in df.columns[1:]:
        df[col] = pd.to_numeric(df[col], errors="coerce")
    logger.info(f"âœ… è·å– {s} æ•°æ®æˆåŠŸ: {len(df)} æ¡")
    return df.set_index("timestamp").sort_index()


# <<< å·²ä¿®æ”¹ï¼šHurstå‡½æ•°ä½¿ç”¨Numba JITè¿›è¡ŒåŠ é€Ÿï¼Œå¹¶æ‰‹åŠ¨å®ç°çº¿æ€§å›å½’ >>>
@numba.jit(nopython=True, cache=True)
def compute_hurst_numba(ts):
    if len(ts) < 100:
        return 0.5

    max_lag = 100
    lags = np.arange(2, max_lag)

    tau = np.empty(len(lags), dtype=np.float64)
    for i, lag in enumerate(lags):
        # Numba-friendly standard deviation calculation
        diff = ts[lag:] - ts[:-lag]
        if len(diff) > 0:
            # Manually calculate std dev: sqrt(E[X^2] - E[X]^2)
            tau[i] = np.sqrt(np.mean(diff**2) - (np.mean(diff)) ** 2)
        else:
            tau[i] = 0.0  # Should not happen if len(ts) is sufficient

    # Filter out zero values to avoid log(0) issues and ensure we have enough points
    valid_tau = tau[tau > 0]
    valid_lags = lags[tau > 0]
    if len(valid_tau) < 2:
        return 0.5

    # --- <<< æ ¸å¿ƒä¿®æ”¹ï¼šæ‰‹åŠ¨å®ç° np.polyfit(deg=1) >>> ---
    # Convert to log scale
    log_lags = np.log(valid_lags)
    log_tau = np.log(valid_tau)

    # Calculate the slope (m) of the best-fit line y = mx + c
    # using the formula: m = ( (mean(x*y) - mean(x)*mean(y)) /
    #                          (mean(x^2) - mean(x)^2) )
    mean_log_lags = np.mean(log_lags)
    mean_log_tau = np.mean(log_tau)

    numerator = np.mean(log_lags * log_tau) - (mean_log_lags * mean_log_tau)
    denominator = np.mean(log_lags**2) - (mean_log_lags**2)

    if denominator == 0:
        return 0.5

    hurst_exponent = numerator / denominator

    return hurst_exponent * 2.0


def get_market_structure_features(df, order=5):
    # ... (æ­¤å‡½æ•°ä¿æŒä¸å˜) ...
    df = df.copy()
    high_peaks_idx, _ = find_peaks(
        df["High"], distance=order, prominence=df["High"].std() * 0.5
    )
    low_peaks_idx, _ = find_peaks(
        -df["Low"], distance=order, prominence=df["Low"].std() * 0.5
    )
    df["swing_high_price"] = np.nan
    df.iloc[high_peaks_idx, df.columns.get_loc("swing_high_price")] = df.iloc[
        high_peaks_idx
    ]["High"]
    df["swing_low_price"] = np.nan
    df.iloc[low_peaks_idx, df.columns.get_loc("swing_low_price")] = df.iloc[
        low_peaks_idx
    ]["Low"]
    df["swing_high_price"] = df["swing_high_price"].ffill()
    df["swing_low_price"] = df["swing_low_price"].ffill()
    df["is_uptrend"] = (
        (df["swing_high_price"] > df["swing_high_price"].shift(1))
        & (df["swing_low_price"] > df["swing_low_price"].shift(1))
    ).astype(int)
    df["is_downtrend"] = (
        (df["swing_high_price"] < df["swing_high_price"].shift(1))
        & (df["swing_low_price"] < df["swing_low_price"].shift(1))
    ).astype(int)
    df["market_structure"] = df["is_uptrend"] - df["is_downtrend"]
    return df[["market_structure"]]


def feature_engineering(df):
    """æ•´åˆæ‰€æœ‰ç‰¹å¾è®¡ç®— - V4.2 å¢åŠ ç‰¹å¾äº¤å‰"""
    df_copy = df.copy()
    logger.info("--- å¼€å§‹è®¡ç®—ç‰¹å¾ (V4.2 å¢å¼ºç‰ˆ - å«ç‰¹å¾äº¤å‰) ---")

    # 1. åŸºç¡€æŒ‡æ ‡ (ä¿æŒä¸å˜)
    df_copy.ta.rsi(length=14, append=True)
    df_copy.ta.macd(fast=12, slow=26, signal=9, append=True)
    df_copy.ta.bbands(length=20, std=2, append=True)
    df_copy.ta.adx(length=14, append=True)
    df_copy.ta.atr(length=14, append=True, col_names=("ATR_14"))
    df_copy.ta.obv(append=True)

    # 2. å¸‚åœºç»“æ„ä¸é•¿å‘¨æœŸè¶‹åŠ¿ (ä¿æŒä¸å˜)
    market_structure_df = get_market_structure_features(df_copy)
    df_copy.ta.macd(
        fast=24,
        slow=52,
        signal=18,
        append=True,
        col_names=("MACD_long", "MACDh_long", "MACDs_long"),
    )

    # 3. å¸‚åœºçŠ¶æ€ä¸æ³¢åŠ¨æ€§ (ä¿æŒä¸å˜)
    logger.info("æ­£åœ¨è®¡ç®—HurstæŒ‡æ•° (æ­¤è¿‡ç¨‹å¯èƒ½éœ€è¦å‡ åˆ†é’Ÿ)...")
    df_copy["hurst"] = (
        df_copy["Close"].rolling(window=100).apply(compute_hurst_numba, raw=True)
    )
    logger.info("HurstæŒ‡æ•°è®¡ç®—å®Œæˆã€‚")
    df_copy["volatility_log"] = (
        (np.log(df_copy["Close"] / df_copy["Close"].shift(1))).rolling(window=20).std()
    )

    # 4. "è§„åˆ™" è½¬åŒ–ä¸º "ç‰¹å¾" (ä¿æŒä¸å˜)
    df_copy["macd_cross_signal"] = (
        df_copy["MACD_12_26_9"] > df_copy["MACDs_12_26_9"]
    ).astype(int)
    df_copy["macd_long_cross_signal"] = (
        df_copy["MACD_long"] > df_copy["MACDs_long"]
    ).astype(int)

    # 5. å¤šæ—¶é—´æ¡†æ¶ç‰¹å¾ (ä¿æŒä¸å˜)
    close_4h = df_copy["Close"].resample("4h").last()
    ema_4h = ta.ema(close_4h, length=50)
    df_copy["ema_4h"] = ema_4h.reindex(df_copy.index, method="ffill")
    df_copy["price_above_ema_4h"] = (df_copy["Close"] > df_copy["ema_4h"]).astype(int)

    # 6. --- <<< æ–°å¢ï¼šç‰¹å¾äº¤å‰ (Feature Crossing) >>> ---
    logger.info("æ­£åœ¨åˆ›å»ºäº¤å‰ç‰¹å¾...")
    # ç¤ºä¾‹1: æ³¢åŠ¨ç‡ä¸è¶‹åŠ¿å¼ºåº¦çš„äº¤äº’ (é«˜ADXå’Œé«˜ATRå¯èƒ½æ„å‘³ç€å¼ºåŠ›çªç ´)
    df_copy["adx_x_atr_norm"] = (df_copy["ADX_14"] / 50) * (
        df_copy["ATR_14"] / df_copy["Close"]
    )
    # ç¤ºä¾‹2: RSIä¸å¸‚åœºçŠ¶æ€çš„äº¤äº’ (è¶‹åŠ¿å¸‚ä¸­çš„RSI vs éœ‡è¡å¸‚çš„RSI)
    df_copy["rsi_x_hurst"] = df_copy["RSI_14"] * df_copy["hurst"]
    # ç¤ºä¾‹3: çŸ­æœŸè¶‹åŠ¿ä¸é•¿å‘¨æœŸè¶‹åŠ¿çš„ç¡®è®¤ (ä¸¤ä¸ªMACDéƒ½çœ‹æ¶¨)
    df_copy["macd_cross_confirm"] = (
        df_copy["macd_cross_signal"] * df_copy["macd_long_cross_signal"]
    )
    logger.info("äº¤å‰ç‰¹å¾åˆ›å»ºå®Œæˆã€‚")

    # 7. æ•´åˆ
    df_copy = pd.concat([df_copy, market_structure_df], axis=1)

    # 8. é€‰æ‹©ç‰¹å¾åˆ—å¹¶è¿›è¡Œæ¸…ç†
    feature_columns = [
        col
        for col in df_copy.columns
        if col
        not in [
            "Open",
            "High",
            "Low",
            "Close",
            "Volume",
            "swing_high_price",
            "swing_low_price",
            "ema_4h",
        ]
    ]

    all_features_df = (
        df_copy[feature_columns].replace([np.inf, -np.inf], np.nan).ffill()
    )

    return all_features_df


# --- (å…¶ä»–å‡½æ•°å¦‚ create_trend_labels, train_and_validate ç­‰éƒ½ä½¿ç”¨æˆ‘ä»¬ä¸Šä¸€è½®è®¨è®ºçš„æœ€æ–°ç‰ˆæœ¬) ---
# ...
# The rest of the script (create_trend_labels, train_and_validate, run_backtest_and_evaluate, __main__)
# remains IDENTICAL to the one provided in the previous response ("ç»™å‡ºä¿®æ”¹åçš„å®Œæ•´ç‰ˆä»£ç ").
# You only need to replace the `compute_hurst` and `feature_engineering` functions
# and add `import numba` at the top.
#
# For completeness, I'll paste the rest of the script again.
#
def create_trend_labels(df, look_forward_steps, target_return, max_drawdown_limit):
    df_copy = df.copy()
    df_copy["target_price"] = df_copy["Close"] * (1 + target_return)
    future_highs = (
        df_copy["High"]
        .rolling(window=look_forward_steps)
        .max()
        .shift(-look_forward_steps)
    )
    future_lows = (
        df_copy["Low"]
        .rolling(window=look_forward_steps)
        .min()
        .shift(-look_forward_steps)
    )
    drawdown_before_profit = (df_copy["Close"] - future_lows) / df_copy["Close"]
    profit_reached = future_highs >= df_copy["target_price"]
    risk_controlled = drawdown_before_profit < max_drawdown_limit
    df_copy["label"] = (profit_reached & risk_controlled).astype(int)
    return df_copy


def create_flattened_sequences(data, labels, look_back=60):
    X, y = [], []
    for i in range(len(data) - look_back):
        X.append(data[i : (i + look_back), :].flatten())
        y.append(labels[i + look_back])
    return np.array(X, dtype=np.float32), np.array(y, dtype=np.int32)


def train_and_validate(train_df, validation_df, look_back, trend_config):
    logger.info("--- å¼€å§‹è®­ç»ƒå’ŒéªŒè¯æµç¨‹ (V4) ---")
    X_train_full_df = feature_engineering(train_df).dropna()
    X_validation_full_df = feature_engineering(validation_df).dropna()
    train_labeled = create_trend_labels(train_df, **trend_config)
    validation_labeled = create_trend_labels(validation_df, **trend_config)
    y_train_full = train_labeled["label"].align(X_train_full_df, join="inner", axis=0)[
        0
    ]
    X_train_full_df = X_train_full_df.align(
        train_labeled["label"], join="inner", axis=0
    )[0]
    y_validation_full = validation_labeled["label"].align(
        X_validation_full_df, join="inner", axis=0
    )[0]
    X_validation_full_df = X_validation_full_df.align(
        validation_labeled["label"], join="inner", axis=0
    )[0]
    X_validation_full_df = X_validation_full_df[X_train_full_df.columns]
    scaler = MinMaxScaler(feature_range=(0, 1))
    X_train_scaled = scaler.fit_transform(X_train_full_df)
    X_validation_scaled = scaler.transform(X_validation_full_df)
    original_columns = X_train_full_df.columns
    flattened_columns = [
        f"{col}_lag_{lag}"
        for lag in range(look_back - 1, -1, -1)
        for col in original_columns
    ]
    joblib.dump(original_columns, FEATURE_COLUMNS_PATH)
    joblib.dump(flattened_columns, FLATTENED_COLUMNS_PATH)
    X_train_np, y_train = create_flattened_sequences(
        X_train_scaled, y_train_full.values, look_back
    )
    X_validation_np, y_validation = create_flattened_sequences(
        X_validation_scaled, y_validation_full.values, look_back
    )
    X_train_df = pd.DataFrame(X_train_np, columns=flattened_columns)
    X_validation_df = pd.DataFrame(X_validation_np, columns=flattened_columns)
    logger.info(f"è®­ç»ƒæ ·æœ¬: {len(X_train_df)}, éªŒè¯æ ·æœ¬: {len(X_validation_df)}")
    train_label_counts = np.bincount(y_train)
    if train_label_counts.size < 2 or train_label_counts[1] == 0:
        logger.error("è®­ç»ƒæ•°æ®ä¸­æ²¡æœ‰æ­£æ ·æœ¬(label=1)ï¼Œæ— æ³•ç»§ç»­ã€‚")
        return None, None, None
    precision_focus_ratio = 0.3
    scale_pos_weight = (
        train_label_counts[0] / train_label_counts[1]
    ) * precision_focus_ratio
    logger.info(f"è°ƒæ•´åçš„ scale_pos_weight (è¿½æ±‚é«˜èƒœç‡): {scale_pos_weight:.2f}")
    lgb_params = {
        "objective": "binary",
        "metric": "logloss",
        "n_estimators": 2000,
        "learning_rate": 0.02,
        "num_leaves": 20,
        "max_depth": 5,
        "seed": 42,
        "n_jobs": -1,
        "verbose": -1,
        "scale_pos_weight": scale_pos_weight,
        "colsample_bytree": 0.7,
        "subsample": 0.7,
        "reg_alpha": 0.1,
    }
    lgb_model = lgb.LGBMClassifier(**lgb_params)
    logger.info("\nå¼€å§‹è®­ç»ƒ LightGBM æ¨¡å‹ (V4)...")
    lgb_model.fit(
        X_train_df,
        y_train,
        eval_set=[(X_validation_df, y_validation)],
        eval_metric="logloss",
        callbacks=[lgb.early_stopping(stopping_rounds=100, verbose=False)],
    )
    y_val_pred_probs = lgb_model.predict_proba(X_validation_df)[:, 1]
    MIN_PRECISION_TARGET = 0.55
    precisions, recalls, thresholds = precision_recall_curve(
        y_validation, y_val_pred_probs
    )
    valid_threshold_indices = np.where(precisions[:-1] >= MIN_PRECISION_TARGET)[0]
    best_threshold = 0.5
    if len(valid_threshold_indices) > 0:
        f1_scores = np.divide(
            2 * recalls * precisions,
            recalls + precisions,
            out=np.zeros_like(recalls),
            where=(recalls + precisions) != 0,
        )
        best_idx_within_valid = np.argmax(f1_scores[valid_threshold_indices])
        final_best_idx = valid_threshold_indices[best_idx_within_valid]
        best_threshold = thresholds[final_best_idx]
        logger.info(
            f"åœ¨æ»¡è¶³èƒœç‡>{MIN_PRECISION_TARGET*100}%çš„æ¡ä»¶ä¸‹ï¼Œæ‰¾åˆ°æœ€ä½³é˜ˆå€¼: {best_threshold:.4f}"
        )
        logger.info(
            f"è¯¥é˜ˆå€¼ä¸‹çš„éªŒè¯é›†è¡¨ç°: Precision={precisions[final_best_idx]:.4f}, Recall={recalls[final_best_idx]:.4f}"
        )
    else:
        logger.warning(
            f"æœªèƒ½æ‰¾åˆ°ä»»ä½•é˜ˆå€¼å¯ä»¥ä½¿éªŒè¯é›†èƒœç‡è¾¾åˆ° {MIN_PRECISION_TARGET*100}%ã€‚å°†ä½¿ç”¨æœ€å¤§åŒ–F1çš„é˜ˆå€¼ã€‚"
        )
        f1_scores = np.divide(
            2 * recalls * precisions,
            recalls + precisions,
            out=np.zeros_like(recalls),
            where=(recalls + precisions) != 0,
        )
        best_f1_idx = np.argmax(f1_scores)
        best_threshold = (
            thresholds[best_f1_idx] if len(thresholds) > best_f1_idx else 0.5
        )
        logger.info(f"åœ¨éªŒè¯é›†ä¸Šæ‰¾åˆ°çš„æœ€ä½³F1é˜ˆå€¼: {best_threshold:.4f}")
    joblib.dump(lgb_model, MODEL_SAVE_PATH)
    joblib.dump(scaler, SCALER_SAVE_PATH)
    logger.info(f"æ¨¡å‹å·²ä¿å­˜åˆ°: {MODEL_SAVE_PATH}")
    return lgb_model, scaler, best_threshold


# --- ğŸš€ 4. å‡çº§ç‰ˆå›æµ‹è¯„ä¼°å‡½æ•° (V4.3 - å¢åŠ å›æ’¤åˆ†å¸ƒåˆ†æ) ---
def run_backtest_and_evaluate(
    test_df, model, scaler, look_back, threshold, trend_config
):
    logger.info(
        "\n" + "=" * 60 + "\n--- å¼€å§‹åœ¨æµ‹è¯•é›†ä¸Šè¿›è¡Œä¸¥æ ¼çš„å›æµ‹è¯„ä¼° (V4) ---\n" + "=" * 60
    )
    original_columns = joblib.load(FEATURE_COLUMNS_PATH)
    flattened_columns = joblib.load(FLATTENED_COLUMNS_PATH)
    test_features_df = feature_engineering(test_df).dropna()
    test_features_aligned = test_features_df.reindex(
        columns=original_columns, fill_value=0
    )
    test_scaled = scaler.transform(test_features_aligned)
    final_signals = []
    logger.info("é€æ ¹Kçº¿éå†æµ‹è¯•é›†è¿›è¡Œé¢„æµ‹...")
    for i in tqdm(range(look_back, len(test_scaled))):
        input_sequence = test_scaled[i - look_back : i, :]
        input_flattened_np = input_sequence.flatten().reshape(1, -1)
        input_df = pd.DataFrame(input_flattened_np, columns=flattened_columns)
        pred_prob = model.predict_proba(input_df)[0][1]
        model_signal = 1 if pred_prob > threshold else 0
        final_signals.append(model_signal)
    actual_labels_df = create_trend_labels(test_df, **trend_config).dropna()
    pred_index = test_features_df.index[look_back : look_back + len(final_signals)]
    pred_series = pd.Series(final_signals, index=pred_index)
    results_df = pd.DataFrame(actual_labels_df["label"]).join(
        pred_series.to_frame("final_signal"), how="inner"
    )
    if results_df.empty or np.sum(results_df["final_signal"]) == 0:
        logger.warning("å›æµ‹æœŸé—´æ²¡æœ‰äº§ç”Ÿä»»ä½•äº¤æ˜“ä¿¡å·ï¼Œæ— æ³•è®¡ç®—èƒœç‡ã€‚")
        return
    y_test_actual = results_df["label"].values
    y_pred_final = results_df["final_signal"].values

    winning_trades_drawdown = []
    winning_signals_df = results_df[
        (results_df["final_signal"] == 1) & (results_df["label"] == 1)
    ]
    if not winning_signals_df.empty:
        logger.info("æ­£åœ¨è®¡ç®—ç›ˆåˆ©ä¿¡å·åœ¨ç›ˆåˆ©å‰çš„æœ€å¤§å›æ’¤...")
        look_forward = trend_config["look_forward_steps"]
        test_df_copy = test_df.copy()
        test_df_copy["future_min_low"] = (
            test_df_copy["Low"].rolling(window=look_forward).min().shift(-look_forward)
        )
        winning_trades_details = winning_signals_df.join(
            test_df_copy[["Close", "future_min_low"]], how="inner"
        )
        winning_trades_details["drawdown_pct"] = (
            (winning_trades_details["Close"] - winning_trades_details["future_min_low"])
            / winning_trades_details["Close"]
        ) * 100
        winning_trades_drawdown = winning_trades_details["drawdown_pct"].tolist()

    print("\n--- [å®¢è§‚] æµ‹è¯•é›†å›æµ‹è¯„ä¼°ç»“æœ (ä»…MLæ¨¡å‹ä¿¡å·) ---")
    print(f"æ€»å›æµ‹Kçº¿æ•°: {len(y_pred_final)}")
    print(f"å‘å‡ºçœ‹æ¶¨ä¿¡å·æ€»æ¬¡æ•° (äº¤æ˜“é¢‘ç‡): {np.sum(y_pred_final)}")
    # å¢åŠ zero_division=0ä»¥é˜²æ­¢åœ¨æ²¡æœ‰ä¿¡å·æ—¶æŠ¥é”™
    print(
        f"ç²¾ç¡®ç‡ (èƒœç‡): {precision_score(y_test_actual, y_pred_final, zero_division=0):.4f}"
    )
    print(
        f"å¬å›ç‡ (ç›ˆåˆ©æœºä¼šæ•æ‰ç‡): {recall_score(y_test_actual, y_pred_final, zero_division=0):.4f}"
    )
    print("\næ··æ·†çŸ©é˜µ (TN, FP / FN, TP):")
    print(confusion_matrix(y_test_actual, y_pred_final))

    if winning_trades_drawdown:
        drawdown_array = np.array(winning_trades_drawdown)
        avg_drawdown = np.mean(drawdown_array)

        print("\n--- [æ–°æŒ‡æ ‡] ç›ˆåˆ©äº¤æ˜“åœ¨ç›ˆåˆ©å‰çš„æœ€å¤§å›æ’¤åˆ†æ ---")
        print(f"ç›ˆåˆ©çš„äº¤æ˜“æ€»æ•°: {len(drawdown_array)}")
        print(f"å¹³å‡å›æ’¤: {avg_drawdown:.4f}%")
        print(f"æœ€å¤§å›æ’¤ (æœ€å·®æƒ…å†µ): {np.max(drawdown_array):.4f}%")
        print(f"æœ€å°å›æ’¤ (æœ€å¥½æƒ…å†µ): {np.min(drawdown_array):.4f}%")

        # --- <<< æ–°å¢ï¼šå›æ’¤åˆ†å¸ƒåˆ†æ >>> ---
        # è®¡ç®—å›æ’¤å¤§äº/å°äºå¹³å‡å›æ’¤çš„äº¤æ˜“ç™¾åˆ†æ¯”
        count_above_avg = np.sum(drawdown_array > avg_drawdown)
        count_below_or_equal_avg = len(drawdown_array) - count_above_avg

        percent_above_avg = (count_above_avg / len(drawdown_array)) * 100
        percent_below_or_equal_avg = (
            count_below_or_equal_avg / len(drawdown_array)
        ) * 100

        print(f"\n  å›æ’¤åˆ†å¸ƒ (ä¸å¹³å‡å€¼ {avg_drawdown:.4f}% ç›¸æ¯”):")
        print(
            f"    - å¤§äºå¹³å‡å›æ’¤çš„äº¤æ˜“å æ¯”: {percent_above_avg:.2f}% ({count_above_avg} ç¬”)"
        )
        print(
            f"    - å°äºæˆ–ç­‰äºå¹³å‡å›æ’¤çš„äº¤æ˜“å æ¯”: {percent_below_or_equal_avg:.2f}% ({count_below_or_equal_avg} ç¬”)"
        )

        # è®¡ç®—ä¸åŒå›æ’¤æ°´å¹³ä¸‹çš„äº¤æ˜“å æ¯”
        print("\n  å›æ’¤æ°´å¹³åˆ†å¸ƒ:")
        for threshold_pct in [0.1, 0.25, 0.5, 0.75]:
            count_below_threshold = np.sum(drawdown_array <= threshold_pct)
            percent_below_threshold = (
                count_below_threshold / len(drawdown_array)
            ) * 100
            print(
                f"    - å›æ’¤ <= {threshold_pct:.2f}% çš„äº¤æ˜“å æ¯”: {percent_below_threshold:.2f}%"
            )

    else:
        print("\n--- [æ–°æŒ‡æ ‡] ç›ˆåˆ©äº¤æ˜“åœ¨ç›ˆåˆ©å‰çš„æœ€å¤§å›æ’¤åˆ†æ ---")
        print("æœ¬æ¬¡å›æµ‹æ²¡æœ‰äº§ç”Ÿä»»ä½•ç›ˆåˆ©çš„äº¤æ˜“ï¼Œæ— æ³•è®¡ç®—ç›¸å…³æŒ‡æ ‡ã€‚")


if __name__ == "__main__":
    if os.path.exists(DATA_CACHE_PATH):
        logger.info(f"ä»ç¼“å­˜åŠ è½½æ•°æ®: {DATA_CACHE_PATH}")
        raw_df = pd.read_csv(DATA_CACHE_PATH, index_col=0, parse_dates=True)
    else:
        raw_df = fetch_binance_klines(
            s=SYMBOL, i=INTERVAL, st=DATA_START_DATE, en=TEST_END
        )
        if not raw_df.empty:
            raw_df.to_csv(DATA_CACHE_PATH)
    if raw_df.empty:
        logger.error("æ•°æ®ä¸ºç©ºï¼Œç¨‹åºé€€å‡ºã€‚")
        exit()
    train_df = raw_df[(raw_df.index >= TRAIN_START) & (raw_df.index < VALIDATION_START)]
    validation_df = raw_df[
        (raw_df.index >= VALIDATION_START) & (raw_df.index < TEST_START)
    ]
    test_df = raw_df[(raw_df.index >= TEST_START) & (raw_df.index <= TEST_END)]
    logger.info(
        f"æ•°æ®é›†åˆ’åˆ†å®Œæˆ: {len(train_df)} è®­ç»ƒ, {len(validation_df)} éªŒè¯, {len(test_df)} æµ‹è¯•ã€‚"
    )
    trained_model, trained_scaler, best_threshold = train_and_validate(
        train_df, validation_df, LOOK_BACK, TREND_CONFIG
    )
    if trained_model and best_threshold is not None:
        run_backtest_and_evaluate(
            test_df,
            trained_model,
            trained_scaler,
            LOOK_BACK,
            best_threshold,
            TREND_CONFIG,
        )
    else:
        logger.error("æ¨¡å‹è®­ç»ƒå¤±è´¥ï¼Œè·³è¿‡å›æµ‹ã€‚")
