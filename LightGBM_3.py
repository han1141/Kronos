import numpy as np
# å…¼å®¹æ€§ä¿®å¤ï¼šéƒ¨åˆ† pandas_ta ç‰ˆæœ¬ä» numpy å¯¼å…¥ NaN
# åœ¨æ–°ç‰ˆæœ¬ numpy ä¸­æ²¡æœ‰å¯¼å‡º NaN ç¬¦å·ï¼Œè¿™é‡Œæ‰‹åŠ¨è®¾ç½®åˆ«åä»¥é¿å… ImportError
if not hasattr(np, "NaN"):
    np.NaN = np.nan  # noqa: N816 (ä¿æŒä¸å¤–éƒ¨åº“å…¼å®¹çš„å¤§å°å†™)
import pandas as pd
import requests
import time
import logging
import os
import joblib
import lightgbm as lgb
from sklearn.preprocessing import MinMaxScaler
from sklearn.metrics import (
    precision_score,
    recall_score,
    precision_recall_curve,
    confusion_matrix,
)
from sklearn.model_selection import TimeSeriesSplit
from sklearn.isotonic import IsotonicRegression
from tqdm import tqdm
from scipy.signal import find_peaks

# <<< æ–°å¢å¯¼å…¥ >>>
import numba

# --- 0. è®¾ç½® ---
logging.basicConfig(
    level=logging.INFO, format="%(asctime)s - %(levelname)s - %(message)s"
)
logger = logging.getLogger(__name__)

# --- ğŸš€ å…¨å±€é…ç½® ---
SYMBOL = "ETHUSDT"
INTERVAL = "30m"
DATA_START_DATE = "2017-01-01"
TRAIN_START = "2018-01-01"
VALIDATION_START = "2024-01-01"
TEST_START = "2025-01-01"
TEST_END = "2025-11-16"
LOOK_BACK = 60
# ä½¿ç”¨æ­¥è¿›æŠ½æ ·çš„æ–¹å¼é™ä½å±•å¹³ç»´åº¦ï¼Œå¦‚æ­¥é•¿ä¸º5åˆ™ä»…å–æ¯5æ ¹ä¸­çš„ä¸€æ ¹
LAG_STRIDE = 5  # é™ç»´å…³é”®å‚æ•°ï¼š5 -> 60çª—å£ä»…å–12ä¸ªæ»ååˆ‡ç‰‡

# å¯é€‰ï¼šæ˜¯å¦è®¡ç®—Hurstï¼ˆé»˜è®¤å…³é—­ï¼Œé¿å…é«˜è®¡ç®—é‡ä¸ä¸ç¨³å®šï¼‰
USE_HURST = False

# å¯é€‰ï¼šæ˜¯å¦è¿›è¡Œæ—¶é—´åºåˆ—äº¤å‰éªŒè¯ï¼ˆWFA/TS-CVï¼‰ï¼Œé»˜è®¤å…³é—­ä»¥åŠ å¿«è¿è¡Œ
ENABLE_TIME_SERIES_CV = False

# å¯é€‰ï¼šä½¿ç”¨è€ƒè™‘äº¤æ˜“æˆæœ¬ä¸æ»‘ç‚¹çš„é˜ˆå€¼é€‰æ‹©ï¼ˆåœ¨éªŒè¯é›†ä¸Šæœ€å¤§åŒ–å‡€æ”¶ç›Šï¼‰
ENABLE_COST_AWARE_THRESHOLD = True

# å¯é€‰ï¼šé£æ§è¿‡æ»¤å™¨ä¸æŒä»“ç®¡ç†
ENABLE_RISK_FILTER = True
ADX_MIN = 15.0
ATR_NORM_MIN = 0.0012  # ATR_14 / Close
REQUIRE_TREND_CONFIRM = False  # æ”¾å®½ï¼šä¸å¼ºåˆ¶MACDåŒå‘¨æœŸå…±æŒ¯
REQUIRE_PRICE_ABOVE_EMA_4H = False  # æ”¾å®½ï¼šå…è®¸å›æ’¤ä¸­çš„å…¥åœº
ENFORCE_NO_OVERLAP = False  # æ”¾å®½ï¼šå…è®¸å¹¶è¡ŒæŒä»“
COOLDOWN_BARS = 2  # è½»åº¦å†·å´ï¼Œå‡å°‘è¿‡å¯†äº¤æ˜“

# å›ºå®šæ­¢ç›ˆï¼šæ¶¨åˆ° target_return ç›´æ¥æ­¢ç›ˆï¼›æ­¢æŸæŒ‰ max_drawdown_limitï¼ˆé»˜è®¤åš 1:1 æˆ–ç•¥å¾®æ­£å‘çš„ç›ˆäºæ¯”ï¼‰
# é»˜è®¤å…³é—­ ATR åŠ¨æ€æ­¢ç›ˆ/æ­¢æŸï¼Œå…ˆç”¨ç®€å•ã€ç¨³å®šçš„å›ºå®š TP/SL ç»“æ„
USE_ATR_BASED_EXITS = False
TP_ATR_MULT = 2.0
SL_ATR_MULT = 1.2

# å¯é€‰ï¼šæ¦‚ç‡æœŸæœ›ä¸ºæ­£æ‰å¼€ä»“ï¼ˆç”¨è¿‘ä¼¼æœŸæœ›ï¼šp*TP - (1-p)*SL - cost > 0ï¼‰
REQUIRE_POSITIVE_EXPECTANCY = True

# å¯é€‰ï¼šé˜ˆå€¼é€‰æ‹©æ—¶è¦æ±‚éªŒè¯é›†æœ€å°‘äº§ç”Ÿçš„äº¤æ˜“æ•°é‡ï¼Œé¿å…è¿‡æ‹Ÿåˆåˆ°æå°‘æ•°æ ·æœ¬
MIN_VALIDATION_TRADES = 30

# æ¦‚ç‡æ ‡å®šï¼Œæå‡pçš„å¯è§£é‡Šæ€§ï¼ˆç”¨äºEVè¯„ä¼°ä¸é˜ˆå€¼æœç´¢/å›æµ‹ï¼‰
ENABLE_PROBA_CALIBRATION = True

# æ¯æ—¥Top-Kç­›é€‰ï¼Œé™åˆ¶æ¯æ—¥äº¤æ˜“ä¸ºå½“æ—¥æœ€é«˜ç½®ä¿¡åº¦çš„Kç¬”
ENABLE_DAILY_TOP_K = True
DAILY_TOP_K = 2

# æ¯æ—¥æœ€å°æˆäº¤æ•°ä¸å…œåº•æ¦‚ç‡é˜ˆå€¼ï¼ˆç¡®ä¿é¢‘ç‡â‰ˆ1â€“2 ç¬”/å¤©ï¼‰
ENABLE_DAILY_MIN_TRADES = True
MIN_DAILY_TRADES = 1
MIN_DAILY_PROB_FLOOR = 0.35

# ç ´æŸä¿æŠ¤ï¼šåˆ°è¾¾ä¸€å®šæµ®ç›ˆåå°†æ­¢æŸæŠ¬è‡³ä¿æœ¬ï¼Œé™ä½å¤§äºæ¯”ä¾‹
ENABLE_BREAK_EVEN = True
BE_TRIGGER_RET = 0.002   # æµ®ç›ˆè¾¾åˆ° +0.20% æ—¶æ¿€æ´»ä¿æœ¬
BE_STOP_RET = 0.0        # æ¿€æ´»åæ­¢æŸæŠ¬åˆ°å…¥åœºä»·ï¼ˆä¿æœ¬ï¼Œæœªè¦†ç›–æ‰‹ç»­è´¹/æ»‘ç‚¹ï¼‰

# å›æµ‹ç›¸å…³ï¼šæ‰‹ç»­è´¹ä¸æ»‘ç‚¹è®¾ç½®ï¼ˆå•è¾¹è´¹ç‡ä¸å•è¾¹æ»‘ç‚¹ï¼‰
# æŒ‚å• Maker æ‰‹ç»­è´¹ï¼ˆä¾‹å¦‚ 0.02% -> 0.0002ï¼Œå¯æŒ‰å®é™…è´¹ç‡è°ƒæ•´ï¼‰
FEE_RATE = 0.0002
SLIPPAGE_RATE = 0.0005  # 5 bpsï¼Œè‹¥è®¤ä¸ºæŒ‚å•å‡ ä¹æ— æ»‘ç‚¹å¯è¿›ä¸€æ­¥ä¸‹è°ƒ

# è´¦æˆ·å±‚æœ€å¤§å›æ’¤ç›‘æ§é˜ˆå€¼ï¼ˆä¾‹å¦‚ 10%ï¼‰ï¼Œä»…ç”¨äºå›æµ‹æœŸç»Ÿè®¡å’Œå‘Šè­¦ï¼Œä¸å¼ºåˆ¶åœæ­¢äº¤æ˜“
ACCOUNT_MAX_DRAWDOWN = 0.10
TREND_CONFIG = {
    # ç›®æ ‡ï¼šåœ¨ä¸€ä¸ªç›¸å¯¹åˆç†çš„æ—¶é—´çª—å£å†…ï¼ˆçº¦ 6 å°æ—¶ï¼‰åšå– 0.75% å·¦å³çš„æ”¶ç›Šï¼Œ
    # å¹¶å…è®¸æ›´å®½çš„å›æ’¤ï¼ˆçº¦ 1.2%ï¼‰ï¼Œä»¥ä¾¿å®é™…äº¤æ˜“ä¸­èƒ½åƒåˆ°æ›´å¤šâ€œå…ˆè·Œåæ¶¨â€çš„æœºä¼šã€‚
    "look_forward_steps": 12,     # å‘å‰æœ€å¤šè§‚å¯Ÿ 12 æ ¹ 30m K çº¿ï¼ˆçº¦ 6 å°æ—¶ï¼‰
    "target_return": 0.0075,      # ç›®æ ‡æ­¢ç›ˆ 0.75%
    "max_drawdown_limit": 0.012,  # æœ€å¤§å®¹å¿å›æ’¤ 1.20%ï¼ˆTP:SL â‰ˆ 1:1.6ï¼‰
}
logger.info(
    f"è®­ç»ƒç›®æ ‡ç›ˆåˆ©ï¼š{TREND_CONFIG['target_return']*100}%ï¼Œæœ€å¤§å›æ’¤é™åˆ¶ï¼š{TREND_CONFIG['max_drawdown_limit']*100}%"
)

# --- æ–‡ä»¶è·¯å¾„ ---
MODELS_DIR, DATA_DIR = "models_gbm2", "data"
MODEL_SAVE_PATH = os.path.join(
    MODELS_DIR, f"eth_model_high_precision_v4_{INTERVAL}.joblib"
)
SCALER_SAVE_PATH = os.path.join(
    MODELS_DIR, f"eth_scaler_high_precision_v4_{INTERVAL}.joblib"
)
FEATURE_COLUMNS_PATH = os.path.join(
    MODELS_DIR, f"feature_columns_high_precision_v4_{INTERVAL}.joblib"
)
FLATTENED_COLUMNS_PATH = os.path.join(
    MODELS_DIR, f"flattened_columns_high_precision_v4_{INTERVAL}.joblib"
)
CALIBRATOR_SAVE_PATH = os.path.join(
    MODELS_DIR, f"eth_calibrator_v4_{INTERVAL}.joblib"
)
DATA_CACHE_PATH = os.path.join(DATA_DIR, f"{SYMBOL.lower()}_{INTERVAL}_data.csv")

os.makedirs(MODELS_DIR, exist_ok=True)
os.makedirs(DATA_DIR, exist_ok=True)


# --- æ•°æ®è·å–ä¸è¾…åŠ©å‡½æ•° ---
def fetch_binance_klines(s, i, st, en=None, l=1000):
    # ... (æ­¤å‡½æ•°ä¿æŒä¸å˜) ...
    url, cols = "https://api.binance.com/api/v3/klines", [
        "timestamp",
        "Open",
        "High",
        "Low",
        "Close",
        "Volume",
        "close_time",
        "quote_asset_volume",
        "number_of_trades",
        "taker_buy_base_volume",
        "taker_buy_quote_volume",
        "ignore",
    ]
    sts, ets = int(pd.to_datetime(st).timestamp() * 1000), (
        int(pd.to_datetime(en).timestamp() * 1000) if en else int(time.time() * 1000)
    )
    all_d, retries, max_retries = [], 0, 5
    while sts < ets:
        try:
            r = requests.get(
                url,
                params={
                    "symbol": s.upper(),
                    "interval": i,
                    "startTime": sts,
                    "endTime": ets,
                    "limit": l,
                },
                timeout=15,
            )
            r.raise_for_status()
            d = r.json()
            if not d:
                break
            all_d.extend(d)
            sts = d[-1][0] + 1
            retries = 0
        except requests.exceptions.RequestException as e:
            retries += 1
            if retries > max_retries:
                logger.error(f"è·å–æ•°æ®å¤±è´¥è¶…è¿‡æœ€å¤§é‡è¯•æ¬¡æ•°: {e}")
                return pd.DataFrame()
            logger.warning(
                f"è·å–æ•°æ®å¤±è´¥ï¼Œæ­£åœ¨é‡è¯• ({retries}/{max_retries})... Error: {e}"
            )
            time.sleep(retries * 2)
    if not all_d:
        return pd.DataFrame()
    df = pd.DataFrame(all_d, columns=cols)[
        ["timestamp", "Open", "High", "Low", "Close", "Volume"]
    ].copy()
    df["timestamp"] = pd.to_datetime(df["timestamp"], unit="ms")
    for col in df.columns[1:]:
        df[col] = pd.to_numeric(df[col], errors="coerce")
    logger.info(f"âœ… è·å– {s} æ•°æ®æˆåŠŸ: {len(df)} æ¡")
    return df.set_index("timestamp").sort_index()


# <<< ä¿®å¤ç‰ˆï¼šHurstå‡½æ•°ï¼ˆNumbaåŠ é€Ÿï¼Œç¨³å¥å–æ ·ï¼Œæ­£ç¡®æ–œç‡ç¼©æ”¾ï¼‰ >>>
@numba.njit(cache=True)
def compute_hurst_numba(ts):
    n = ts.shape[0]
    if n < 20:
        return 0.5

    # ç¨³å¥é€‰æ‹©æ»åï¼šæœ€å¤šåˆ° N/2ï¼Œå¹¶é™åˆ¶é‡‡æ ·ç‚¹æ•°é‡ï¼Œé¿å…å¤§æ»åæ ·æœ¬è¿‡å°‘å¯¼è‡´ä¸ç¨³å®š
    max_lag = n // 2
    if max_lag < 3:
        return 0.5

    # è‡³å¤šé‡‡æ · ~25 ä¸ªæ»åç‚¹
    step = max(1, max_lag // 25)
    count = ((max_lag - 2) // step) + 1

    # ç´¯ç§¯å¯¹æ•°åŸŸçš„çŸ©ï¼Œç”¨äºçº¿æ€§å›å½’ï¼ˆæ— éœ€ä¸­é—´æ•°ç»„ä¸è¿‡æ»¤ï¼‰
    valid = 0
    sum_x = 0.0
    sum_y = 0.0
    sum_x2 = 0.0
    sum_xy = 0.0

    for i in range(count):
        lag = 2 + i * step
        if lag > max_lag:
            break
        # å·®åˆ†
        m = 0.0
        ln = n - lag
        if ln <= 1:
            continue
        # è®¡ç®—å‡å€¼
        for k in range(ln):
            m += ts[lag + k] - ts[k]
        m /= ln
        # è®¡ç®—æ–¹å·®ï¼ˆddof=0ï¼‰
        v = 0.0
        for k in range(ln):
            d = (ts[lag + k] - ts[k]) - m
            v += d * d
        v /= ln
        if v <= 0.0:
            continue
        tau = np.sqrt(v)
        x = np.log(lag)
        y = np.log(tau)
        valid += 1
        sum_x += x
        sum_y += y
        sum_x2 += x * x
        sum_xy += x * y

    if valid < 2:
        return 0.5

    mx = sum_x / valid
    my = sum_y / valid
    cov = (sum_xy / valid) - (mx * my)
    varx = (sum_x2 / valid) - (mx * mx)
    if varx <= 0.0:
        return 0.5
    slope = cov / varx  # Hurst æ–œç‡ï¼ˆæ— éœ€Ã—2ï¼‰
    # å¤¹ç´§èŒƒå›´ï¼Œé˜²æº¢å‡º
    if slope < 0.0:
        slope = 0.0
    elif slope > 1.0:
        slope = 1.0
    return slope


def get_market_structure_features(df, order=5):
    # åˆ é™¤æœªæ¥ä¿¡æ¯æ³„éœ²ï¼šfind_peaks éœ€è¦å·¦å³ä¸¤ä¾§æ•°æ®ç¡®è®¤å³°å€¼
    # è¿™é‡Œä½¿ç”¨â€œå¯¹ç§°å³°å€¼ç¡®è®¤åå»¶è¿Ÿorderæ ¹â€åŸåˆ™ï¼š
    # å…ˆåœ¨å…¨å±€ä¸Šå®šä½å³°å€¼ï¼Œä½†å°†ç¡®è®¤ç»“æœæ•´ä½“å‘åç§»åŠ¨ order æ ¹ï¼Œ
    # ä¿è¯åœ¨æ—¶åˆ»tä»…èƒ½çœ‹è§ t-order ä¹‹å‰è¢«ç¡®è®¤çš„ç»“æ„ç‚¹ã€‚
    df = df.copy()
    high_peaks_idx, _ = find_peaks(
        df["High"].values, distance=order, prominence=max(df["High"].std() * 0.5, 1e-9)
    )
    low_peaks_idx, _ = find_peaks(
        (-df["Low"]).values, distance=order, prominence=max(df["Low"].std() * 0.5, 1e-9)
    )

    swing_high_raw = np.full(len(df), np.nan)
    swing_high_raw[high_peaks_idx] = df["High"].values[high_peaks_idx]
    swing_low_raw = np.full(len(df), np.nan)
    swing_low_raw[low_peaks_idx] = df["Low"].values[low_peaks_idx]

    # å°†ç¡®è®¤è¿‡çš„å³°å€¼æ•´ä½“åç§» order æ ¹ï¼Œé¿å…åœ¨tä½¿ç”¨åˆ°tä¹‹åçš„æ•°æ®
    df["swing_high_price"] = pd.Series(swing_high_raw, index=df.index).shift(order).ffill()
    df["swing_low_price"] = pd.Series(swing_low_raw, index=df.index).shift(order).ffill()

    df["is_uptrend"] = (
        (df["swing_high_price"] > df["swing_high_price"].shift(1))
        & (df["swing_low_price"] > df["swing_low_price"].shift(1))
    ).astype(int)
    df["is_downtrend"] = (
        (df["swing_high_price"] < df["swing_high_price"].shift(1))
        & (df["swing_low_price"] < df["swing_low_price"].shift(1))
    ).astype(int)
    df["market_structure"] = df["is_uptrend"] - df["is_downtrend"]
    return df[["market_structure"]]


def ema_series(s: pd.Series, length: int) -> pd.Series:
    return s.ewm(span=length, adjust=False, min_periods=length).mean()


def rsi_series(s: pd.Series, length: int = 14) -> pd.Series:
    delta = s.diff()
    gain = delta.clip(lower=0.0)
    loss = -delta.clip(upper=0.0)
    avg_gain = gain.ewm(alpha=1 / length, adjust=False, min_periods=length).mean()
    avg_loss = loss.ewm(alpha=1 / length, adjust=False, min_periods=length).mean()
    rs = avg_gain / avg_loss
    rsi = 100 - (100 / (1 + rs))
    # è¾¹ç•Œå¤„ç†
    rsi = rsi.where(avg_loss != 0, 100.0)
    rsi = rsi.where(avg_gain != 0, 0.0)
    return rsi.rename(f"RSI_{length}")


def macd_df(s: pd.Series, fast: int = 12, slow: int = 26, signal: int = 9,
            col_names: tuple | None = None) -> pd.DataFrame:
    ema_fast = ema_series(s, fast)
    ema_slow = ema_series(s, slow)
    macd_line = ema_fast - ema_slow
    signal_line = macd_line.ewm(span=signal, adjust=False, min_periods=signal).mean()
    hist = macd_line - signal_line
    if col_names is None:
        cols = (f"MACD_{fast}_{slow}_{signal}", f"MACDh_{fast}_{slow}_{signal}", f"MACDs_{fast}_{slow}_{signal}")
    else:
        cols = col_names
    return pd.DataFrame({cols[0]: macd_line, cols[1]: hist, cols[2]: signal_line}, index=s.index)


def bbands_df(s: pd.Series, length: int = 20, std_mult: float = 2.0) -> pd.DataFrame:
    ma = s.rolling(window=length, min_periods=length).mean()
    sd = s.rolling(window=length, min_periods=length).std(ddof=0)
    lower = ma - std_mult * sd
    upper = ma + std_mult * sd
    return pd.DataFrame({
        f"BBL_{length}_{std_mult}": lower,
        f"BBM_{length}_{std_mult}": ma,
        f"BBU_{length}_{std_mult}": upper,
    }, index=s.index)


def atr_series(high: pd.Series, low: pd.Series, close: pd.Series, length: int = 14) -> pd.Series:
    prev_close = close.shift(1)
    tr = pd.concat([
        (high - low),
        (high - prev_close).abs(),
        (low - prev_close).abs(),
    ], axis=1).max(axis=1)
    atr = tr.ewm(alpha=1 / length, adjust=False, min_periods=length).mean()
    return atr.rename(f"ATR_{length}")


def adx_series(high: pd.Series, low: pd.Series, close: pd.Series, length: int = 14) -> pd.Series:
    up_move = high.diff()
    down_move = -low.diff()
    plus_dm = ((up_move > down_move) & (up_move > 0)).astype(float) * up_move.clip(lower=0.0)
    minus_dm = ((down_move > up_move) & (down_move > 0)).astype(float) * down_move.clip(lower=0.0)

    prev_close = close.shift(1)
    tr = pd.concat([
        (high - low),
        (high - prev_close).abs(),
        (low - prev_close).abs(),
    ], axis=1).max(axis=1)

    atr = tr.ewm(alpha=1 / length, adjust=False, min_periods=length).mean()
    plus_di = 100 * (plus_dm.ewm(alpha=1 / length, adjust=False, min_periods=length).mean() / atr)
    minus_di = 100 * (minus_dm.ewm(alpha=1 / length, adjust=False, min_periods=length).mean() / atr)
    dx = 100 * (plus_di.subtract(minus_di).abs() / (plus_di + minus_di))
    adx = dx.ewm(alpha=1 / length, adjust=False, min_periods=length).mean()
    return adx.rename(f"ADX_{length}")


def obv_series(close: pd.Series, volume: pd.Series) -> pd.Series:
    direction = close.diff().fillna(0.0)
    sign = direction.apply(lambda x: 1.0 if x > 0 else (-1.0 if x < 0 else 0.0))
    obv = (sign * volume).cumsum()
    return obv.rename("OBV")


def feature_engineering(df):
    """æ•´åˆæ‰€æœ‰ç‰¹å¾è®¡ç®— - V4.2 å¢åŠ ç‰¹å¾äº¤å‰"""
    df_copy = df.copy()
    logger.info("--- å¼€å§‹è®¡ç®—ç‰¹å¾ (V4.2 å¢å¼ºç‰ˆ - å«ç‰¹å¾äº¤å‰) ---")

    # 1. åŸºç¡€æŒ‡æ ‡ï¼ˆè‡ªå®ç°ï¼‰
    df_copy["RSI_14"] = rsi_series(df_copy["Close"], length=14)
    macd_cols = macd_df(df_copy["Close"], fast=12, slow=26, signal=9)
    df_copy = pd.concat([df_copy, macd_cols], axis=1)
    bb_cols = bbands_df(df_copy["Close"], length=20, std_mult=2.0)
    df_copy = pd.concat([df_copy, bb_cols], axis=1)
    df_copy["ADX_14"] = adx_series(df_copy["High"], df_copy["Low"], df_copy["Close"], length=14)
    df_copy["ATR_14"] = atr_series(df_copy["High"], df_copy["Low"], df_copy["Close"], length=14)
    df_copy["OBV"] = obv_series(df_copy["Close"], df_copy["Volume"]) 

    # 2. å¸‚åœºç»“æ„ä¸é•¿å‘¨æœŸè¶‹åŠ¿ (ä¿æŒä¸å˜)
    market_structure_df = get_market_structure_features(df_copy)
    macd_long = macd_df(
        df_copy["Close"], fast=24, slow=52, signal=18,
        col_names=("MACD_long", "MACDh_long", "MACDs_long")
    )
    df_copy = pd.concat([df_copy, macd_long], axis=1)

    # 3. å¸‚åœºçŠ¶æ€ä¸æ³¢åŠ¨æ€§ï¼ˆHurst é»˜è®¤å…³é—­ï¼Œé¿å…ä¸ç¨³å®šä¸é«˜è®¡ç®—é‡ï¼‰
    if USE_HURST:
        logger.info("æ­£åœ¨è®¡ç®—HurstæŒ‡æ•° (å¯èƒ½è¾ƒæ…¢)...")
        # å¯é€‰ï¼šåœ¨è¾ƒä½é¢‘ç‡ä¸Šè®¡ç®—æ›´ç¨³å¥çš„Hurståå†å¯¹é½
        # è¿™é‡Œä»æä¾›åŸå§‹å®ç°çš„å¼€å…³
        df_copy["hurst"] = (
            df_copy["Close"].rolling(window=100).apply(compute_hurst_numba, raw=True)
        )
        logger.info("HurstæŒ‡æ•°è®¡ç®—å®Œæˆã€‚")
    else:
        df_copy["hurst"] = 0.5  # å…³é—­æ—¶ä½¿ç”¨0.5ä½œä¸ºå¸¸é‡å ä½ï¼Œé¿å…å™ªå£°å½±å“
    df_copy["volatility_log"] = (
        (np.log(df_copy["Close"] / df_copy["Close"].shift(1))).rolling(window=20).std()
    )

    # 4. "è§„åˆ™" è½¬åŒ–ä¸º "ç‰¹å¾" (ä¿æŒä¸å˜)
    df_copy["macd_cross_signal"] = (
        df_copy["MACD_12_26_9"] > df_copy["MACDs_12_26_9"]
    ).astype(int)
    df_copy["macd_long_cross_signal"] = (
        df_copy["MACD_long"] > df_copy["MACDs_long"]
    ).astype(int)

    # 5. å¤šæ—¶é—´æ¡†æ¶ç‰¹å¾ï¼ˆä¿®æ­£4Hæ³„éœ²ï¼šä»…ä½¿ç”¨å·²å®Œæˆçš„4Hå‘¨æœŸï¼‰
    # ä½¿ç”¨å³é—­åˆçª— + å³æ ‡ç­¾ï¼Œç¡®ä¿æ—¶é—´æˆ³ä»£è¡¨â€œä¸Šä¸€æ ¹å·²æ”¶ç›˜çš„4H Kçº¿â€
    close_4h = df_copy["Close"].resample("4h", label="right", closed="right").last()
    ema_4h = ema_series(close_4h, length=50)
    # å…³é”®ï¼šæ•´ä½“åç§»ä¸€æ ¹4Hï¼Œé¿å…åœ¨15mçš„4HåŒºé—´å†…çœ‹åˆ°å½“å‰æœªå®Œæˆçš„4Hæ•°æ®
    ema_4h_shifted = ema_4h.shift(1)
    df_copy["ema_4h"] = ema_4h_shifted.reindex(df_copy.index, method="ffill")
    df_copy["price_above_ema_4h"] = (df_copy["Close"] > df_copy["ema_4h"]).astype(int)

    # 6. --- <<< æ–°å¢ï¼šç‰¹å¾äº¤å‰ (Feature Crossing) >>> ---
    logger.info("æ­£åœ¨åˆ›å»ºäº¤å‰ç‰¹å¾...")
    # ç¤ºä¾‹1: æ³¢åŠ¨ç‡ä¸è¶‹åŠ¿å¼ºåº¦çš„äº¤äº’ (é«˜ADXå’Œé«˜ATRå¯èƒ½æ„å‘³ç€å¼ºåŠ›çªç ´)
    df_copy["adx_x_atr_norm"] = (df_copy["ADX_14"] / 50) * (
        df_copy["ATR_14"] / df_copy["Close"]
    )
    # ç¤ºä¾‹2: RSIä¸å¸‚åœºçŠ¶æ€çš„äº¤äº’ (è¶‹åŠ¿å¸‚ä¸­çš„RSI vs éœ‡è¡å¸‚çš„RSI)
    df_copy["rsi_x_hurst"] = df_copy["RSI_14"] * df_copy["hurst"]
    # ç¤ºä¾‹3: çŸ­æœŸè¶‹åŠ¿ä¸é•¿å‘¨æœŸè¶‹åŠ¿çš„ç¡®è®¤ (ä¸¤ä¸ªMACDéƒ½çœ‹æ¶¨)
    df_copy["macd_cross_confirm"] = (
        df_copy["macd_cross_signal"] * df_copy["macd_long_cross_signal"]
    )
    logger.info("äº¤å‰ç‰¹å¾åˆ›å»ºå®Œæˆã€‚")

    # 7. æ•´åˆ
    df_copy = pd.concat([df_copy, market_structure_df], axis=1)

    # 8. é€‰æ‹©ç‰¹å¾åˆ—å¹¶è¿›è¡Œæ¸…ç†
    feature_columns = [
        col
        for col in df_copy.columns
        if col
        not in [
            "Open",
            "High",
            "Low",
            "Close",
            "Volume",
            "swing_high_price",
            "swing_low_price",
            "ema_4h",
        ]
    ]

    # é™„åŠ ä½ç»´ã€ç¨³å®šçš„æ´¾ç”Ÿç‰¹å¾ï¼šæ”¶ç›Šç‡ä¸å¤šå°ºåº¦åŠ¨é‡
    df_copy["ret_1"] = df_copy["Close"].pct_change()
    df_copy["ret_4"] = df_copy["Close"].pct_change(4)
    df_copy["ret_16"] = df_copy["Close"].pct_change(16)
    df_copy["rsi_delta_1"] = df_copy["RSI_14"].diff(1)
    df_copy["macd_delta_1"] = df_copy["MACD_12_26_9"].diff(1)

    # åˆå¹¶å¹¶æ¸…ç†
    all_features_df = df_copy[feature_columns + [
        "ret_1","ret_4","ret_16","rsi_delta_1","macd_delta_1"
    ]].replace([np.inf, -np.inf], np.nan).ffill()

    return all_features_df


# --- (å…¶ä»–å‡½æ•°å¦‚ create_trend_labels, train_and_validate ç­‰éƒ½ä½¿ç”¨æˆ‘ä»¬ä¸Šä¸€è½®è®¨è®ºçš„æœ€æ–°ç‰ˆæœ¬) ---
# ...
# The rest of the script (create_trend_labels, train_and_validate, run_backtest_and_evaluate, __main__)
# remains IDENTICAL to the one provided in the previous response ("ç»™å‡ºä¿®æ”¹åçš„å®Œæ•´ç‰ˆä»£ç ").
# You only need to replace the `compute_hurst` and `feature_engineering` functions
# and add `import numba` at the top.
#
# For completeness, I'll paste the rest of the script again.
#
def create_trend_labels(df, look_forward_steps, target_return, max_drawdown_limit):
    """
    æ ‡ç­¾ A ç‰ˆæœ¬ï¼šåªå…³æ³¨â€œæœªæ¥æ˜¯å¦æœ‰è¶³å¤Ÿä¸Šè¡Œç©ºé—´â€ï¼Œä¸åœ¨æ ‡ç­¾ä¸­å¼ºè¡Œçº¦æŸå›æ’¤ã€‚

    å®šä¹‰ï¼š
      - label = 1 å½“ä¸”ä»…å½“ï¼šåœ¨æœªæ¥ look_forward_steps æ ¹ K çº¿å†…ï¼Œ
        æœ€é«˜ä»·æ›¾ç»è§¦åŠæˆ–è¶…è¿‡å½“å‰æ”¶ç›˜ä»· * (1 + target_return)ï¼›
      - å¦åˆ™ label = 0ã€‚

    è¯´æ˜ï¼š
      - max_drawdown_limit å‚æ•°åœ¨æ ‡ç­¾ä¸­ä¸å†ä½¿ç”¨ï¼Œåªä½œä¸ºç­–ç•¥/å›æµ‹å±‚ TP/SL çš„é£é™©æ§åˆ¶ï¼›
      - è¿™æ ·å¯ä»¥å¢åŠ æ­£æ ·æœ¬æ•°é‡ï¼Œä½¿æ¨¡å‹ä¸“æ³¨äºå­¦ä¹ â€œåç»­æœ‰è¶³å¤Ÿä¸Šè¡Œç©ºé—´â€çš„æƒ…å½¢ï¼Œ
        å›æ’¤æ§åˆ¶åˆ™åœ¨äº¤æ˜“æ‰§è¡Œé€»è¾‘ä¸­é€šè¿‡æ­¢æŸ/ä¿æœ¬æ¥å®Œæˆã€‚
    """
    df_copy = df.copy()
    df_copy["target_price"] = df_copy["Close"] * (1 + target_return)
    future_highs = (
        df_copy["High"]
        .rolling(window=look_forward_steps)
        .max()
        .shift(-look_forward_steps)
    )
    profit_reached = future_highs >= df_copy["target_price"]
    df_copy["label"] = profit_reached.astype(int)
    return df_copy


def create_flattened_sequences(data, labels, look_back=60, stride=1):
    """
    ä»¥æ­¥è¿›æ–¹å¼å±•å¹³åºåˆ—ï¼Œå‡å°‘ç»´åº¦å¹¶é™ä½å†—ä½™ã€‚
    data: np.ndarray [T, F]
    labels: np.ndarray [T]
    look_back: å†å²çª—å£é•¿åº¦
    stride: æ­¥é•¿ï¼Œ>1 æ—¶ä»…é‡‡æ ·ç¨€ç–å†å²åˆ‡ç‰‡ï¼Œé™ç»´å…³é”®
    """
    X, y = [], []
    if stride < 1:
        stride = 1
    # é€‰æ‹©å°† [i-look_back, i) ä¸­æŒ‰æ­¥é•¿æŠ½æ ·
    for i in range(look_back, len(data)):
        window = data[i - look_back : i : stride, :]
        X.append(window.flatten())
        y.append(labels[i])
    return np.array(X, dtype=np.float32), np.array(y, dtype=np.int32)


def train_and_validate(train_df, validation_df, look_back, trend_config):
    logger.info("--- å¼€å§‹è®­ç»ƒå’ŒéªŒè¯æµç¨‹ (V4) ---")
    X_train_full_df = feature_engineering(train_df).dropna()
    X_validation_full_df = feature_engineering(validation_df).dropna()
    train_labeled = create_trend_labels(train_df, **trend_config)
    validation_labeled = create_trend_labels(validation_df, **trend_config)
    y_train_full = train_labeled["label"].align(X_train_full_df, join="inner", axis=0)[
        0
    ]
    X_train_full_df = X_train_full_df.align(
        train_labeled["label"], join="inner", axis=0
    )[0]
    y_validation_full = validation_labeled["label"].align(
        X_validation_full_df, join="inner", axis=0
    )[0]
    X_validation_full_df = X_validation_full_df.align(
        validation_labeled["label"], join="inner", axis=0
    )[0]
    X_validation_full_df = X_validation_full_df[X_train_full_df.columns]
    scaler = MinMaxScaler(feature_range=(0, 1))
    X_train_scaled = scaler.fit_transform(X_train_full_df)
    X_validation_scaled = scaler.transform(X_validation_full_df)
    original_columns = X_train_full_df.columns
    # åŸºäºæ­¥é•¿çš„æ‰å¹³åŒ–åˆ—åï¼ˆä¾‹å¦‚ lag_59, lag_54, ..., lag_0ï¼‰
    selected_lags = list(range(look_back - 1, -1, -LAG_STRIDE))
    flattened_columns = [
        f"{col}_lag_{lag}"
        for lag in selected_lags
        for col in original_columns
    ]
    joblib.dump(original_columns, FEATURE_COLUMNS_PATH)
    joblib.dump(flattened_columns, FLATTENED_COLUMNS_PATH)
    X_train_np, y_train = create_flattened_sequences(
        X_train_scaled, y_train_full.values, look_back, stride=LAG_STRIDE
    )
    X_validation_np, y_validation = create_flattened_sequences(
        X_validation_scaled, y_validation_full.values, look_back, stride=LAG_STRIDE
    )
    X_train_df = pd.DataFrame(X_train_np, columns=flattened_columns)
    X_validation_df = pd.DataFrame(X_validation_np, columns=flattened_columns)
    logger.info(f"è®­ç»ƒæ ·æœ¬: {len(X_train_df)}, éªŒè¯æ ·æœ¬: {len(X_validation_df)}")
    train_label_counts = np.bincount(y_train)
    if train_label_counts.size < 2 or train_label_counts[1] == 0:
        logger.error("è®­ç»ƒæ•°æ®ä¸­æ²¡æœ‰æ­£æ ·æœ¬(label=1)ï¼Œæ— æ³•ç»§ç»­ã€‚")
        return None, None, None
    precision_focus_ratio = 0.3
    scale_pos_weight = (
        train_label_counts[0] / train_label_counts[1]
    ) * precision_focus_ratio
    logger.info(f"è°ƒæ•´åçš„ scale_pos_weight (è¿½æ±‚é«˜èƒœç‡): {scale_pos_weight:.2f}")
    lgb_params = {
        "objective": "binary",
        "metric": "logloss",
        "n_estimators": 2000,
        "learning_rate": 0.02,
        "num_leaves": 20,
        "max_depth": 5,
        "seed": 42,
        "n_jobs": -1,
        "verbose": -1,
        "scale_pos_weight": scale_pos_weight,
        "colsample_bytree": 0.7,
        "subsample": 0.7,
        "reg_alpha": 0.1,
    }

    # å¯é€‰ï¼šæ—¶é—´åºåˆ—äº¤å‰éªŒè¯ï¼ˆWFA/TS-CVï¼‰ä»¥è¯„ä¼°ç¨³å¥æ€§
    if ENABLE_TIME_SERIES_CV:
        logger.info("å¯åŠ¨æ—¶é—´åºåˆ—äº¤å‰éªŒè¯ (TS-CV) è¯„ä¼°ç¨³å¥æ€§...")
        tscv = TimeSeriesSplit(n_splits=3)
        cv_precisions, cv_recalls = [], []
        for fold, (tr_idx, va_idx) in enumerate(tscv.split(X_train_df)):
            X_tr, y_tr = X_train_df.iloc[tr_idx], y_train[tr_idx]
            X_va, y_va = X_train_df.iloc[va_idx], y_train[va_idx]

            cv_model = lgb.LGBMClassifier(**{**lgb_params, "n_estimators": 500})
            cv_model.fit(
                X_tr,
                y_tr,
                eval_set=[(X_va, y_va)],
                eval_metric="logloss",
                callbacks=[lgb.early_stopping(stopping_rounds=50, verbose=False)],
            )
            y_va_prob = cv_model.predict_proba(X_va)[:, 1]
            # ä½¿ç”¨0.5é˜ˆå€¼ç²—ç•¥è¯„ä¼°
            y_va_pred = (y_va_prob > 0.5).astype(int)
            p = precision_score(y_va, y_va_pred, zero_division=0)
            r = recall_score(y_va, y_va_pred, zero_division=0)
            cv_precisions.append(p)
            cv_recalls.append(r)
            logger.info(f"TS-CV æŠ˜{fold+1}: Precision={p:.4f}, Recall={r:.4f}")
        logger.info(
            f"TS-CV å¹³å‡: Precision={np.mean(cv_precisions):.4f}, Recall={np.mean(cv_recalls):.4f}"
        )
    lgb_model = lgb.LGBMClassifier(**lgb_params)
    logger.info("\nå¼€å§‹è®­ç»ƒ LightGBM æ¨¡å‹ (V4)...")
    lgb_model.fit(
        X_train_df,
        y_train,
        eval_set=[(X_validation_df, y_validation)],
        eval_metric="logloss",
        callbacks=[lgb.early_stopping(stopping_rounds=100, verbose=False)],
    )
    y_val_pred_probs = lgb_model.predict_proba(X_validation_df)[:, 1]
    if ENABLE_PROBA_CALIBRATION:
        try:
            calibrator = IsotonicRegression(out_of_bounds="clip")
            calibrator.fit(y_val_pred_probs, y_validation)
            y_val_pred_probs = calibrator.transform(y_val_pred_probs)
            joblib.dump(calibrator, CALIBRATOR_SAVE_PATH)
            logger.info("å·²å®Œæˆæ¦‚ç‡æ ‡å®š(Isotonic)å¹¶ä¿å­˜æ ¡å‡†å™¨ã€‚")
        except Exception as e:
            logger.warning(f"æ¦‚ç‡æ ‡å®šå¤±è´¥ï¼Œä½¿ç”¨æœªæ ‡å®šæ¦‚ç‡ã€‚Error: {e}")

    # ç»Ÿä¸€çš„é˜ˆå€¼å˜é‡ä¸æ”¶ç›Šè®°å½•ï¼š
    # - è‹¥æˆæœ¬æ„ŸçŸ¥æœç´¢æ‰¾åˆ°â€œéªŒè¯é›†å¹³å‡å‡€æ”¶ç›Š > 0â€çš„é˜ˆå€¼ï¼Œåˆ™ä¼˜å…ˆé‡‡ç”¨ï¼›
    # - å¦åˆ™å›é€€åˆ°åŸºäºç²¾ç¡®ç‡/F1 çš„é˜ˆå€¼é€‰æ‹©ï¼Œé¿å…å¼ºè¡Œé”å®šåœ¨äºæŸé˜ˆå€¼ä¸Šã€‚
    best_threshold = 0.5
    best_avg_net = None

    if ENABLE_COST_AWARE_THRESHOLD:
        # ä½¿ç”¨éªŒè¯é›†åŸºäºå‡€æ”¶ç›Šé€‰æ‹©é˜ˆå€¼
        logger.info("åŸºäºå‡€æ”¶ç›Šé€‰æ‹©æœ€ä½³é˜ˆå€¼ï¼ˆå«æ‰‹ç»­è´¹ä¸æ»‘ç‚¹ï¼‰...")
        # æ„å»ºä¸éªŒè¯åºåˆ—å¯¹é½çš„ç´¢å¼•ä¸ä»·æ ¼åºåˆ—
        val_feat_index = X_validation_full_df.index
        pred_index = val_feat_index[look_back : look_back + len(y_val_pred_probs)]
        v_close = validation_df["Close"].reindex(pred_index)
        v_high = validation_df["High"].reindex(pred_index)
        v_low = validation_df["Low"].reindex(pred_index)

        # é£æ§è¿‡æ»¤å™¨ï¼ˆä»…åœ¨éœ€è¦æ—¶å¯ç”¨ï¼‰
        if ENABLE_RISK_FILTER:
            risk_mask = np.ones(len(pred_index), dtype=bool)
            try:
                adx_vals = X_validation_full_df["ADX_14"].reindex(pred_index).values
                atr_vals = X_validation_full_df["ATR_14"].reindex(pred_index).values
                macd_conf = X_validation_full_df["macd_cross_confirm"].reindex(pred_index).values.astype(bool)
                price_above = X_validation_full_df["price_above_ema_4h"].reindex(pred_index).values.astype(bool)
            except KeyError:
                # å…¼å®¹åˆ—ç¼ºå¤±çš„æƒ…å†µ
                adx_vals = np.full(len(pred_index), np.nan)
                atr_vals = np.full(len(pred_index), np.nan)
                macd_conf = np.zeros(len(pred_index), dtype=bool)
                price_above = np.zeros(len(pred_index), dtype=bool)

            if ADX_MIN is not None:
                risk_mask &= np.isfinite(adx_vals) & (adx_vals >= ADX_MIN)
            if ATR_NORM_MIN is not None:
                atr_norm = np.divide(atr_vals, v_close.values, out=np.zeros_like(atr_vals), where=np.isfinite(atr_vals) & np.isfinite(v_close.values) & (v_close.values > 0))
                risk_mask &= atr_norm >= ATR_NORM_MIN
            if REQUIRE_TREND_CONFIRM:
                risk_mask &= macd_conf
            if REQUIRE_PRICE_ABOVE_EMA_4H:
                risk_mask &= price_above
        else:
            risk_mask = np.ones(len(pred_index), dtype=bool)

        # ä½¿ç”¨ä¸€ç»„å€™é€‰é˜ˆå€¼ï¼ˆåˆ†ä½æ•°ï¼‰
        # å€™é€‰é˜ˆå€¼æ›´å¤šé›†ä¸­åœ¨é«˜åˆ†ä½ï¼Œå‡å°‘è¿‡å¤šäº¤æ˜“
        qs = np.concatenate(
            [
                np.linspace(0.35, 0.90, 24),
                np.linspace(0.91, 0.98, 8),
            ]
        )
        thresh_candidates = np.unique(np.quantile(y_val_pred_probs, qs))
        best_avg_net = -1e9
        look_forward = trend_config["look_forward_steps"]
        dd_limit = trend_config["max_drawdown_limit"]

        cost_per_trade = 2 * (FEE_RATE + SLIPPAGE_RATE)
        # é¢„è®¡ç®—æ¯æ—¥ç´¢å¼•
        day_index = pd.Index(pred_index).normalize()
        unique_days = np.unique(day_index.values)

        for th in thresh_candidates:
            sig_raw = (y_val_pred_probs > th)
            cand_mask = (sig_raw & risk_mask)

            # æ¯æ—¥Top-K + æ¯æ—¥æœ€å°æˆäº¤æ•°
            if ENABLE_DAILY_TOP_K or ENABLE_DAILY_MIN_TRADES:
                allow = np.zeros(len(pred_index), dtype=bool)
                for d in unique_days:
                    # å€™é€‰å±‚çº§ï¼šcand(å«EV)ã€prob+riskï¼ˆä¸å«EVï¼‰ã€prob-onlyã€å…¨é‡
                    day_mask_all = (day_index.values == d)
                    day_cand = np.where(day_mask_all & cand_mask)[0]
                    day_prob_risk = np.where(day_mask_all & sig_raw & risk_mask)[0]
                    day_prob_only = np.where(day_mask_all & sig_raw)[0]
                    day_all = np.where(day_mask_all)[0]

                    selected = []
                    # å…ˆå– cand ä¸­çš„Top-K
                    if day_cand.size > 0:
                        k = DAILY_TOP_K if ENABLE_DAILY_TOP_K else day_cand.size
                        k = min(k, day_cand.size)
                        topk = day_cand[np.argsort(y_val_pred_probs[day_cand])[-k:]]
                        selected.extend(topk.tolist())

                    # è‹¥éœ€è¦æ¯æ—¥æœ€å°æˆäº¤æ•°ï¼Œåˆ™æŒ‰å±‚çº§è¡¥è¶³
                    need = 0
                    if ENABLE_DAILY_MIN_TRADES:
                        need = max(0, MIN_DAILY_TRADES - len(selected))
                    if need > 0 and day_prob_risk.size > 0:
                        # å»æ‰å·²é€‰å¹¶è¡¥è¶³
                        remain = np.setdiff1d(day_prob_risk, np.array(selected, dtype=int), assume_unique=False)
                        if remain.size > 0:
                            take = min(need, remain.size)
                            extra = remain[np.argsort(y_val_pred_probs[remain])[-take:]]
                            selected.extend(extra.tolist())
                            need = max(0, MIN_DAILY_TRADES - len(selected))
                    if need > 0 and day_prob_only.size > 0:
                        remain = np.setdiff1d(day_prob_only, np.array(selected, dtype=int), assume_unique=False)
                        if remain.size > 0:
                            # ä»…åœ¨æ»¡è¶³å…œåº•æ¦‚ç‡é˜ˆå€¼ä¸‹è¡¥å……
                            remain = remain[y_val_pred_probs[remain] >= MIN_DAILY_PROB_FLOOR]
                            if remain.size > 0:
                                take = min(need, remain.size)
                                extra = remain[np.argsort(y_val_pred_probs[remain])[-take:]]
                                selected.extend(extra.tolist())

                    # è½åˆ°allow
                    if len(selected) > 0:
                        allow[np.array(selected, dtype=int)] = True
                sig = allow.astype(int)
            else:
                sig = cand_mask.astype(int)
            pnl_list = []
            busy_until = -1
            for t, s in enumerate(sig):
                if s != 1:
                    continue
                if t <= busy_until:
                    continue
                if pd.isna(v_close.iloc[t]) or pd.isna(v_high.iloc[t]) or pd.isna(v_low.iloc[t]):
                    continue
                entry = v_close.iloc[t] * (1 + SLIPPAGE_RATE)
                # åŠ¨æ€æ­¢ç›ˆæ­¢æŸï¼ˆå—é™äºé…ç½®è¾¹ç•Œï¼‰
                if USE_ATR_BASED_EXITS and np.isfinite(atr_vals[t]) and v_close.iloc[t] > 0:
                    atr_n = float(atr_vals[t] / v_close.iloc[t])
                    tp_ret = max(trend_config["target_return"], TP_ATR_MULT * atr_n)
                    sl_ret = min(dd_limit, SL_ATR_MULT * atr_n)
                else:
                    tp_ret = trend_config["target_return"]
                    sl_ret = dd_limit

                # æœŸæœ›ä¸ºæ­£è¿‡æ»¤ï¼ˆå¯é€‰ï¼‰
                if REQUIRE_POSITIVE_EXPECTANCY:
                    p = float(y_val_pred_probs[t])
                    if (p * tp_ret - (1 - p) * sl_ret - cost_per_trade) <= 0:
                        continue

                target = entry * (1 + tp_ret)
                stop = entry * (1 - sl_ret)
                t_end = min(t + look_forward, len(pred_index) - 1)
                hit = False
                stopped = False
                be_active = False
                breakeven = False
                be_trigger = entry * (1 + BE_TRIGGER_RET) if ENABLE_BREAK_EVEN else 0.0
                be_stop = entry * (1 + BE_STOP_RET) if ENABLE_BREAK_EVEN else 0.0
                for j in range(t + 1, t_end + 1):
                    # å…ˆæ£€æŸ¥æ˜¯å¦è§¦å‘æ­¢æŸï¼Œå†æ£€æŸ¥æ˜¯å¦å‘½ä¸­ç›®æ ‡
                    if v_low.iloc[j] <= stop:
                        exit_price = stop * (1 - SLIPPAGE_RATE)
                        stopped = True
                        break
                    if v_high.iloc[j] >= target:
                        exit_price = target * (1 - SLIPPAGE_RATE)
                        hit = True
                        break
                    if ENABLE_BREAK_EVEN and (not be_active) and (v_high.iloc[j] >= be_trigger):
                        be_active = True
                    if ENABLE_BREAK_EVEN and be_active and (v_low.iloc[j] <= be_stop):
                        exit_price = be_stop * (1 - SLIPPAGE_RATE)
                        breakeven = True
                        break
                if not hit and not stopped and not breakeven:
                    exit_price = v_close.iloc[t_end] * (1 - SLIPPAGE_RATE)
                    busy_until = t_end + COOLDOWN_BARS
                else:
                    busy_until = j + COOLDOWN_BARS
                gross = (exit_price - entry) / entry
                net = gross - 2 * FEE_RATE
                pnl_list.append(net)
            # åªæœ‰å½“äº¤æ˜“æ•°æ»¡è¶³é˜ˆå€¼æ—¶ï¼Œæ‰è€ƒè™‘è¯¥é˜ˆå€¼ï¼Œå¦åˆ™è·³è¿‡ä»¥é¿å…æ ·æœ¬è¿‡å°‘
            trade_cnt = len(pnl_list)
            if trade_cnt >= MIN_VALIDATION_TRADES and trade_cnt > 0:
                avg_net = float(np.mean(pnl_list))
                if avg_net > best_avg_net:
                    best_avg_net = avg_net
                    best_threshold = float(th)
        if best_avg_net == -1e9:
            # å¦‚æœæ‰€æœ‰å€™é€‰é˜ˆå€¼éƒ½æœªè¾¾åˆ°æœ€å°äº¤æ˜“æ•°è¦æ±‚ï¼Œåˆ™æ”¾å®½è¦æ±‚ï¼Œé€‰æ‹©å¹³å‡å‡€æ”¶ç›Šæœ€é«˜è€…
            logger.warning(
                f"æœªæ‰¾åˆ°æ»¡è¶³æœ€å°äº¤æ˜“æ•°({MIN_VALIDATION_TRADES})çš„é˜ˆå€¼ï¼Œæ”¾å®½è¦æ±‚è¿›è¡Œé€‰æ‹©ã€‚"
            )
            best_avg_net = -1e9
            best_threshold = 0.5
            for th in thresh_candidates:
                sig_raw = y_val_pred_probs > th
                sig = (sig_raw & risk_mask).astype(int)
                pnl_list = []
                busy_until = -1
                for t, s in enumerate(sig):
                    if s != 1:
                        continue
                    if t <= busy_until:
                        continue
                    if pd.isna(v_close.iloc[t]) or pd.isna(v_high.iloc[t]) or pd.isna(
                        v_low.iloc[t]
                    ):
                        continue
                    entry = v_close.iloc[t] * (1 + SLIPPAGE_RATE)
                    if (
                        USE_ATR_BASED_EXITS
                        and np.isfinite(atr_vals[t])
                        and v_close.iloc[t] > 0
                    ):
                        atr_n = float(atr_vals[t] / v_close.iloc[t])
                        tp_ret = max(
                            trend_config["target_return"], TP_ATR_MULT * atr_n
                        )
                        sl_ret = min(dd_limit, SL_ATR_MULT * atr_n)
                    else:
                        tp_ret = trend_config["target_return"]
                        sl_ret = dd_limit
                    # æ”¾å®½é˜¶æ®µï¼šä¸å¼ºåˆ¶æ­£æœŸæœ›è¿‡æ»¤ï¼Œé¿å…æ— äº¤æ˜“å¯¼è‡´é˜ˆå€¼é€€åŒ–
                    target = entry * (1 + tp_ret)
                    stop = entry * (1 - sl_ret)
                    t_end = min(t + look_forward, len(pred_index) - 1)
                    hit = False
                    stopped = False
                    for j in range(t + 1, t_end + 1):
                        if v_low.iloc[j] <= stop:
                            exit_price = stop * (1 - SLIPPAGE_RATE)
                            stopped = True
                            break
                        if v_high.iloc[j] >= target:
                            exit_price = target * (1 - SLIPPAGE_RATE)
                            hit = True
                            break
                    if not hit and not stopped:
                        exit_price = v_close.iloc[t_end] * (1 - SLIPPAGE_RATE)
                        busy_until = t_end + COOLDOWN_BARS
                    else:
                        busy_until = j + COOLDOWN_BARS
                    gross = (exit_price - entry) / entry
                    net = gross - 2 * FEE_RATE
                    pnl_list.append(net)
                if len(pnl_list) > 0:
                    avg_net = float(np.mean(pnl_list))
                    if avg_net > best_avg_net:
                        best_avg_net = avg_net
                        best_threshold = float(th)
        if best_avg_net == -1e9:
            logger.warning(
                "åŸºäºå‡€æ”¶ç›Šé€‰æ‹©æœªèƒ½æ‰¾åˆ°æœ‰æ•ˆé˜ˆå€¼ï¼Œå°†åœ¨å›é€€è·¯å¾„ä¸­ä½¿ç”¨F1æˆ–é»˜è®¤é˜ˆå€¼ã€‚"
            )
            best_avg_net = None
        elif best_avg_net <= 0:
            logger.warning(
                f"åŸºäºå‡€æ”¶ç›Šæœç´¢çš„æœ€ä¼˜é˜ˆå€¼åœ¨éªŒè¯é›†ä¸Šçš„å¹³å‡å‡€æ”¶ç›Šä»ä¸ºè´Ÿ: {best_avg_net:.5f}ï¼Œå°†å›é€€åˆ°åŸºäºç²¾ç¡®ç‡/F1 çš„é˜ˆå€¼æœç´¢ã€‚"
            )
        else:
            logger.info(
                f"åŸºäºå‡€æ”¶ç›Šé€‰æ‹©çš„æœ€ä½³é˜ˆå€¼: {best_threshold:.4f} (éªŒè¯é›†å¹³å‡å‡€æ”¶ç›Š: {best_avg_net:.5f})"
            )

    # --- ç»Ÿä¸€çš„ F1/ç²¾ç¡®ç‡ å›é€€é€»è¾‘ ---
    # è§¦å‘æ¡ä»¶ï¼š
    # 1) æ²¡å¼€å¯æˆæœ¬æ„ŸçŸ¥æœç´¢ï¼›æˆ–
    # 2) æˆæœ¬æ„ŸçŸ¥æœç´¢æ‰¾ä¸åˆ°ä»»ä½•æœ‰æ•ˆé˜ˆå€¼ï¼›æˆ–
    # 3) æ‰¾åˆ°çš„æœ€ä½³é˜ˆå€¼åœ¨éªŒè¯é›†ä¸Šçš„å¹³å‡å‡€æ”¶ç›Šä»ä¸ºè´Ÿã€‚
    if (not ENABLE_COST_AWARE_THRESHOLD) or (best_avg_net is None) or (
        best_avg_net is not None and best_avg_net <= 0
    ):
        MIN_PRECISION_TARGET = 0.55
        precisions, recalls, thresholds = precision_recall_curve(
            y_validation, y_val_pred_probs
        )
        valid_threshold_indices = np.where(precisions[:-1] >= MIN_PRECISION_TARGET)[0]
        if len(valid_threshold_indices) > 0:
            f1_scores = np.divide(
                2 * recalls * precisions,
                recalls + precisions,
                out=np.zeros_like(recalls),
                where=(recalls + precisions) != 0,
            )
            best_idx_within_valid = np.argmax(f1_scores[valid_threshold_indices])
            final_best_idx = valid_threshold_indices[best_idx_within_valid]
            best_threshold = thresholds[final_best_idx]
            logger.info(
                f"åœ¨æ»¡è¶³èƒœç‡>{MIN_PRECISION_TARGET*100}%çš„æ¡ä»¶ä¸‹ï¼Œæ‰¾åˆ°æœ€ä½³é˜ˆå€¼: {best_threshold:.4f}"
            )
            logger.info(
                f"è¯¥é˜ˆå€¼ä¸‹çš„éªŒè¯é›†è¡¨ç°: Precision={precisions[final_best_idx]:.4f}, Recall={recalls[final_best_idx]:.4f}"
            )
        else:
            logger.warning(
                f"æœªèƒ½æ‰¾åˆ°ä»»ä½•é˜ˆå€¼å¯ä»¥ä½¿éªŒè¯é›†èƒœç‡è¾¾åˆ° {MIN_PRECISION_TARGET*100}%ã€‚å°†ä½¿ç”¨æœ€å¤§åŒ–F1çš„é˜ˆå€¼ã€‚"
            )
            f1_scores = np.divide(
                2 * recalls * precisions,
                recalls + precisions,
                out=np.zeros_like(recalls),
                where=(recalls + precisions) != 0,
            )
            best_f1_idx = np.argmax(f1_scores)
            best_threshold = (
                thresholds[best_f1_idx] if len(thresholds) > best_f1_idx else 0.5
            )
            logger.info(f"åœ¨éªŒè¯é›†ä¸Šæ‰¾åˆ°çš„æœ€ä½³F1é˜ˆå€¼: {best_threshold:.4f}")
    joblib.dump(lgb_model, MODEL_SAVE_PATH)
    joblib.dump(scaler, SCALER_SAVE_PATH)
    logger.info(f"æ¨¡å‹å·²ä¿å­˜åˆ°: {MODEL_SAVE_PATH}")
    return lgb_model, scaler, best_threshold


# --- ğŸš€ 4. å‡çº§ç‰ˆå›æµ‹è¯„ä¼°å‡½æ•° (V4.3 - å¢åŠ å›æ’¤åˆ†å¸ƒåˆ†æ) ---
def run_backtest_and_evaluate(
    test_df, model, scaler, look_back, threshold, trend_config,
    fee_rate: float = FEE_RATE, slippage_rate: float = SLIPPAGE_RATE,
):
    logger.info(
        "\n" + "=" * 60 + "\n--- å¼€å§‹åœ¨æµ‹è¯•é›†ä¸Šè¿›è¡Œä¸¥æ ¼çš„å›æµ‹è¯„ä¼° (V4) ---\n" + "=" * 60
    )
    original_columns = joblib.load(FEATURE_COLUMNS_PATH)
    flattened_columns = joblib.load(FLATTENED_COLUMNS_PATH)
    test_features_df = feature_engineering(test_df).dropna()
    test_features_aligned = test_features_df.reindex(
        columns=original_columns, fill_value=0
    )
    test_scaled = scaler.transform(test_features_aligned)
    logger.info("é€æ ¹Kçº¿éå†æµ‹è¯•é›†è¿›è¡Œé¢„æµ‹...")

    # é¢„å–ç”¨äºé£æ§è¿‡æ»¤ä¸æ”¶ç›Šè®¡ç®—çš„åºåˆ—ï¼ˆä¸ç‰¹å¾å¯¹é½ï¼‰
    idx_full = test_features_df.index
    close_arr = test_df["Close"].reindex(idx_full).values
    high_arr = test_df["High"].reindex(idx_full).values
    low_arr = test_df["Low"].reindex(idx_full).values
    # å¯èƒ½ç¼ºå°‘çš„åˆ—ä½¿ç”¨å®‰å…¨é»˜è®¤å€¼
    adx_arr = test_features_df.get("ADX_14", pd.Series(np.nan, index=idx_full)).reindex(idx_full).values
    atr_arr = test_features_df.get("ATR_14", pd.Series(np.nan, index=idx_full)).reindex(idx_full).values
    macd_conf_arr = test_features_df.get("macd_cross_confirm", pd.Series(0, index=idx_full)).reindex(idx_full).astype(bool).values
    price_above_arr = test_features_df.get("price_above_ema_4h", pd.Series(0, index=idx_full)).reindex(idx_full).astype(bool).values
    atr_norm = np.divide(atr_arr, close_arr, out=np.zeros_like(atr_arr), where=np.isfinite(atr_arr) & np.isfinite(close_arr) & (close_arr > 0))

    cost_per_trade = 2 * (fee_rate + slippage_rate)
    # è½½å…¥æ ¡å‡†å™¨ï¼ˆè‹¥å­˜åœ¨ï¼‰
    calibrator = None
    if ENABLE_PROBA_CALIBRATION and os.path.exists(CALIBRATOR_SAVE_PATH):
        try:
            calibrator = joblib.load(CALIBRATOR_SAVE_PATH)
            logger.info("å·²åŠ è½½æ¦‚ç‡æ ¡å‡†å™¨ç”¨äºå›æµ‹é¢„æµ‹ã€‚")
        except Exception as e:
            logger.warning(f"åŠ è½½æ¦‚ç‡æ ¡å‡†å™¨å¤±è´¥ï¼Œä½¿ç”¨æœªæ ‡å®šæ¦‚ç‡ã€‚Error: {e}")

    # ä¸€æ¬¡æ€§è®¡ç®—å…¨éƒ¨æ¦‚ç‡ä¸åŠ¨æ€TP/SL
    probs = np.zeros(len(test_scaled))
    for i in tqdm(range(look_back, len(test_scaled))):
        input_sequence = test_scaled[i - look_back : i : LAG_STRIDE, :]
        input_flattened_np = input_sequence.flatten().reshape(1, -1)
        input_df = pd.DataFrame(input_flattened_np, columns=flattened_columns)
        p = model.predict_proba(input_df)[0][1]
        probs[i] = float(calibrator.transform([p])[0]) if calibrator is not None else float(p)

    # æ„é€ åŸºç¡€æ©ç ï¼ˆé£é™©è¿‡æ»¤ + æ¦‚ç‡é˜ˆå€¼ + æœŸæœ›ä¸ºæ­£ï¼‰
    n = len(test_scaled)
    base_mask = np.zeros(n, dtype=bool)
    tp_arr = np.full(n, TREND_CONFIG["target_return"], dtype=float)
    sl_arr = np.full(n, TREND_CONFIG["max_drawdown_limit"], dtype=float)
    if USE_ATR_BASED_EXITS:
        with np.errstate(divide="ignore", invalid="ignore"):
            atrn = np.divide(atr_arr, close_arr, out=np.zeros_like(atr_arr), where=np.isfinite(atr_arr) & (close_arr > 0))
        tp_arr = np.maximum(tp_arr, TP_ATR_MULT * atrn)
        sl_arr = np.minimum(sl_arr, SL_ATR_MULT * atrn)

    # é£é™©è¿‡æ»¤
    risk_ok = np.ones(n, dtype=bool)
    if ENABLE_RISK_FILTER:
        if ADX_MIN is not None:
            risk_ok &= np.isfinite(adx_arr) & (adx_arr >= ADX_MIN)
        if ATR_NORM_MIN is not None:
            risk_ok &= np.isfinite(atr_norm) & (atr_norm >= ATR_NORM_MIN)
        if REQUIRE_TREND_CONFIRM:
            risk_ok &= macd_conf_arr
        if REQUIRE_PRICE_ABOVE_EMA_4H:
            risk_ok &= price_above_arr

    # æ¦‚ç‡é˜ˆå€¼
    prob_ok = probs > threshold
    # æœŸæœ›ä¸ºæ­£
    if REQUIRE_POSITIVE_EXPECTANCY:
        ev_ok = (probs * tp_arr - (1 - probs) * sl_arr - cost_per_trade) > 0
    else:
        ev_ok = np.ones(n, dtype=bool)
    cand_mask = prob_ok & risk_ok & ev_ok

    # åº”ç”¨æ¯æ—¥Top-Kä¸æ¯æ—¥æœ€å°æˆäº¤æ•°ï¼ˆæŒ‰åŸå§‹ç´¢å¼•çš„æ—¥æœŸï¼‰
    selected_mask = np.zeros(n, dtype=bool)
    if ENABLE_DAILY_TOP_K or ENABLE_DAILY_MIN_TRADES:
        idx = np.arange(n)
        day_index_full = pd.Index(idx_full)
        for day, grp in pd.Series(idx, index=day_index_full).groupby(day_index_full.normalize()):
            idxs = grp.values
            idxs = idxs[(idxs >= look_back)]
            day_cand = idxs[cand_mask[idxs]]
            day_prob_risk = idxs[prob_ok[idxs] & risk_ok[idxs]]
            day_prob_only = idxs[prob_ok[idxs]]

            selected = []
            # å…ˆå– cand ä¸­çš„Top-K
            if day_cand.size > 0:
                k = DAILY_TOP_K if ENABLE_DAILY_TOP_K else day_cand.size
                k = min(k, day_cand.size)
                topk = day_cand[np.argsort(probs[day_cand])[-k:]]
                selected.extend(topk.tolist())

            # æŒ‰æ¯æ—¥æœ€å°æˆäº¤æ•°å…œåº•
            need = 0
            if ENABLE_DAILY_MIN_TRADES:
                need = max(0, MIN_DAILY_TRADES - len(selected))
            if need > 0 and day_prob_risk.size > 0:
                remain = np.setdiff1d(day_prob_risk, np.array(selected, dtype=int), assume_unique=False)
                if remain.size > 0:
                    take = min(need, remain.size)
                    extra = remain[np.argsort(probs[remain])[-take:]]
                    selected.extend(extra.tolist())
                    need = max(0, MIN_DAILY_TRADES - len(selected))
            if need > 0 and day_prob_only.size > 0:
                remain = np.setdiff1d(day_prob_only, np.array(selected, dtype=int), assume_unique=False)
                if remain.size > 0:
                    remain = remain[probs[remain] >= MIN_DAILY_PROB_FLOOR]
                    if remain.size > 0:
                        take = min(need, remain.size)
                        extra = remain[np.argsort(probs[remain])[-take:]]
                        selected.extend(extra.tolist())

            if len(selected) > 0:
                selected_mask[np.array(selected, dtype=int)] = True
    else:
        selected_mask = cand_mask

    # è‹¥ç­›é€‰åæ— ä»»ä½•ä¿¡å·ï¼Œé€æ­¥æ”¾å®½ï¼šå…ˆå–æ¶ˆEVè¿‡æ»¤ï¼Œå†å–æ¶ˆé£é™©è¿‡æ»¤ï¼Œæœ€åä»…ä¿ç•™é˜ˆå€¼
    if not np.any(selected_mask[look_back:]):
        logger.warning("æ— äº¤æ˜“ä¿¡å·ï¼ˆåŒ…å«EVä¸é£é™©è¿‡æ»¤ã€Top-Kï¼‰ã€‚æ”¾å®½EVè¿‡æ»¤åé‡è¯•...")
        cand_no_ev = prob_ok & risk_ok  # å»æ‰EVè¿‡æ»¤
        selected_mask = np.zeros(n, dtype=bool)
        if ENABLE_DAILY_TOP_K:
            idx = np.arange(n)
            day_index_full = pd.Index(idx_full)
            for day, grp in pd.Series(idx, index=day_index_full).groupby(day_index_full.normalize()):
                idxs = grp.values
                idxs = idxs[(idxs >= look_back)]
                day_candidates = idxs[cand_no_ev[idxs]]
                if day_candidates.size == 0:
                    continue
                k = min(DAILY_TOP_K, day_candidates.size)
                topk = day_candidates[np.argsort(probs[day_candidates])[-k:]]
                selected_mask[topk] = True
        else:
            selected_mask = cand_no_ev

    if not np.any(selected_mask[look_back:]):
        logger.warning("æ— äº¤æ˜“ä¿¡å·ï¼ˆå–æ¶ˆEVåä»ä¸ºç©ºï¼‰ã€‚æ”¾å®½é£é™©è¿‡æ»¤åé‡è¯•...")
        cand_no_risk = prob_ok  # ä»…é˜ˆå€¼
        selected_mask = np.zeros(n, dtype=bool)
        if ENABLE_DAILY_TOP_K:
            idx = np.arange(n)
            day_index_full = pd.Index(idx_full)
            for day, grp in pd.Series(idx, index=day_index_full).groupby(day_index_full.normalize()):
                idxs = grp.values
                idxs = idxs[(idxs >= look_back)]
                day_candidates = idxs[cand_no_risk[idxs]]
                if day_candidates.size == 0:
                    continue
                k = min(DAILY_TOP_K, day_candidates.size)
                topk = day_candidates[np.argsort(probs[day_candidates])[-k:]]
                selected_mask[topk] = True
        else:
            selected_mask = cand_no_risk

    if not np.any(selected_mask[look_back:]):
        logger.warning("æ— äº¤æ˜“ä¿¡å·ï¼ˆå–æ¶ˆé£é™©åä»ä¸ºç©ºï¼‰ã€‚æœ€ç»ˆå›é€€åˆ°é˜ˆå€¼ç­›é€‰ä¸”ä¸ä½¿ç”¨Top-Kã€‚")
        selected_mask = prob_ok

    # å…œåº•ä¿æŠ¤ï¼šè‹¥åœ¨ä»¥ä¸Šæ‰€æœ‰æ”¾å®½åä»ç„¶åœ¨å›æµ‹åŒºé—´å†…æ²¡æœ‰ä»»ä½•ä¿¡å·ï¼Œ
    # åˆ™é€€åŒ–ä¸ºâ€œä»…ä½¿ç”¨é£é™©è¿‡æ»¤ + æ¯æ—¥ Top-Kâ€çš„è§„åˆ™ï¼Œå®Œå…¨ç§»é™¤æ¦‚ç‡é˜ˆå€¼ä¸æœŸæœ›è¿‡æ»¤ï¼Œ
    # ä»¥ä¾¿åœ¨æç«¯æ ‡ç­¾/é˜ˆå€¼è®¾ç½®ä¸‹ä»èƒ½è§‚å¯Ÿç­–ç•¥çš„å¤§è‡´è¡Œä¸ºã€‚
    if not np.any(selected_mask[look_back:]):
        logger.warning(
            "å›æµ‹æœ€ç»ˆä»æ— ä»»ä½•äº¤æ˜“ä¿¡å·ï¼Œå°†ä½¿ç”¨ä»…åŸºäºé£é™©è¿‡æ»¤å’Œæ¯æ—¥Top-Kçš„å…œåº•è§„åˆ™ï¼ˆä¸ä½¿ç”¨æ¦‚ç‡é˜ˆå€¼ä¸æœŸæœ›è¿‡æ»¤ï¼‰ã€‚"
        )
        selected_mask = np.zeros(n, dtype=bool)
        if ENABLE_DAILY_TOP_K:
            idx_all = np.arange(n)
            day_index_full = pd.Index(idx_full)
            for day, grp in pd.Series(idx_all, index=day_index_full).groupby(
                day_index_full.normalize()
            ):
                idxs = grp.values
                idxs = idxs[(idxs >= look_back)]
                day_candidates = idxs[risk_ok[idxs]]
                if day_candidates.size == 0:
                    continue
                k = min(DAILY_TOP_K, day_candidates.size)
                topk = day_candidates[np.argsort(probs[day_candidates])[-k:]]
                selected_mask[topk] = True
        else:
            selected_mask = risk_ok.copy()

    # é¡ºåºæ‰§è¡Œäº¤æ˜“å¹¶ç»Ÿè®¡ï¼ŒåŒæ—¶è¾“å‡º final_signalsï¼ˆä»…è®°å½•å¤šå¤´ä¿¡å·ï¼Œç”¨äºåˆ†ç±»è¯„ä¼°ï¼‰
    final_signals = []
    pnl_list = []
    trade_count = 0
    busy_until = -1
    # è´¦æˆ·å±‚æœ€å¤§å›æ’¤ç›‘æ§ï¼šä»¥æƒç›Šæ›²çº¿ä¸ºåŸºå‡†ï¼Œä»…ç”¨äºç»Ÿè®¡å’Œå‘Šè­¦ï¼Œä¸å¼ºåˆ¶åœæ­¢äº¤æ˜“
    equity = 1.0
    equity_peak = 1.0
    max_dd_overall = 0.0

    for i in range(look_back, n):
        long_signal = bool(selected_mask[i])

        # ç®€å•åšç©ºé€»è¾‘ï¼šæ¨¡å‹æœªç»™å¤šå¤´ä¿¡å· + ADX è¶³å¤Ÿå¼ºåˆ™è€ƒè™‘å¼€ç©º
        short_signal = False
        if (not long_signal) and np.isfinite(adx_arr[i]) and (adx_arr[i] >= ADX_MIN):
            short_signal = True

        if (ENFORCE_NO_OVERLAP or COOLDOWN_BARS > 0) and i <= busy_until:
            # æŒæœ‰ä¸­æˆ–å†·å´æœŸï¼Œä¸å¼€æ–°ä»“ï¼Œå¤šå¤´ä¿¡å·æ ‡è®°ä¸º 0
            final_signals.append(0)
            continue

        if long_signal:
            # å¤šå¤´äº¤æ˜“
            final_signals.append(1)
            trade_count += 1
            entry_price = close_arr[i] * (1 + slippage_rate)
            target_price = entry_price * (1 + tp_arr[i])
            stop_price = entry_price * (1 - sl_arr[i])
            be_trigger = entry_price * (1 + BE_TRIGGER_RET) if ENABLE_BREAK_EVEN else 0.0
            be_stop = entry_price * (1 + BE_STOP_RET) if ENABLE_BREAK_EVEN else 0.0
            look_forward = trend_config["look_forward_steps"]
            i_end = min(i + look_forward, n - 1)
            hit = False
            stopped = False
            be_active = False
            breakeven = False
            exit_j = i_end
            for j in range(i + 1, i_end + 1):
                # 1) å•ç¬” TP/SL/ä¿æœ¬é€»è¾‘
                if low_arr[j] <= stop_price:
                    exit_price = stop_price * (1 - slippage_rate)
                    stopped = True
                    exit_j = j
                    break
                if high_arr[j] >= target_price:
                    exit_price = target_price * (1 - slippage_rate)
                    hit = True
                    exit_j = j
                    break
                if ENABLE_BREAK_EVEN and (not be_active) and (high_arr[j] >= be_trigger):
                    be_active = True
                if ENABLE_BREAK_EVEN and be_active and (low_arr[j] <= be_stop):
                    exit_price = be_stop * (1 - slippage_rate)
                    breakeven = True
                    exit_j = j
                    break

                # 2) è´¦æˆ·å±‚æœ€å¤§å›æ’¤ï¼šä»¥å½“å‰æ”¶ç›˜ä»·ä¼°ç®—æƒç›Šï¼Œè‹¥å›æ’¤è¶…è¿‡é˜ˆå€¼åˆ™å¼ºåˆ¶å¹³ä»“
                mark_price = close_arr[j]
                if mark_price > 0:
                    open_gross_ret = (mark_price - entry_price) / entry_price
                    open_net_ret = open_gross_ret - 2 * fee_rate
                    temp_equity = equity * (1.0 + open_net_ret)
                    temp_peak = max(equity_peak, temp_equity)
                    if temp_peak > 0:
                        temp_dd = 1.0 - temp_equity / temp_peak
                        if temp_dd >= ACCOUNT_MAX_DRAWDOWN:
                            exit_price = mark_price * (1 - slippage_rate)
                            stopped = True
                            exit_j = j
                            logger.warning(
                                f"è´¦æˆ·å±‚å›æ’¤è¾¾åˆ° {temp_dd*100:.2f}% (é˜ˆå€¼ {ACCOUNT_MAX_DRAWDOWN*100:.2f}%)ï¼Œåœ¨ {idx_full[j]} å¼ºåˆ¶å¹³ä»“ã€‚"
                            )
                            break

            if not hit and not stopped and not breakeven:
                exit_price = close_arr[i_end] * (1 - slippage_rate)
                exit_j = i_end

            gross_ret = (exit_price - entry_price) / entry_price
            net_ret = gross_ret - 2 * fee_rate
            pnl_list.append(net_ret)

            # æ›´æ–°è´¦æˆ·æƒç›Šä¸æœ€å¤§å›æ’¤ç›‘æ§
            equity *= (1.0 + net_ret)
            if equity > equity_peak:
                equity_peak = equity
            if equity_peak > 0:
                cur_dd = 1.0 - equity / equity_peak
                if cur_dd > max_dd_overall:
                    max_dd_overall = cur_dd

            if ENFORCE_NO_OVERLAP or COOLDOWN_BARS > 0:
                busy_until = exit_j + COOLDOWN_BARS

        elif short_signal:
            # ç©ºå¤´äº¤æ˜“ï¼šä»…è®¡å…¥æ”¶ç›Šï¼Œä¸å½±å“å¤šå¤´åˆ†ç±»è¯„ä¼°ï¼ˆfinal_signals è®°ä¸º 0ï¼‰
            final_signals.append(0)
            trade_count += 1
            entry_price = close_arr[i] * (1 - slippage_rate)  # åšç©ºæŒ‰å–å‡ºä»·å…¥åœº
            target_price = entry_price * (1 - tp_arr[i])      # ä»·æ ¼ä¸‹è·Œè·åˆ©
            stop_price = entry_price * (1 + sl_arr[i])        # ä¸Šæ¶¨è§¦å‘æ­¢æŸ
            be_trigger = entry_price * (1 - BE_TRIGGER_RET) if ENABLE_BREAK_EVEN else 0.0
            be_stop = entry_price * (1 - BE_STOP_RET) if ENABLE_BREAK_EVEN else 0.0
            look_forward = trend_config["look_forward_steps"]
            i_end = min(i + look_forward, n - 1)
            hit = False
            stopped = False
            be_active = False
            breakeven = False
            exit_j = i_end
            for j in range(i + 1, i_end + 1):
                # 1) å•ç¬” TP/SL/ä¿æœ¬é€»è¾‘ï¼ˆç©ºå¤´æ–¹å‘ï¼‰
                if high_arr[j] >= stop_price:
                    exit_price = stop_price * (1 + slippage_rate)
                    stopped = True
                    exit_j = j
                    break
                if low_arr[j] <= target_price:
                    exit_price = target_price * (1 + slippage_rate)
                    hit = True
                    exit_j = j
                    break
                if ENABLE_BREAK_EVEN and (not be_active) and (low_arr[j] <= be_trigger):
                    be_active = True
                if ENABLE_BREAK_EVEN and be_active and (high_arr[j] >= be_stop):
                    exit_price = be_stop * (1 + slippage_rate)
                    breakeven = True
                    exit_j = j
                    break

                # 2) è´¦æˆ·å±‚æœ€å¤§å›æ’¤ï¼šä»¥å½“å‰æ”¶ç›˜ä»·ä¼°ç®—æƒç›Šï¼Œè‹¥å›æ’¤è¶…è¿‡é˜ˆå€¼åˆ™å¼ºåˆ¶å¹³ä»“ï¼ˆç©ºå¤´æ–¹å‘ï¼‰
                mark_price = close_arr[j]
                if mark_price > 0:
                    open_gross_ret = (entry_price - mark_price) / entry_price
                    open_net_ret = open_gross_ret - 2 * fee_rate
                    temp_equity = equity * (1.0 + open_net_ret)
                    temp_peak = max(equity_peak, temp_equity)
                    if temp_peak > 0:
                        temp_dd = 1.0 - temp_equity / temp_peak
                        if temp_dd >= ACCOUNT_MAX_DRAWDOWN:
                            exit_price = mark_price * (1 + slippage_rate)
                            stopped = True
                            exit_j = j
                            logger.warning(
                                f"è´¦æˆ·å±‚å›æ’¤è¾¾åˆ° {temp_dd*100:.2f}% (é˜ˆå€¼ {ACCOUNT_MAX_DRAWDOWN*100:.2f}%)ï¼Œåœ¨ {idx_full[j]} å¼ºåˆ¶å¹³ä»“ï¼ˆç©ºå¤´å¤´å¯¸ï¼‰ã€‚"
                            )
                            break

            if not hit and not stopped and not breakeven:
                exit_price = close_arr[i_end] * (1 + slippage_rate)
                exit_j = i_end

            gross_ret = (entry_price - exit_price) / entry_price
            net_ret = gross_ret - 2 * fee_rate
            pnl_list.append(net_ret)

            # æ›´æ–°è´¦æˆ·æƒç›Šä¸æœ€å¤§å›æ’¤ç›‘æ§
            equity *= (1.0 + net_ret)
            if equity > equity_peak:
                equity_peak = equity
            if equity_peak > 0:
                cur_dd = 1.0 - equity / equity_peak
                if cur_dd > max_dd_overall:
                    max_dd_overall = cur_dd

            if ENFORCE_NO_OVERLAP or COOLDOWN_BARS > 0:
                busy_until = exit_j + COOLDOWN_BARS

        else:
            # æ— äº¤æ˜“
            final_signals.append(0)
    actual_labels_df = create_trend_labels(test_df, **trend_config).dropna()
    pred_index = test_features_df.index[look_back : look_back + len(final_signals)]
    pred_series = pd.Series(final_signals, index=pred_index)
    results_df = pd.DataFrame(actual_labels_df["label"]).join(
        pred_series.to_frame("final_signal"), how="inner"
    )
    if results_df.empty or np.sum(results_df["final_signal"]) == 0:
        logger.warning("å›æµ‹æœŸé—´æ²¡æœ‰äº§ç”Ÿä»»ä½•äº¤æ˜“ä¿¡å·ï¼Œæ— æ³•è®¡ç®—èƒœç‡ã€‚")
        return
    y_test_actual = results_df["label"].values
    y_pred_final = results_df["final_signal"].values

    winning_trades_drawdown = []
    winning_signals_df = results_df[
        (results_df["final_signal"] == 1) & (results_df["label"] == 1)
    ]
    if not winning_signals_df.empty:
        logger.info("æ­£åœ¨è®¡ç®—ç›ˆåˆ©ä¿¡å·åœ¨ç›ˆåˆ©å‰çš„æœ€å¤§å›æ’¤...")
        look_forward = trend_config["look_forward_steps"]
        test_df_copy = test_df.copy()
        test_df_copy["future_min_low"] = (
            test_df_copy["Low"].rolling(window=look_forward).min().shift(-look_forward)
        )
        winning_trades_details = winning_signals_df.join(
            test_df_copy[["Close", "future_min_low"]], how="inner"
        )
        winning_trades_details["drawdown_pct"] = (
            (winning_trades_details["Close"] - winning_trades_details["future_min_low"])
            / winning_trades_details["Close"]
        ) * 100
        winning_trades_drawdown = winning_trades_details["drawdown_pct"].tolist()

    print("\n--- [å®¢è§‚] æµ‹è¯•é›†å›æµ‹è¯„ä¼°ç»“æœ (ä»…MLæ¨¡å‹ä¿¡å·) ---")
    print(f"æ€»å›æµ‹Kçº¿æ•°: {len(y_pred_final)}")
    print(f"å‘å‡ºçœ‹æ¶¨ä¿¡å·æ€»æ¬¡æ•° (äº¤æ˜“é¢‘ç‡): {np.sum(y_pred_final)}")
    # å¢åŠ zero_division=0ä»¥é˜²æ­¢åœ¨æ²¡æœ‰ä¿¡å·æ—¶æŠ¥é”™
    print(
        f"ç²¾ç¡®ç‡ (èƒœç‡): {precision_score(y_test_actual, y_pred_final, zero_division=0):.4f}"
    )
    print(
        f"å¬å›ç‡ (ç›ˆåˆ©æœºä¼šæ•æ‰ç‡): {recall_score(y_test_actual, y_pred_final, zero_division=0):.4f}"
    )
    print("\næ··æ·†çŸ©é˜µ (TN, FP / FN, TP):")
    print(confusion_matrix(y_test_actual, y_pred_final))

    if winning_trades_drawdown:
        drawdown_array = np.array(winning_trades_drawdown)
        avg_drawdown = np.mean(drawdown_array)

        print("\n--- [æ–°æŒ‡æ ‡] ç›ˆåˆ©äº¤æ˜“åœ¨ç›ˆåˆ©å‰çš„æœ€å¤§å›æ’¤åˆ†æ ---")
        print(f"ç›ˆåˆ©çš„äº¤æ˜“æ€»æ•°: {len(drawdown_array)}")
        print(f"å¹³å‡å›æ’¤: {avg_drawdown:.4f}%")
        print(f"æœ€å¤§å›æ’¤ (æœ€å·®æƒ…å†µ): {np.max(drawdown_array):.4f}%")
        print(f"æœ€å°å›æ’¤ (æœ€å¥½æƒ…å†µ): {np.min(drawdown_array):.4f}%")

        # --- <<< æ–°å¢ï¼šå›æ’¤åˆ†å¸ƒåˆ†æ >>> ---
        # è®¡ç®—å›æ’¤å¤§äº/å°äºå¹³å‡å›æ’¤çš„äº¤æ˜“ç™¾åˆ†æ¯”
        count_above_avg = np.sum(drawdown_array > avg_drawdown)
        count_below_or_equal_avg = len(drawdown_array) - count_above_avg

        percent_above_avg = (count_above_avg / len(drawdown_array)) * 100
        percent_below_or_equal_avg = (
            count_below_or_equal_avg / len(drawdown_array)
        ) * 100

        print(f"\n  å›æ’¤åˆ†å¸ƒ (ä¸å¹³å‡å€¼ {avg_drawdown:.4f}% ç›¸æ¯”):")
        print(
            f"    - å¤§äºå¹³å‡å›æ’¤çš„äº¤æ˜“å æ¯”: {percent_above_avg:.2f}% ({count_above_avg} ç¬”)"
        )
        print(
            f"    - å°äºæˆ–ç­‰äºå¹³å‡å›æ’¤çš„äº¤æ˜“å æ¯”: {percent_below_or_equal_avg:.2f}% ({count_below_or_equal_avg} ç¬”)"
        )

        # è®¡ç®—ä¸åŒå›æ’¤æ°´å¹³ä¸‹çš„äº¤æ˜“å æ¯”
        print("\n  å›æ’¤æ°´å¹³åˆ†å¸ƒ:")
        for threshold_pct in [0.1, 0.25, 0.5, 0.75]:
            count_below_threshold = np.sum(drawdown_array <= threshold_pct)
            percent_below_threshold = (
                count_below_threshold / len(drawdown_array)
            ) * 100
            print(
                f"    - å›æ’¤ <= {threshold_pct:.2f}% çš„äº¤æ˜“å æ¯”: {percent_below_threshold:.2f}%"
            )

    else:
        print("\n--- [æ–°æŒ‡æ ‡] ç›ˆåˆ©äº¤æ˜“åœ¨ç›ˆåˆ©å‰çš„æœ€å¤§å›æ’¤åˆ†æ ---")
        print("æœ¬æ¬¡å›æµ‹æ²¡æœ‰äº§ç”Ÿä»»ä½•ç›ˆåˆ©çš„äº¤æ˜“ï¼Œæ— æ³•è®¡ç®—ç›¸å…³æŒ‡æ ‡ã€‚")

    # --- æ”¶ç›Šç»Ÿè®¡ï¼ˆå·²åœ¨é¢„æµ‹å¾ªç¯ä¸­ç´¯ç§¯ pnl_listï¼Œå¹¶è€ƒè™‘é£æ§ä¸ä¸é‡å ï¼‰ ---

    if trade_count > 0:
        pnl_arr = np.array(pnl_list)
        print("\n--- [è€ƒè™‘äº¤æ˜“æˆæœ¬ä¸æ»‘ç‚¹] ç­–ç•¥æ”¶ç›Šæ¦‚è§ˆ ---")
        print(f"äº¤æ˜“æ¬¡æ•°: {trade_count}")
        print(f"å¹³å‡å•ç¬”å‡€æ”¶ç›Š: {np.mean(pnl_arr):.5f}")
        print(f"èƒœç‡(å‡€æ”¶ç›Š>0): {np.mean(pnl_arr > 0):.4f}")
        print(f"ç´¯è®¡å‡€æ”¶ç›Š: {np.sum(pnl_arr):.4f}")
        # è¾“å‡ºè´¦æˆ·å±‚æœ€å¤§å›æ’¤ç›‘æ§ç»“æœ
        if max_dd_overall > 0:
            print(f"è´¦æˆ·å±‚æœ€å¤§å›æ’¤(åŸºäºå›æµ‹æƒç›Šæ›²çº¿): {max_dd_overall*100:.2f}% (é˜ˆå€¼: {ACCOUNT_MAX_DRAWDOWN*100:.2f}%)")
            if max_dd_overall >= ACCOUNT_MAX_DRAWDOWN:
                logger.warning(
                    f"è´¦æˆ·æƒç›Šæœ€å¤§å›æ’¤å·²è¾¾åˆ° {max_dd_overall*100:.2f}%ï¼Œè¶…è¿‡ç›‘æ§é˜ˆå€¼ {ACCOUNT_MAX_DRAWDOWN*100:.2f}%ã€‚"
                )
    else:
        print("\n--- [è€ƒè™‘äº¤æ˜“æˆæœ¬ä¸æ»‘ç‚¹] ç­–ç•¥æ”¶ç›Šæ¦‚è§ˆ ---")
        print("æ— äº¤æ˜“ï¼Œæ— æ”¶ç›Šç»Ÿè®¡ã€‚")


if __name__ == "__main__":
    if os.path.exists(DATA_CACHE_PATH):
        logger.info(f"ä»ç¼“å­˜åŠ è½½æ•°æ®: {DATA_CACHE_PATH}")
        raw_df = pd.read_csv(DATA_CACHE_PATH, index_col=0, parse_dates=True)
    else:
        raw_df = fetch_binance_klines(
            s=SYMBOL, i=INTERVAL, st=DATA_START_DATE, en=TEST_END
        )
        if not raw_df.empty:
            raw_df.to_csv(DATA_CACHE_PATH)
    if raw_df.empty:
        logger.error("æ•°æ®ä¸ºç©ºï¼Œç¨‹åºé€€å‡ºã€‚")
        exit()
    train_df = raw_df[(raw_df.index >= TRAIN_START) & (raw_df.index < VALIDATION_START)]
    validation_df = raw_df[
        (raw_df.index >= VALIDATION_START) & (raw_df.index < TEST_START)
    ]
    test_df = raw_df[(raw_df.index >= TEST_START) & (raw_df.index <= TEST_END)]
    logger.info(
        f"æ•°æ®é›†åˆ’åˆ†å®Œæˆ: {len(train_df)} è®­ç»ƒ, {len(validation_df)} éªŒè¯, {len(test_df)} æµ‹è¯•ã€‚"
    )
    trained_model, trained_scaler, best_threshold = train_and_validate(
        train_df, validation_df, LOOK_BACK, TREND_CONFIG
    )
    if trained_model and best_threshold is not None:
        run_backtest_and_evaluate(
            test_df,
            trained_model,
            trained_scaler,
            LOOK_BACK,
            best_threshold,
            TREND_CONFIG,
        )
    else:
        logger.error("æ¨¡å‹è®­ç»ƒå¤±è´¥ï¼Œè·³è¿‡å›æµ‹ã€‚")
