# Kronos å¾®è°ƒè®­ç»ƒæµç¨‹è¯´æ˜

## ğŸ¯ å¾®è°ƒæ¦‚è¿°

Kronosæä¾›äº†å®Œæ•´çš„å¾®è°ƒç®¡é“ï¼Œå…è®¸ç”¨æˆ·åœ¨è‡ªå·±çš„æ•°æ®é›†ä¸Šè¿›è¡Œæ¨¡å‹å¾®è°ƒï¼Œä»¥é€‚åº”ç‰¹å®šå¸‚åœºæˆ–äº¤æ˜“ç­–ç•¥ã€‚æœ¬æŒ‡å—ä»¥ä¸­å›½Aè‚¡å¸‚åœºä¸ºä¾‹ï¼Œå±•ç¤ºå®Œæ•´çš„å¾®è°ƒæµç¨‹ã€‚

> **å…è´£å£°æ˜**: æ­¤ç®¡é“ä»…ç”¨äºæ¼”ç¤ºå¾®è°ƒè¿‡ç¨‹ï¼Œæ˜¯ä¸€ä¸ªç®€åŒ–ç¤ºä¾‹ï¼Œä¸æ˜¯ç”Ÿäº§å°±ç»ªçš„é‡åŒ–äº¤æ˜“ç³»ç»Ÿã€‚

## ğŸ—ï¸ å¾®è°ƒæ¶æ„

```mermaid
graph TD
    A[åŸå§‹å¸‚åœºæ•°æ®] --> B[Qlibæ•°æ®é¢„å¤„ç†]
    B --> C[æ•°æ®é›†åˆ’åˆ†]
    C --> D[Tokenizerå¾®è°ƒ]
    D --> E[Predictorå¾®è°ƒ]
    E --> F[æ¨¡å‹è¯„ä¼°]
    F --> G[å›æµ‹éªŒè¯]
    
    H[é…ç½®æ–‡ä»¶] --> B
    H --> D
    H --> E
    H --> G
```

## ğŸ“‹ å‰ç½®æ¡ä»¶

### 1. ç¯å¢ƒå‡†å¤‡
```bash
# å®‰è£…åŸºç¡€ä¾èµ–
pip install -r requirements.txt

# å®‰è£…Qlibï¼ˆç”¨äºAè‚¡æ•°æ®ï¼‰
pip install pyqlib

# å¯é€‰ï¼šå®‰è£…Comet MLï¼ˆå®éªŒè·Ÿè¸ªï¼‰
pip install comet-ml
```

### 2. æ•°æ®å‡†å¤‡
```bash
# ä¸‹è½½å¹¶è®¾ç½®Qlibæ•°æ®
# å‚è€ƒå®˜æ–¹æŒ‡å—: https://github.com/microsoft/qlib
```

### 3. é…ç½®æ–‡ä»¶è®¾ç½®
ç¼–è¾‘ [`finetune/config.py`](finetune/config.py) ä¸­çš„å…³é”®è·¯å¾„ï¼š

```python
# å¿…é¡»ä¿®æ”¹çš„è·¯å¾„
self.qlib_data_path = "~/.qlib/qlib_data/cn_data"  # Qlibæ•°æ®è·¯å¾„
self.dataset_path = "./data/processed_datasets"    # å¤„ç†åæ•°æ®ä¿å­˜è·¯å¾„
self.save_path = "./outputs/models"                # æ¨¡å‹ä¿å­˜è·¯å¾„
self.backtest_result_path = "./outputs/backtest_results"  # å›æµ‹ç»“æœè·¯å¾„

# é¢„è®­ç»ƒæ¨¡å‹è·¯å¾„
self.pretrained_tokenizer_path = "NeoQuasar/Kronos-Tokenizer-base"
self.pretrained_predictor_path = "NeoQuasar/Kronos-small"
```

## ğŸ”„ å®Œæ•´å¾®è°ƒæµç¨‹

### æ­¥éª¤1: é…ç½®å®éªŒå‚æ•°

#### 1.1 æ•°æ®é…ç½®
```python
# åœ¨ config.py ä¸­è®¾ç½®
class Config:
    # æ•°æ®å‚æ•°
    self.instrument = 'csi300'  # è‚¡ç¥¨æ± ï¼šcsi300/csi800/csi1000
    self.dataset_begin_time = "2011-01-01"
    self.dataset_end_time = '2025-06-05'
    
    # æ—¶é—´çª—å£
    self.lookback_window = 90   # å†å²çª—å£é•¿åº¦
    self.predict_window = 10    # é¢„æµ‹çª—å£é•¿åº¦
    self.max_context = 512      # æ¨¡å‹æœ€å¤§ä¸Šä¸‹æ–‡
    
    # ç‰¹å¾åˆ—
    self.feature_list = ['open', 'high', 'low', 'close', 'vol', 'amt']
    self.time_feature_list = ['minute', 'hour', 'weekday', 'day', 'month']
```

#### 1.2 è®­ç»ƒé…ç½®
```python
# è®­ç»ƒè¶…å‚æ•°
self.epochs = 30
self.batch_size = 50
self.tokenizer_learning_rate = 2e-4
self.predictor_learning_rate = 4e-5

# æ•°æ®é›†åˆ’åˆ†
self.train_time_range = ["2011-01-01", "2022-12-31"]
self.val_time_range = ["2022-09-01", "2024-06-30"]
self.test_time_range = ["2024-04-01", "2025-06-05"]
```

#### 1.3 å®éªŒè·Ÿè¸ªé…ç½®
```python
# Comet MLé…ç½®ï¼ˆå¯é€‰ï¼‰
self.use_comet = True
self.comet_config = {
    "api_key": "YOUR_COMET_API_KEY",
    "project_name": "Kronos-Finetune-Demo",
    "workspace": "your_comet_workspace"
}
```

### æ­¥éª¤2: æ•°æ®é¢„å¤„ç†

```bash
python finetune/qlib_data_preprocess.py
```

**åŠŸèƒ½è¯´æ˜**:
- ä»QlibåŠ è½½åŸå§‹å¸‚åœºæ•°æ®
- ç”Ÿæˆæ—¶é—´ç‰¹å¾
- åˆ›å»ºæ»‘åŠ¨çª—å£æ ·æœ¬
- åˆ’åˆ†è®­ç»ƒ/éªŒè¯/æµ‹è¯•é›†
- ä¿å­˜ä¸ºpickleæ–‡ä»¶

**è¾“å‡ºæ–‡ä»¶**:
```
./data/processed_datasets/
â”œâ”€â”€ train_data.pkl    # è®­ç»ƒæ•°æ®
â”œâ”€â”€ val_data.pkl      # éªŒè¯æ•°æ®
â””â”€â”€ test_data.pkl     # æµ‹è¯•æ•°æ®
```

**æ•°æ®æ ¼å¼**:
```python
# æ¯ä¸ªæ ·æœ¬çš„ç»“æ„
{
    'input_data': tensor,      # è¾“å…¥ç‰¹å¾ [lookback_window, feature_dim]
    'target_data': tensor,     # ç›®æ ‡æ•°æ® [predict_window, feature_dim]
    'input_timestamps': list,  # è¾“å…¥æ—¶é—´æˆ³
    'target_timestamps': list, # ç›®æ ‡æ—¶é—´æˆ³
    'symbol': str             # è‚¡ç¥¨ä»£ç 
}
```

### æ­¥éª¤3: Tokenizerå¾®è°ƒ

```bash
# å¤šGPUè®­ç»ƒï¼ˆæ¨èï¼‰
torchrun --standalone --nproc_per_node=2 finetune/train_tokenizer.py

# å•GPUè®­ç»ƒ
python finetune/train_tokenizer.py
```

**è®­ç»ƒè¿‡ç¨‹**:
1. åŠ è½½é¢„è®­ç»ƒTokenizer
2. åœ¨æ–°æ•°æ®ä¸Šå¾®è°ƒç¼–ç å™¨
3. ä¼˜åŒ–é‡æ„æŸå¤±
4. ä¿å­˜æœ€ä½³æ£€æŸ¥ç‚¹

**å…³é”®å‚æ•°**:
```python
# åœ¨è®­ç»ƒè„šæœ¬ä¸­
learning_rate = config.tokenizer_learning_rate  # 2e-4
batch_size = config.batch_size                  # 50
epochs = config.epochs                          # 30
```

**è¾“å‡º**:
```
./outputs/models/finetune_tokenizer_demo/
â”œâ”€â”€ checkpoints/
â”‚   â”œâ”€â”€ best_model/          # æœ€ä½³æ¨¡å‹
â”‚   â”œâ”€â”€ epoch_10/           # å®šæœŸæ£€æŸ¥ç‚¹
â”‚   â””â”€â”€ epoch_20/
â”œâ”€â”€ logs/                   # è®­ç»ƒæ—¥å¿—
â””â”€â”€ config.json            # è®­ç»ƒé…ç½®
```

### æ­¥éª¤4: Predictorå¾®è°ƒ

```bash
# å¤šGPUè®­ç»ƒï¼ˆæ¨èï¼‰
torchrun --standalone --nproc_per_node=2 finetune/train_predictor.py

# å•GPUè®­ç»ƒ
python finetune/train_predictor.py
```

**è®­ç»ƒè¿‡ç¨‹**:
1. åŠ è½½å¾®è°ƒåçš„Tokenizer
2. åŠ è½½é¢„è®­ç»ƒPredictor
3. åœ¨æ–°æ•°æ®ä¸Šå¾®è°ƒé¢„æµ‹å™¨
4. ä¼˜åŒ–é¢„æµ‹æŸå¤±
5. ä¿å­˜æœ€ä½³æ£€æŸ¥ç‚¹

**å…³é”®å‚æ•°**:
```python
learning_rate = config.predictor_learning_rate  # 4e-5
batch_size = config.batch_size                  # 50
accumulation_steps = config.accumulation_steps  # 1
```

**è¾“å‡º**:
```
./outputs/models/finetune_predictor_demo/
â”œâ”€â”€ checkpoints/
â”‚   â”œâ”€â”€ best_model/          # æœ€ä½³æ¨¡å‹
â”‚   â”œâ”€â”€ epoch_15/           # å®šæœŸæ£€æŸ¥ç‚¹
â”‚   â””â”€â”€ epoch_25/
â”œâ”€â”€ logs/                   # è®­ç»ƒæ—¥å¿—
â””â”€â”€ config.json            # è®­ç»ƒé…ç½®
```

### æ­¥éª¤5: æ¨¡å‹è¯„ä¼°ä¸å›æµ‹

```bash
python finetune/qlib_test.py --device cuda:0
```

**è¯„ä¼°è¿‡ç¨‹**:
1. åŠ è½½å¾®è°ƒåçš„æ¨¡å‹
2. åœ¨æµ‹è¯•é›†ä¸Šç”Ÿæˆé¢„æµ‹
3. è®¡ç®—é¢„æµ‹ä¿¡å·
4. æ‰§è¡Œå›æµ‹ç­–ç•¥
5. ç”Ÿæˆæ€§èƒ½æŠ¥å‘Š

**å›æµ‹ç­–ç•¥**:
```python
# Top-Kç­–ç•¥é…ç½®
n_symbol_hold = 50      # æŒä»“è‚¡ç¥¨æ•°é‡
n_symbol_drop = 5       # å‰”é™¤è‚¡ç¥¨æ•°é‡
hold_thresh = 5         # æœ€å°æŒä»“æœŸ
```

**è¾“å‡ºç»“æœ**:
```
./outputs/backtest_results/
â”œâ”€â”€ backtest_results.json    # è¯¦ç»†ç»“æœ
â”œâ”€â”€ performance_plot.png     # æ”¶ç›Šæ›²çº¿å›¾
â””â”€â”€ analysis_report.txt      # åˆ†ææŠ¥å‘Š
```

## ğŸ“Š è®­ç»ƒç›‘æ§

### 1. æŸå¤±å‡½æ•°ç›‘æ§
```python
# Tokenizerè®­ç»ƒ
reconstruction_loss = F.mse_loss(reconstructed, original)

# Predictorè®­ç»ƒ  
prediction_loss = F.mse_loss(predicted, target)
```

### 2. éªŒè¯æŒ‡æ ‡
```python
# é¢„æµ‹ç²¾åº¦æŒ‡æ ‡
mae = mean_absolute_error(y_true, y_pred)
mse = mean_squared_error(y_true, y_pred)
r2 = r2_score(y_true, y_pred)

# é‡‘èæŒ‡æ ‡
sharpe_ratio = annual_return / annual_volatility
max_drawdown = max(cumulative_returns) - min(cumulative_returns)
```

### 3. Comet MLé›†æˆ
```python
# è‡ªåŠ¨è®°å½•çš„æŒ‡æ ‡
experiment.log_metric("train_loss", loss)
experiment.log_metric("val_loss", val_loss)
experiment.log_metric("learning_rate", lr)
experiment.log_parameters(config.__dict__)
```

## ğŸ”§ é«˜çº§é…ç½®

### 1. å¤šGPUè®­ç»ƒé…ç½®
```bash
# è®¾ç½®GPUæ•°é‡
export CUDA_VISIBLE_DEVICES=0,1,2,3

# å¯åŠ¨åˆ†å¸ƒå¼è®­ç»ƒ
torchrun --standalone --nproc_per_node=4 finetune/train_predictor.py
```

### 2. å†…å­˜ä¼˜åŒ–
```python
# æ¢¯åº¦ç´¯ç§¯
accumulation_steps = 4  # æœ‰æ•ˆæ‰¹å¤§å° = batch_size * accumulation_steps

# æ··åˆç²¾åº¦è®­ç»ƒ
use_amp = True
scaler = GradScaler()
```

### 3. å­¦ä¹ ç‡è°ƒåº¦
```python
# ä½™å¼¦é€€ç«
scheduler = CosineAnnealingLR(optimizer, T_max=epochs)

# é¢„çƒ­ç­–ç•¥
warmup_steps = epochs // 10
```

### 4. æ•°æ®å¢å¼º
```python
# æ—¶é—´åºåˆ—å¢å¼º
- æ·»åŠ å™ªå£°
- æ—¶é—´æ‰­æ›²
- å¹…åº¦ç¼©æ”¾
- çª—å£åˆ‡ç‰‡
```

## ğŸ“ˆ ç»“æœåˆ†æ

### 1. è®­ç»ƒæ›²çº¿åˆ†æ
```python
# ç»˜åˆ¶æŸå¤±æ›²çº¿
plt.plot(train_losses, label='Training Loss')
plt.plot(val_losses, label='Validation Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend()
plt.show()
```

### 2. é¢„æµ‹è´¨é‡è¯„ä¼°
```python
# é¢„æµ‹vså®é™…å¯¹æ¯”
correlation = np.corrcoef(predictions, actuals)[0,1]
directional_accuracy = np.mean(np.sign(predictions) == np.sign(actuals))
```

### 3. å›æµ‹æ€§èƒ½åˆ†æ
```python
# å…³é”®æŒ‡æ ‡
annual_return = 0.15      # å¹´åŒ–æ”¶ç›Šç‡
sharpe_ratio = 1.2        # å¤æ™®æ¯”ç‡
max_drawdown = 0.08       # æœ€å¤§å›æ’¤
win_rate = 0.55          # èƒœç‡
```

## âš ï¸ æ³¨æ„äº‹é¡¹

### 1. æ•°æ®è´¨é‡
- ç¡®ä¿æ•°æ®æ— ç¼ºå¤±å€¼
- å¤„ç†å¼‚å¸¸å€¼å’Œåœç‰Œ
- è€ƒè™‘è‚¡ç¥¨åˆ†çº¢é™¤æƒ

### 2. è¿‡æ‹Ÿåˆé˜²èŒƒ
- ä½¿ç”¨éªŒè¯é›†æ—©åœ
- æ­£åˆ™åŒ–æŠ€æœ¯
- äº¤å‰éªŒè¯

### 3. è®¡ç®—èµ„æº
- GPUå†…å­˜éœ€æ±‚ï¼š8GB+
- è®­ç»ƒæ—¶é—´ï¼šæ•°å°æ—¶åˆ°æ•°å¤©
- å­˜å‚¨éœ€æ±‚ï¼šæ•°GB

### 4. å®é™…åº”ç”¨è€ƒè™‘
- äº¤æ˜“æˆæœ¬å»ºæ¨¡
- å¸‚åœºå†²å‡»æˆæœ¬
- æµåŠ¨æ€§çº¦æŸ
- é£é™©ç®¡ç†

## ğŸ”„ ç”Ÿäº§éƒ¨ç½²

### 1. æ¨¡å‹å¯¼å‡º
```python
# ä¿å­˜å®Œæ•´æ¨¡å‹
torch.save({
    'tokenizer': tokenizer.state_dict(),
    'predictor': predictor.state_dict(),
    'config': config
}, 'kronos_finetuned.pth')
```

### 2. æ¨ç†ä¼˜åŒ–
```python
# æ¨¡å‹é‡åŒ–
model = torch.quantization.quantize_dynamic(model, {torch.nn.Linear}, dtype=torch.qint8)

# TorchScriptç¼–è¯‘
scripted_model = torch.jit.script(model)
```

### 3. æœåŠ¡åŒ–éƒ¨ç½²
```python
# Flask APIç¤ºä¾‹
@app.route('/predict', methods=['POST'])
def predict():
    data = request.json
    prediction = model.predict(data)
    return jsonify(prediction)
```

## ğŸ“š æ‰©å±•é˜…è¯»

- [Qlibå®˜æ–¹æ–‡æ¡£](https://github.com/microsoft/qlib)
- [PyTorchåˆ†å¸ƒå¼è®­ç»ƒ](https://pytorch.org/tutorials/intermediate/ddp_tutorial.html)
- [Comet MLå®éªŒè·Ÿè¸ª](https://www.comet.com/docs/)
- [é‡åŒ–æŠ•èµ„ç­–ç•¥](https://www.quantstart.com/)

---

**ä¸‹ä¸€æ­¥**: å®Œæˆå¾®è°ƒåï¼Œå¯ä»¥ä½¿ç”¨å¾®è°ƒåçš„æ¨¡å‹è¿›è¡Œå®é™…é¢„æµ‹ï¼Œæˆ–é›†æˆåˆ°é‡åŒ–äº¤æ˜“ç³»ç»Ÿä¸­ã€‚