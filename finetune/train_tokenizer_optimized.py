#!/usr/bin/env python3
"""
ä¼˜åŒ–ç‰ˆBTC tokenizerè®­ç»ƒè„šæœ¬
åŒ…å«æ—©åœæœºåˆ¶ã€å¢å¼ºç›‘æ§å’Œæ•°æ®å¢å¼ºåŠŸèƒ½
"""
import os
import sys
import json
import time
import argparse
import torch
import torch.nn.functional as F
from torch.utils.data import DataLoader
import numpy as np

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from finetune.btc_config_optimized import BTCConfigOptimized
from finetune.dataset import QlibDataset
from model.kronos import KronosTokenizer


class EarlyStopping:
    """æ—©åœæœºåˆ¶å®ç°"""
    def __init__(self, patience=5, min_delta=1e-5, mode='min'):
        self.patience = patience
        self.min_delta = min_delta
        self.mode = mode
        self.best_score = None
        self.counter = 0
        self.early_stop = False
        
    def __call__(self, val_loss):
        score = -val_loss if self.mode == 'min' else val_loss
        
        if self.best_score is None:
            self.best_score = score
        elif score < self.best_score + self.min_delta:
            self.counter += 1
            if self.counter >= self.patience:
                self.early_stop = True
        else:
            self.best_score = score
            self.counter = 0
        
        return self.early_stop


class TrainingMonitor:
    """è®­ç»ƒç›‘æ§å™¨"""
    def __init__(self, config):
        self.config = config
        self.metrics_history = {
            'train_loss': [],
            'val_loss': [],
            'learning_rate': [],
            'reconstruction_loss': [],
            'quantization_loss': [],
            'gradient_norm': []
        }
    
    def log_metrics(self, epoch, metrics):
        """è®°å½•è®­ç»ƒæŒ‡æ ‡"""
        for key, value in metrics.items():
            if key in self.metrics_history:
                self.metrics_history[key].append(value)
        
        # æ‰“å°è¯¦ç»†æŒ‡æ ‡
        print(f"\n--- Epoch {epoch} è¯¦ç»†æŒ‡æ ‡ ---")
        for key, value in metrics.items():
            if isinstance(value, float):
                print(f"{key}: {value:.6f}")
            else:
                print(f"{key}: {value}")
    
    def save_metrics(self, save_path):
        """ä¿å­˜è®­ç»ƒæŒ‡æ ‡åˆ°æ–‡ä»¶"""
        metrics_file = os.path.join(save_path, 'training_metrics.json')
        with open(metrics_file, 'w') as f:
            json.dump(self.metrics_history, f, indent=2)
        print(f"è®­ç»ƒæŒ‡æ ‡å·²ä¿å­˜åˆ°: {metrics_file}")


def apply_data_augmentation(batch_x, config):
    """åº”ç”¨æ•°æ®å¢å¼º"""
    if not config.data_augmentation['enabled']:
        return batch_x
    
    augmented_x = batch_x.clone()
    
    # æ·»åŠ é«˜æ–¯å™ªå£°
    if config.data_augmentation['noise_std'] > 0:
        noise = torch.randn_like(augmented_x) * config.data_augmentation['noise_std']
        augmented_x += noise
    
    # éšæœºç¼©æ”¾
    scale_range = config.data_augmentation['scale_factor_range']
    scale_factor = torch.uniform(scale_range[0], scale_range[1], (1,)).item()
    augmented_x *= scale_factor
    
    return augmented_x


def create_optimized_scheduler(optimizer, config, steps_per_epoch):
    """åˆ›å»ºä¼˜åŒ–çš„å­¦ä¹ ç‡è°ƒåº¦å™¨"""
    scheduler_config = config.get_scheduler_config(optimizer, steps_per_epoch)
    
    scheduler = torch.optim.lr_scheduler.OneCycleLR(
        optimizer=optimizer,
        max_lr=scheduler_config['max_lr'],
        steps_per_epoch=steps_per_epoch,
        epochs=config.epochs,
        pct_start=scheduler_config['pct_start'],
        div_factor=scheduler_config['div_factor'],
        final_div_factor=scheduler_config['final_div_factor'],
        anneal_strategy=scheduler_config['anneal_strategy']
    )
    
    return scheduler


def create_dataloaders(config):
    """åˆ›å»ºæ•°æ®åŠ è½½å™¨"""
    print("åˆ›å»ºæ•°æ®åŠ è½½å™¨...")
    try:
        print("æ­£åœ¨åˆ›å»ºè®­ç»ƒæ•°æ®é›†...")
        train_dataset = QlibDataset('train', config=config)
        print(f"è®­ç»ƒæ•°æ®é›†åˆ›å»ºæˆåŠŸï¼Œå¤§å°: {len(train_dataset)}")
        
        print("æ­£åœ¨åˆ›å»ºéªŒè¯æ•°æ®é›†...")
        valid_dataset = QlibDataset('val', config=config)
        print(f"éªŒè¯æ•°æ®é›†åˆ›å»ºæˆåŠŸï¼Œå¤§å°: {len(valid_dataset)}")
        
    except Exception as e:
        print(f"âŒ æ•°æ®é›†åˆ›å»ºå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        raise e

    train_loader = DataLoader(
        train_dataset,
        batch_size=config.batch_size,
        shuffle=True,
        num_workers=config.num_workers,
        pin_memory=False,
        drop_last=True
    )
    val_loader = DataLoader(
        valid_dataset,
        batch_size=config.batch_size,
        shuffle=False,
        num_workers=config.num_workers,
        pin_memory=False,
        drop_last=False
    )
    print(f"æ•°æ®åŠ è½½å™¨åˆ›å»ºå®Œæˆã€‚è®­ç»ƒæ­¥æ•°/epoch: {len(train_loader)}, éªŒè¯æ­¥æ•°: {len(val_loader)}")
    return train_loader, val_loader, train_dataset, valid_dataset


def train_model(model, device, config, save_dir):
    """ä¼˜åŒ–ç‰ˆè®­ç»ƒå‡½æ•°"""
    start_time = time.time()
    print(f"æ‰¹æ¬¡å¤§å°: {config.batch_size}")
    
    # æ‰“å°ä¼˜åŒ–é…ç½®æ‘˜è¦
    config.print_optimization_summary()
    
    train_loader, val_loader, train_dataset, valid_dataset = create_dataloaders(config)
    
    optimizer = torch.optim.AdamW(
        model.parameters(),
        lr=config.tokenizer_learning_rate,
        weight_decay=config.adam_weight_decay,
        betas=(config.adam_beta1, config.adam_beta2)
    )
    
    # ä½¿ç”¨ä¼˜åŒ–çš„å­¦ä¹ ç‡è°ƒåº¦å™¨
    scheduler = create_optimized_scheduler(optimizer, config, len(train_loader))
    
    # åˆå§‹åŒ–æ—©åœæœºåˆ¶
    early_stopping = None
    if config.early_stopping_enabled:
        early_stopping = EarlyStopping(
            patience=config.early_stopping_patience,
            min_delta=config.early_stopping_min_delta,
            mode='min'
        )
        print(f"æ—©åœæœºåˆ¶å·²å¯ç”¨: patience={config.early_stopping_patience}")
    
    # åˆå§‹åŒ–è®­ç»ƒç›‘æ§å™¨
    monitor = TrainingMonitor(config)
    
    best_val_loss = float('inf')
    batch_idx_global_train = 0
    saved_models = []  # ä¿å­˜çš„æ¨¡å‹åˆ—è¡¨
    
    for epoch_idx in range(config.epochs):
        epoch_start_time = time.time()
        model.train()
        
        # è®¾ç½®æ•°æ®é›†ç§å­
        train_dataset.set_epoch_seed(epoch_idx * 10000)
        valid_dataset.set_epoch_seed(0)
        
        print(f"\n=== Epoch {epoch_idx + 1}/{config.epochs} ===")
        
        # è®­ç»ƒå¾ªç¯
        epoch_train_loss = 0.0
        epoch_recon_loss = 0.0
        epoch_quant_loss = 0.0
        epoch_grad_norm = 0.0
        num_batches = 0
        
        for i, (ori_batch_x, _) in enumerate(train_loader):
            ori_batch_x = ori_batch_x.squeeze(0).to(device, non_blocking=True)
            
            # åº”ç”¨æ•°æ®å¢å¼º
            if config.data_augmentation['enabled']:
                ori_batch_x = apply_data_augmentation(ori_batch_x, config)
            
            # æ¢¯åº¦ç´¯ç§¯å¾ªç¯
            current_batch_total_loss = 0.0
            current_batch_recon_loss = 0.0
            current_batch_quant_loss = 0.0
            
            for j in range(config.accumulation_steps):
                start_idx = j * (ori_batch_x.shape[0] // config.accumulation_steps)
                end_idx = (j + 1) * (ori_batch_x.shape[0] // config.accumulation_steps)
                batch_x = ori_batch_x[start_idx:end_idx]
                
                # å‰å‘ä¼ æ’­
                try:
                    zs, bsq_loss, _, _ = model(batch_x)
                    z_pre, z = zs
                    
                    # æŸå¤±è®¡ç®—
                    recon_loss_pre = F.mse_loss(z_pre, batch_x)
                    recon_loss_all = F.mse_loss(z, batch_x)
                    recon_loss = recon_loss_pre + recon_loss_all
                    loss = (recon_loss + bsq_loss) / 2
                    
                    loss_scaled = loss / config.accumulation_steps
                    current_batch_total_loss += loss.item()
                    current_batch_recon_loss += recon_loss.item()
                    current_batch_quant_loss += bsq_loss.item()
                    loss_scaled.backward()
                    
                except Exception as e:
                    print(f"å‰å‘ä¼ æ’­é”™è¯¯: {e}")
                    continue
            
            # è®¡ç®—æ¢¯åº¦èŒƒæ•°
            grad_norm = torch.nn.utils.clip_grad_norm_(
                model.parameters(), 
                max_norm=config.gradient_clip_norm
            )
            
            optimizer.step()
            scheduler.step()
            optimizer.zero_grad()
            
            # ç´¯ç§¯ç»Ÿè®¡
            epoch_train_loss += current_batch_total_loss / config.accumulation_steps
            epoch_recon_loss += current_batch_recon_loss / config.accumulation_steps
            epoch_quant_loss += current_batch_quant_loss / config.accumulation_steps
            epoch_grad_norm += grad_norm.item()
            num_batches += 1
            
            # è®°å½•æ—¥å¿—
            if (batch_idx_global_train + 1) % config.log_interval == 0:
                avg_loss = current_batch_total_loss / config.accumulation_steps
                print(
                    f"[Epoch {epoch_idx + 1}/{config.epochs}, Step {i + 1}/{len(train_loader)}] "
                    f"LR {optimizer.param_groups[0]['lr']:.6f}, Loss: {avg_loss:.4f}, "
                    f"Recon: {current_batch_recon_loss/config.accumulation_steps:.4f}, "
                    f"Quant: {current_batch_quant_loss/config.accumulation_steps:.4f}, "
                    f"GradNorm: {grad_norm:.4f}"
                )
            
            batch_idx_global_train += 1
        
        # è®¡ç®—å¹³å‡è®­ç»ƒæŒ‡æ ‡
        avg_train_loss = epoch_train_loss / num_batches if num_batches > 0 else 0
        avg_recon_loss = epoch_recon_loss / num_batches if num_batches > 0 else 0
        avg_quant_loss = epoch_quant_loss / num_batches if num_batches > 0 else 0
        avg_grad_norm = epoch_grad_norm / num_batches if num_batches > 0 else 0
        
        # éªŒè¯å¾ªç¯
        model.eval()
        tot_val_loss = 0.0
        val_sample_count = 0
        with torch.no_grad():
            for ori_batch_x, _ in val_loader:
                ori_batch_x = ori_batch_x.squeeze(0).to(device, non_blocking=True)
                try:
                    zs, _, _, _ = model(ori_batch_x)
                    _, z = zs
                    val_loss_item = F.mse_loss(z, ori_batch_x)
                    
                    tot_val_loss += val_loss_item.item() * ori_batch_x.size(0)
                    val_sample_count += ori_batch_x.size(0)
                except Exception as e:
                    print(f"éªŒè¯é”™è¯¯: {e}")
                    continue
        
        avg_val_loss = tot_val_loss / val_sample_count if val_sample_count > 0 else float('inf')
        
        # è®°å½•è®­ç»ƒæŒ‡æ ‡
        current_lr = optimizer.param_groups[0]['lr']
        metrics = {
            'epoch': epoch_idx + 1,
            'train_loss': avg_train_loss,
            'val_loss': avg_val_loss,
            'learning_rate': current_lr,
            'reconstruction_loss': avg_recon_loss,
            'quantization_loss': avg_quant_loss,
            'gradient_norm': avg_grad_norm
        }
        monitor.log_metrics(epoch_idx + 1, metrics)
        
        # Epochæ€»ç»“
        epoch_time = time.time() - epoch_start_time
        total_time = time.time() - start_time
        print(f"\n--- Epoch {epoch_idx + 1}/{config.epochs} æ€»ç»“ ---")
        print(f"è®­ç»ƒæŸå¤±: {avg_train_loss:.4f}")
        print(f"éªŒè¯æŸå¤±: {avg_val_loss:.4f}")
        print(f"å­¦ä¹ ç‡: {current_lr:.6f}")
        print(f"æœ¬è½®ç”¨æ—¶: {epoch_time:.2f}ç§’")
        print(f"æ€»ç”¨æ—¶: {total_time:.2f}ç§’")
        
        # ä¿å­˜æœ€ä½³æ¨¡å‹
        if avg_val_loss < best_val_loss:
            best_val_loss = avg_val_loss
            save_path = f"{save_dir}/checkpoints/best_model"
            os.makedirs(save_path, exist_ok=True)
            try:
                model.save_pretrained(save_path)
                print(f"æœ€ä½³æ¨¡å‹å·²ä¿å­˜åˆ° {save_path} (éªŒè¯æŸå¤±: {best_val_loss:.4f})")
                
                # ç®¡ç†ä¿å­˜çš„æ¨¡å‹æ•°é‡
                saved_models.append((avg_val_loss, save_path))
                saved_models.sort(key=lambda x: x[0])  # æŒ‰æŸå¤±æ’åº
                
                # åªä¿ç•™å‰kä¸ªæœ€ä½³æ¨¡å‹
                if len(saved_models) > config.validation_config['save_top_k']:
                    saved_models = saved_models[:config.validation_config['save_top_k']]
                
            except Exception as e:
                print(f"æ¨¡å‹ä¿å­˜å¤±è´¥: {e}")
        
        # æ—©åœæ£€æŸ¥
        if early_stopping is not None:
            if early_stopping(avg_val_loss):
                print(f"\nğŸ›‘ æ—©åœè§¦å‘ï¼éªŒè¯æŸå¤±è¿ç»­{config.early_stopping_patience}è½®æœªæ”¹å–„")
                print(f"æœ€ä½³éªŒè¯æŸå¤±: {best_val_loss:.4f}")
                break
    
    # ä¿å­˜è®­ç»ƒæŒ‡æ ‡
    monitor.save_metrics(save_dir)
    
    return {
        'best_val_loss': best_val_loss,
        'total_epochs': epoch_idx + 1,
        'early_stopped': early_stopping.early_stop if early_stopping else False
    }


def main():
    parser = argparse.ArgumentParser()
    parser.add_argument('--config', type=str, default='btc_config_optimized', help='é…ç½®æ¨¡å—å')
    parser.add_argument('--test', action='store_true', help='æµ‹è¯•æ¨¡å¼')
    args = parser.parse_args()
    
    # åŠ è½½ä¼˜åŒ–é…ç½®
    config = BTCConfigOptimized()
    
    if args.test:
        print("=== æµ‹è¯•æ¨¡å¼ ===")
        print("ä¼˜åŒ–é…ç½®åŠ è½½æˆåŠŸ")
        config.print_optimization_summary()
        return
    
    # è®¾ç½®è®¾å¤‡
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print(f"ä½¿ç”¨è®¾å¤‡: {device}")
    
    # è®¾ç½®ä¿å­˜ç›®å½•
    save_dir = os.path.join(config.save_path, config.tokenizer_save_folder_name)
    os.makedirs(os.path.join(save_dir, 'checkpoints'), exist_ok=True)
    
    print("å¼€å§‹ä¼˜åŒ–ç‰ˆè®­ç»ƒ...")
    print("æ–°å¢åŠŸèƒ½: æ—©åœæœºåˆ¶ã€æ•°æ®å¢å¼ºã€å¢å¼ºç›‘æ§")
    
    try:
        # å°è¯•åŠ è½½é¢„è®­ç»ƒæ¨¡å‹
        print(f"å°è¯•åŠ è½½é¢„è®­ç»ƒæ¨¡å‹: {config.pretrained_tokenizer_path}")
        
        if os.path.exists(config.pretrained_tokenizer_path):
            print("æ£€æµ‹åˆ°æœ¬åœ°æ¨¡å‹è·¯å¾„")
            model = KronosTokenizer.from_pretrained(config.pretrained_tokenizer_path)
        else:
            print("æ£€æµ‹åˆ°Hugging Faceæ¨¡å‹æ ‡è¯†ç¬¦ï¼Œå°è¯•ä»Hubä¸‹è½½...")
            try:
                model = KronosTokenizer.from_pretrained(config.pretrained_tokenizer_path)
            except Exception as e:
                print(f"âš ï¸ ä»Hugging FaceåŠ è½½å¤±è´¥: {e}")
                print("ä½¿ç”¨é»˜è®¤å‚æ•°åˆå§‹åŒ–æ–°çš„KronosTokenizer...")
                
                model = KronosTokenizer(
                    d_in=6,
                    d_model=512,
                    n_heads=8,
                    ff_dim=2048,
                    n_enc_layers=6,
                    n_dec_layers=6,
                    ffn_dropout_p=0.1,
                    attn_dropout_p=0.1,
                    resid_dropout_p=0.1,
                    s1_bits=8,
                    s2_bits=8,
                    beta=0.25,
                    gamma0=1.0,
                    gamma=0.99,
                    zeta=1e-4,
                    group_size=1
                )
                print("âœ… ä½¿ç”¨é»˜è®¤å‚æ•°åˆå§‹åŒ–æ¨¡å‹æˆåŠŸ")
        
        model.to(device)
        print("âœ… æ¨¡å‹åŠ è½½/åˆå§‹åŒ–æˆåŠŸ")
        
        # å¼€å§‹è®­ç»ƒ
        result = train_model(model, device, config, save_dir)
        
        print(f"\nğŸ‰ è®­ç»ƒå®Œæˆï¼")
        print(f"æœ€ä½³éªŒè¯æŸå¤±: {result['best_val_loss']:.4f}")
        print(f"å®é™…è®­ç»ƒè½®æ•°: {result['total_epochs']}/{config.epochs}")
        if result['early_stopped']:
            print("âœ… é€šè¿‡æ—©åœæœºåˆ¶æå‰ç»“æŸè®­ç»ƒ")
        
    except Exception as e:
        print(f"âŒ æ¨¡å‹åŠ è½½æˆ–è®­ç»ƒå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()


if __name__ == '__main__':
    main()